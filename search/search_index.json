{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"High Performance Computing (HPC)","text":"<p>Welcome to HPC at New Jersey Institute of Technology (NJIT).</p> <p>NJIT provides High Performance Computing resources to support scientific computing for faculty and students. These resources include CPU nodes, GPU nodes, parallel storage, high speed, low latency Infiniband networking and a fully optimized scientific software stack.</p> <ul> <li>"},{"location":"#virtual-tour-of-njit-data-center","title":"Virtual Tour of NJIT Data Center","text":"<p>Wulver is built through a partnership with DataBank, which is live in DataBank\u2019s Piscataway, N.J. data center (EWR2) and will support NJIT\u2019s research efforts. This infrastructure will bolster NJIT\u2019s research initiatives. You can access the 3D virtual tour of HPC data center below:</p> <p></p>"},{"location":"#hpc-highlights","title":"HPC Highlights","text":"<p>Contact Us</p> <p>To create a ticket or request for software installation, visit Contact Us.</p> <p>Clusters</p> <p>See the list of availble clusters, their specifications, and how to access them in Clusters.</p> <p>Software Modules</p> <p>See Software Modules for list of software packages installed on our cluster.</p> <p>Cluster Maintenance Updates</p> <p>To see the latest maintenance updates on NJIT cluster, please visit Cluster Maintenance.</p> <p>HPC Events</p> <p>Check the upcoming events hosted by HPC team and register from HPC Events.</p> <p>HPC Trainings and Webinars</p> <p>Make sure to stay up-to-date with the latest webinar sessions on HPC by visiting the HPC Training.</p> <p>FAQs</p> <p>For any queries regarding the usage of our cluster, please visit the FAQs which are organized by topic.</p>"},{"location":"#hpc-latest-news","title":"HPC latest News!","text":"<ul> <li> <p>HPC Spring Events, 2025</p> <p>ARCS HPC has hosted the following events this Spring. Please access the HPC training videos to view the webinar recordings and slides.</p> <ul> <li>Introduction to Wulver: Getting Started</li> <li>Introduction to Wulver: Accessing System &amp; Running Jobs</li> </ul> <p>We will soon update HPC Events with the upcoming events.</p> <p>Wulver Scheduled Maintenance</p> <p>Wulver will be out of service for maintenance once a month for updates, repairs, and upgrades.  The schedule is 9 a.m. to 9 p.m. the second Tuesday of every month.  During the maintenance period, the logins will be disabled and the jobs that do not end before the maintenance window begins will be held until the maintenance is complete and the cluster is returned to production. For example, if you submit a job the day before maintenance, your job will enter a pending state (you will see job status <code>PD</code> when using <code>squeue -u $LOGNAME</code>). You can either adjust the walltime or wait until maintenance ends. Please stay informed about maintenance updates at Cluster Maintenance Updates and News.</p> </li> <li> <p>Monthly HPC User Meeting</p> <p>We are currently offering a new monthly event for HPC researchers at NJIT: The HPC Monthly User Meeting. This event is open to all NJIT students, faculty, and staff who use or are interested in NJIT's HPC resources and services. No prior registration is required.</p> <ul> <li>Date: Wednesday, February 19</li> <li>Location: Campus Center, Room 235</li> <li>Time: 2:00 PM - 3:00 PM</li> </ul> <p>Open Office Hours</p> <p>This spring semester, we are offering drop-in office hours every Monday and Wednesday from 2:00 to 4:00 p.m. Stop by to meet with our student consultants and ask any questions you have about using HPC resources. There's no need to create a ticket in advance; if follow-up is needed, the student consultants will open a ticket on your behalf, and you'll receive further instructions. </p> <ul> <li>Date: Every Monday and Wednesday</li> <li>Location: GITC 2404</li> <li>Time: 2:00 PM - 4:00 PM</li> </ul> </li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/","title":"HPC Events","text":""},{"location":"HPC_Events_and_Workshops/Calendar/#2025-spring","title":"2025 Spring","text":""},{"location":"HPC_Events_and_Workshops/Calendar/#python-and-conda-environments-in-hpc-from-basics-to-best-practices","title":"Python and Conda Environments in HPC: From Basics to Best Practices","text":"<p>oin us for an informative webinar designed to offer a basic concept of using Python for High-Performance Computing (HPC) and effectively managing Python environments with Conda. This webinar will empower participants to leverage the power of Python for their scientific computing needs on HPC systems.</p> <p> Registration and Event Details</p>"},{"location":"HPC_Events_and_Workshops/Calendar/#monthly-hpc-user-meeting","title":"Monthly HPC User Meeting","text":"<p>We are currently offering a new monthly event for HPC researchers at NJIT: The HPC Monthly User Meeting. This event is open to all NJIT students, faculty, and staff who use or are interested in NJIT's HPC resources and services. This monthly meeting will be an opportunity for discussion, sharing ideas, talking with HPC facilitators, and learning about new resources and future plans for HPC research services. No prior registration is required.</p> <ul> <li>Date: Wednesday, February 19</li> <li>Location: Campus Center, Room 235</li> <li>Time: 2:00 PM - 3:00 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/#introduction-to-wulver-getting-started","title":"Introduction to Wulver: Getting Started","text":"<p>Join us for an informative webinar designed to introduce NJIT's HPC environment, Wulver. This virtual session will provide essential information about the Wulver cluster, how to get an account, and allocation details.</p> <p> Registration and Event Details</p>"},{"location":"HPC_Events_and_Workshops/Calendar/#introduction-to-wulver-accessing-system-running-jobs","title":"Introduction to Wulver: Accessing System &amp; Running Jobs","text":"<p>The HPC training event focuses on providing the fundamentals of SLURM (Simple Linux Utility for Resource Management), a workload manager. This virtual session will equip you with the essential skills needed to effectively utilize HPC resources using SLURM.</p> <p> Registration and Event Details</p>"},{"location":"HPC_Events_and_Workshops/Calendar/#2024-fall","title":"2024 Fall","text":""},{"location":"HPC_Events_and_Workshops/Calendar/#slurm-batch-system-basics","title":"SLURM Batch System Basics","text":"<p>Join us for an informative webinar designed to introduce researchers, scientists, and HPC users to the fundamentals of the SLURM (Simple Linux Utility for Resource Management) workload manager. This virtual  session will equip you with essential skills to effectively utilize HPC resources through SLURM.</p> <p> Registration and Event Details</p>"},{"location":"HPC_Events_and_Workshops/Calendar/#introduction-to-containers-on-wulver","title":"Introduction to Containers on Wulver","text":"<p>The HPC training event on using Singularity containers provides participants with a comprehensive introduction to container technology and its advantages in high-performance computing environments. Attendees will learn the fundamentals of Singularity, including installation, basic commands, and workflow, as well as how to create and build containers using definition files and existing Docker images.</p> <p> Registration and Event Details</p>"},{"location":"HPC_Events_and_Workshops/Calendar/#job-arrays-and-advanced-submission-techniques-for-hpc","title":"Job Arrays and Advanced Submission Techniques for HPC","text":"<p>Elevate your High-Performance Computing skills with our advanced SLURM webinar! This session is designed for HPC users who are familiar with basic SLURM commands and are ready to dive into more sophisticated job management techniques.</p> <p> Registration and Event Details</p>"},{"location":"HPC_Events_and_Workshops/Calendar/#intro-to-mpi-workshop","title":"Intro to MPI Workshop","text":"<p>This workshop is designed to provide C and Fortran programmers with a hands-on introduction to MPI programming. Participants will learn to write scalable code using MPI, the standard programming tool for scalable parallel computing</p> <p> Registration and Event Details</p>"},{"location":"HPC_Events_and_Workshops/Calendar/#2024-summer","title":"2024 Summer","text":""},{"location":"HPC_Events_and_Workshops/Calendar/#nvidia-workshop","title":"NVIDIA Workshop","text":""},{"location":"HPC_Events_and_Workshops/Calendar/#fundamentals-of-accelerated-data-science","title":"Fundamentals of Accelerated Data Science","text":"<p>Learn to use GPU-accelerated resources to analyze data. This is an intermediate level workshop that is intended for those who have some familiarity with Python, especially NumPy and SciPy libraries.</p> <p> Registration and Event Details</p>"},{"location":"HPC_Events_and_Workshops/Calendar/#hpc-research-symposium","title":"HPC Research Symposium","text":"<p>The introduction of our new shared HPC cluster, Wulver, has expanded our computational capacity and made the research into vital areas more accessible to our faculty. The Symposium will also feature several lightning talks from NJIT researchers highlighting use of High Performance Computing resources in their research.</p> <p> Registration and Event Details</p>"},{"location":"HPC_Events_and_Workshops/Calendar/#slurm-workload-manager-workshop","title":"SLURM Workload Manager Workshop","text":"<p>Advanced Research Computing Services, in collaboration with SchedMD, is hosting a two-day SLURM Workload Manager workshop. The workshop includes lectures, demos, and lab environments to enhance understanding of SLURM job scheduling, resource management, and troubleshooting.</p> <p> Registration and Event Details</p>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/1_slurm/","title":"SLURM Batch System Basics","text":"<p>Join us for an informative webinar designed to introduce researchers, scientists, and HPC users to the fundamentals of the SLURM (Simple Linux Utility for Resource Management) workload manager. This virtual  session will equip you with essential skills to effectively utilize HPC resources through SLURM.</p> <ul> <li>Date: Sep 18<sup>th</sup> 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/1_slurm/#topics-covered","title":"Topics Covered","text":"<ul> <li>Introduction to SLURM and its role in HPC environments</li> <li>Basic SLURM commands for job submission, monitoring, and management</li> <li>How to write effective job scripts for various application types</li> <li>Understanding SLURM partitions, quality of service, and job priorities</li> <li>Best practices for resource requests and job optimization</li> <li>Troubleshooting common issues in job submission and execution</li> </ul> <p>Our experienced HPC specialists will guide you through practical examples and provide tips for efficient use of SLURM in your research workflows. Whether you're new to HPC or looking to refine your SLURM skills, this webinar will help you maximize your productivity on SLURM-based clusters.</p>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/1_slurm/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/2_containers/","title":"Introduction to Containers on Wulver","text":"<p>The HPC training event on using Singularity containers provides participants with a comprehensive introduction to container technology and its advantages in high-performance computing environments.</p> <ul> <li>Date: Oct 16<sup>th</sup> 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/2_containers/#topics-covered","title":"Topics Covered","text":"<ul> <li>Introduction to containers and its role in HPC environments</li> <li>Fundamentals of Singularity, including installation, basic commands, and workflow</li> <li>Create and build containers using definition files and existing Docker images</li> <li>How to execute containerized applications on HPC clusters</li> <li>Use Containers via SLURM</li> <li>Performance optimization techniques</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/2_containers/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/3_slurm_advanced/","title":"Job Arrays and Advanced Submission Techniques for HPC","text":"<p>Elevate your High-Performance Computing skills with our advanced SLURM webinar! This session is designed for HPC users who are familiar with basic SLURM commands and are ready to dive into more sophisticated job management techniques.</p> <ul> <li>Date: Nov 20<sup>th</sup> 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/3_slurm_advanced/#topics-covered","title":"Topics Covered","text":""},{"location":"HPC_Events_and_Workshops/Calendar/2024/3_slurm_advanced/#job-arrays","title":"Job Arrays","text":"<ul> <li>Understanding the concept and benefits of job arrays</li> <li>Syntax for submitting and managing job arrays</li> <li>Best practices for efficient array job design</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/3_slurm_advanced/#advanced-job-submission-techniques","title":"Advanced Job Submission Techniques","text":"<ul> <li>Dependency chains and complex workflows</li> <li>Resource optimization strategies</li> <li>Using SLURM's advanced options for improved job control</li> <li>Techniques for balancing resource requests and job efficiency</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/3_slurm_advanced/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/4_nvidia/","title":"NVIDIA Workshop","text":""},{"location":"HPC_Events_and_Workshops/Calendar/2024/4_nvidia/#fundamentals-of-accelerated-data-science","title":"Fundamentals of Accelerated Data Science","text":"<p>Learn to use GPU-accelerated resources to analyze data. This is an intermediate level workshop that is intended for those who have some familiarity with Python, especially NumPy and SciPy libraries.</p> <ul> <li>Date: July 15<sup>th</sup> 2024</li> <li>Location: GITC 3700</li> <li>Time: 9 AM - 5 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/4_nvidia/#schedule","title":"Schedule","text":"Time Topic 9:00 AM - 9:15 AM Introduction 9:15 AM - 11:30 AM GPU-Accelerated Data Manipulation 11:30 AM - 12:00 PM GPU-Accelerated Machine Learning 12:00 PM - 1:00PM Lunch 1:00 PM - 2:30 PM GPU-Accelerated Machine Learning (contd) 2:45 PM - 4:45 PM Project: Data Analysis to Save the UK 4:45 PM - 5:00 PM Assessment and Q&amp;A <p>Coffee and Lunch will be provided. See more detail about the workshop here.</p> <p>Click here for the registration.</p>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/5_symposium/","title":"HPC Research Symposium","text":""},{"location":"HPC_Events_and_Workshops/Calendar/2024/5_symposium/#hpc-research-symposium","title":"HPC Research Symposium","text":"<p>This past year has been transformative for HPC research at NJIT. The introduction of our new shared HPC cluster, Wulver, has expanded our computational capacity and made research into vital areas more accessible to our faculty. The Advanced Research Computing Services group of Information Services and Technology, in collaboration with Dell Technologies, invites you to a symposium on July 16, 2024, to celebrate the launch of Wulver.</p> <p>The Symposium will feature a keynote from Anthony Dina, Global Field CTO for Unstructured Data Solutions at Dell Technologies, and invited speakers from NJIT, Dibakar Datta from the Department of Mechanical and Industrial Engineering, and Cambridge Computer Services, Jose Alvarez. The Symposium will also feature several lightning talks from NJIT researchers highlighting the use of High Performance Computing resources in their research.</p> <p>Please join us to learn how our researchers use HPC resources and connect with the NJIT HPC community.</p> <p>Please register for the symposium here.</p> <ul> <li>Date: July 16<sup>th</sup>, 2024</li> <li>Location: Campus Center Atrium</li> <li>Time: 9AM - 5PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/5_symposium/#agenda","title":"Agenda","text":"Time Session Title 9:00 - 9:15 Welcome Remarks Ed Wozencroft, Vice President for Digital Strategy &amp; CIO 9:15 - 9:35 Research Computing @ NJIT Overview of Research Computing Services Gedaliah Wolosh, Director High Performance Research Computing 9:40 - 10:30 Keynote It\u2019s Time Research Behaves More Like Formula 1 Anthony Dina, Global Field CTO for the Unstructured Data Solutions at Dell Technologies 10:30 - 10:40 Break 10:40 - 11:20 Invited Speaker Electro-Chemo-Mechanics of Multiscale Active Materials for Next-Generation Energy Storage Dibakar Datta, Associate Professor, Mechanical &amp; Industrial Engineering 11:20 - 12:00 Lightning Talks I Hemodynamics and Cancer Cell Transport in a Tortuous in vivo Microvessel Ali Kazempour, Peter Balogh Research Group, Mechanical &amp; Industrial Engineering Running Two-phase Flows on Wulver: Introduction to Basilisk Matthew Cho, Shahriar Afkhami Research Group, Mathematical Sciences Temporal Super-Resolution of Solar Images with Generative AI Jialing Li, Jason Wang Research Group, Computer Science Numerical study of Thermo-Marangoni flow induced by a warm plate Shivam Verma, Pushpendra Singh Research Group, Mechanical &amp; Industrial Engineering 12:10 - 13:00 Lunch 13:00 - 13:30 Invited Speaker Introduction to Grace Hopper and ARM Technology in Higher Education Jose Alvarez, Vice President Research Computing HPC/AI, Cambridge Computer Services 13:30- 14:20 Lightning Talks II Inference of Nullability Annotations using Machine Learning Kazi Amanul Islam Siddiqui, Martin Kellogg Research Group, Computer Science Deep Learning for Spatial Image Super-Resolution of Solar Observations Chunhui Xu, Jason Wang Research Group, Computer Science Volume Integral Method for Electromagnetic Equations Matthew Cassini, Thi Phong Nguyen Research Group, Mathematical Sciences Enhancing Region-based Image Captioning with Contextual Feature Exploitation Al Shahriar Rubel, Fadi Deek Research Group, Informatics Instability between the two-layer Poiseuille flow with the VOF method Nastaran Rezaei, Shahriar Afkhami Research Group, Mathematical Sciences 14:20 - 14:30 Break 14:30 - 15:00 Research Computing @ NJIT Introducing Open OnDemand web portal: New Technologies on Wulver Kate Cahill, Associate Director High Performance Research Computing 15:00 - 15:50 Lightning Talks III Red Blood Cell Modeling Reveals 3D Angiogenic Wall Shear Stress Patterns Mir Md Nasim Hossain, Peter Balogh Research Group, Mechanical &amp; Industrial Engineering Understanding and Forecasting Space Weather with Artificial Intelligence Hongyang Zhang, Jason Wang Research Group, Computer Science Entropic pressure on fluctuating solid membranes Rubayet Hassan, Fatemeh Ahmadpoor Research Group, Mechanical &amp; Industrial Engineering Investigation of PFAS Adsorption on Functionalized COF-300 Daniel Mottern, Joshua Young Research Group, Chemical &amp; Materials Engineering Large Language models (LLM) for hardware Deepak Vungarala, Shaahin Angizi Research Group, Electrical and Computer Engineering 15:50 - 16:00 Break 16:00 - 17:00 Poster Session Molecular Dynamic study on Ar nano-bubble stability in water Targol Teymourian, Jay Meegoda Research Group, Civil &amp; Environmental Engineering Possible mechanism for sonolytic degradation of PFAS Laura Nwanebu, Jay Meegoda Research Group, Civil &amp; Environmental Engineering Entropic Force Near Fluctuating Surface Rubayet Hassan, Fatemeh Ahmadpoor Research Group, Mechanical &amp; Industrial Engineering DFT investigations of the enantioselective phase-transfer-catalyzed aza-Michael cyclization of ureas Diana Marlen Castaneda Bagatella, Pier Champagne Research Group, Chemistry and Environmental Science"},{"location":"HPC_Events_and_Workshops/Calendar/2024/6_slurm_workshop/","title":"SLURM Workload Manager Workshop","text":"<p>Advanced Research Computing Services in collaboration with SchedMD is pleased to announce a two-day workshop on SLURM Workload Manager on August 13-14, 2024. This immersive 2-day experience will take you through comprehensive technical scenarios with lectures, demos, and workshop lab environments. The Slurm trainer will assist in identifying commonalities between previously used resources and schedulers, offering increased understanding and adoption of SLURM job scheduling, resource management, and troubleshooting techniques.</p> <p>Registration is now closed.</p>"},{"location":"HPC_Events_and_Workshops/Calendar/2024/7_Intro%20to%20MPI%20Workshop/","title":"Intro to MPI Workshop","text":"<p>NJIT is an in-person, satellite location for a two-day HPC workshop hosted by the Pittsburgh Supercomputing Center (PSC) on December 10 &amp; 11. This is a great introduction to using MPI programming to scale up your computational research. Attendees will leave with a working knowledge of how to write scalable codes using MPI \u2013 the standard programming tool of scalable parallel computing.</p> <p>Registration is now closed.</p> <p>To download the training materials, check MPI Workshop Agenda.</p>"},{"location":"HPC_Events_and_Workshops/Calendar/2025/1_intro_to_Wulver_I/","title":"Introduction to Wulver: Getting Started","text":"<p>This is the first webinar of the 2025 Spring semester introducing the NJIT HPC environment. This webinar provides basic information about our new High-Performance Computing (HPC) research cluster, Wulver.</p> <ul> <li>Date: Jan 22<sup>nd</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2025/1_intro_to_Wulver_I/#topics-covered","title":"Topics Covered","text":"<ul> <li>Introduction to HPC (High Performance Computing)</li> <li>Hardware and architecture of Wulver</li> <li>Guidance on how to obtain an account and login to the cluster</li> <li>Understanding allocations to utilize the shared resources</li> </ul> <p>Our experienced HPC specialists will guide you through practical examples and provide tips for proper use of cluster in your research workflows.</p>"},{"location":"HPC_Events_and_Workshops/Calendar/2025/1_intro_to_Wulver_I/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/Calendar/2025/2_intro_to_Wulver_II/","title":"Introduction to Wulver: Accessing System &amp; Running Jobs","text":"<p>This is the second webinar of the 2025 Spring semester introducing the NJIT HPC environment. This webinar provided the basic information in learning more about our new High Performance Computing (HPC) research cluster, Wulver.</p> <ul> <li>Date: Jan 29<sup>th</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2025/2_intro_to_Wulver_II/#topics-covered","title":"Topics Covered","text":"<ul> <li>HPC allocations</li> <li>How to access HPC software</li> <li>Introduction to SLURM and its role in HPC environments</li> <li>Basic SLURM commands for job submission, monitoring, and management</li> <li>Troubleshooting common issues in job submission and execution</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2025/2_intro_to_Wulver_II/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/Calendar/2025/3_conda_training/","title":"Python and Conda Environments in HPC: From Basics to Best Practices","text":"<p>This is the third webinar of the 2025 Spring semester introducing the NJIT HPC environment. Participants will gain an introductory understanding of using Python for HPC and effectively managing their Python environments using Conda. This knowledge will empower them to leverage the power of Python for their scientific computing needs on HPC systems.</p> <ul> <li>Date: Feb 26<sup>th</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2025/3_conda_training/#topics-covered","title":"Topics Covered","text":"<ul> <li>Learn how to manage Python environments using Conda.</li> <li>How to create Conda environments in different locations and install Python packages.</li> <li>Become familiar with common tools and libraries for scientific computing in Python.</li> </ul>"},{"location":"HPC_Events_and_Workshops/Calendar/2025/3_conda_training/#registration","title":"Registration","text":""},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/","title":"HPC Education and Training","text":"<p>NJIT HPC provides practical training in high performance computing for students and researchers at various levels of expertise. HPC training for research professionals aims to enhance their capabilities in utilizing high-performance computing, data-intensive computing, and data analytics within their respective research fields. </p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#training-programs","title":"Training Programs","text":""},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#2025-spring","title":"2025 Spring","text":""},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#introduction-to-wulver-accessing-system-running-jobs","title":"Introduction to Wulver: Accessing System &amp; Running Jobs","text":"<p>This is the second webinar of the 2025 Spring semester, focusing on job submission, monitoring, and management on Wulver. This webinar also provides common tips for troubleshooting issues that users may encounter during job submission.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#key-highlights","title":"Key Highlights:","text":"<ul> <li>How to Access HPC Software</li> <li>Introduction to SLURM and Its Role in HPC Environments </li> <li>Manage Slurm Jobs </li> <li>Troubleshooting Common Issues </li> <li>Slurm Interactive Jobs and Use GUI Apps</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#introduction-to-wulver-getting-started","title":"Introduction to Wulver: Getting Started","text":"<p>This is the first webinar of the 2025 Spring semester introducing the NJIT HPC environment. This webinar provides basic information about our new High-Performance Computing (HPC) research cluster, Wulver.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#key-highlights_1","title":"Key Highlights:","text":"<ul> <li>Introduction to HPC (High Performance Computing)</li> <li>Hardware and architecture of Wulver</li> <li>Guidance on how to obtain an account and login to the cluster </li> <li>Data Storage systems</li> <li>Understanding allocations to utilize the shared resources</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#2024-fall","title":"2024 Fall","text":""},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#job-arrays-and-advanced-submission-techniques-for-hpc","title":"Job Arrays and Advanced Submission Techniques for HPC","text":"<p>This is the final in a series of three webinars in the fall semester. designed to introduce researchers, scientists, and HPC users to the fundamentals of the containers. This session aims to provide useful information on submitting SLURM jobs efficiently by covering job arrays, job dependencies, checkpointing, and addressing common SLURM job issues.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#key-highlights_2","title":"Key Highlights:","text":"<ul> <li>Understanding the concept and benefits of job arrays</li> <li>Syntax for submitting and managing job arrays</li> <li>Best practices for efficient array job design</li> <li>Dependency chains and complex workflows</li> <li>Resource optimization strategies</li> <li>Using SLURM's advanced options for improved job control</li> <li>Checkpointing the jobs and use of 3<sup>rd</sup> party checkpointing tool</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#introduction-to-containers-on-wulver","title":"Introduction to Containers on Wulver","text":"<p>This is the second in a series of three webinars in the fall semester, designed to introduce researchers, scientists, and HPC users to the fundamentals of the containers. Attendees will learn the fundamentals of Singularity, including installation, basic commands, and workflow, as well as how to create and build containers using definition files and existing Docker images. The training will cover executing containerized applications on HPC clusters and integrating with job schedulers like SLURM, while also addressing security considerations and performance optimization techniques.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#key-highlights_3","title":"Key Highlights:","text":"<ul> <li>Introduction to containers and its role in HPC environments</li> <li>Fundamentals of Singularity, including installation, basic commands, and workflow</li> <li>Create and build containers using definition files and existing Docker images</li> <li>How to execute containerized applications on HPC clusters</li> <li>Use Containers via SLURM</li> <li>Performance optimization techniques</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#slurm-batch-system-basics","title":"SLURM Batch System Basics","text":"<p>This is the first in a series of three webinars in the fall semester. designed to introduce researchers, scientists, and HPC users to the fundamentals of the SLURM (Simple Linux Utility for Resource Management) workload manager. This virtual  session will equip you with essential skills to effectively utilize HPC resources through SLURM.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#key-highlights_4","title":"Key Highlights:","text":"<ul> <li>Introduction to SLURM and its role in HPC environments </li> <li>Basic SLURM commands for job submission, monitoring, and management </li> <li>How to write effective job scripts for various application types </li> <li>Understanding SLURM partitions, quality of service, and job priorities </li> <li>Best practices for resource requests and job optimization </li> <li>Troubleshooting common issues in job submission and execution</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#2024-spring","title":"2024 Spring","text":"<p>Since Wulver is quite different from the older cluster Lochness, the HPC training programs are designed to guide both new and existing users on how to use the new cluster. The following trainings provide the basic information on  </p> <ul> <li>Introduction to HPC </li> <li>Performance Optimization</li> <li>Job Submission and Management</li> <li>Managing Conda Environment </li> </ul> <p>If you still have any questions on HPC usage, please contact the HPC Facilitator.</p> <ul> <li> </li> </ul> <ul> <li> </li> </ul> <ul> <li> </li> </ul>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#getting-started-on-wulver-session-i","title":"Getting Started on Wulver: Session I","text":"<p>This is the first in a series of three webinars introducing the NJIT HPC environment. This webinar provided the basic information in learning more about our new High Performance Computing (HPC) research cluster, Wulver.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#key-highlights_5","title":"Key Highlights:","text":"<ul> <li>HPC concepts</li> <li>Hardware and architecture of the Wulver cluster</li> <li>Guidance on how to obtain an account and receive an allocation to utilize the shared resources. </li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#getting-started-on-wulver-session-ii","title":"Getting Started on Wulver: Session II","text":"<p>This session offered an overview of the environment on the Wulver cluster, including file management, working with the batch system (SLURM), and accessing software. </p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#key-highlights_6","title":"Key Highlights:","text":"<ul> <li>HPC allocations</li> <li>Using SLURM</li> <li>Job submissions</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#introduction-to-python-and-conda","title":"Introduction to Python and Conda","text":"<p>Participants will gain an introductory understanding of using Python for HPC and effectively managing their Python environments using Conda. This knowledge will empower them to leverage the power of Python for their scientific computing needs on HPC systems.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#key-highlights_7","title":"Key Highlights:","text":"<ul> <li>Learn how to manage Python environments for HPC using Conda.</li> <li>Become familiar with common tools and libraries for scientific computing in Python.</li> </ul> <p> Download Slides</p>"},{"location":"OnDemand/","title":"Open OnDemand","text":""},{"location":"OnDemand/#overview","title":"Overview","text":"<p>Open OnDemand is now available for NJIT HPC access at ondemand.njit.edu!</p> <p>Open OnDemand is a browser-based gateway to NJIT's Wulver cluster and shared storage. It offers a graphical interface allowing users to view, edit, download, and upload files. Users can manage and create job templates for the cluster and access interactive applications such as remote desktops to cluster nodes. Additionally, Open OnDemand supports GUI-based software like Jupyter Lab/Notebook, Matlab, and RStudio, all accessible through a web browser on virtually any device. No additional software installation is necessary, and users can operate with minimal Linux and job scheduler command knowledge.</p> <p>This is an open source project developed through NSF funding.</p>"},{"location":"OnDemand/#features","title":"Features","text":"<ul> <li>Easy to use</li> <li>Great for researchers and students new to HPC</li> <li>Convenient for experienced users as well</li> </ul>"},{"location":"OnDemand/#using-ondemand","title":"Using OnDemand","text":""},{"location":"OnDemand/#logging-into-ondemand","title":"Logging into OnDemand","text":"<p>If you have access to the Wulver cluster, you can use OnDemand. Open any browser and go to ondemand.njit.edu. Use your UCID and password to log in. If you are off campus, you will need to set up VPN to access the platform.</p>"},{"location":"OnDemand/#dashboard","title":"Dashboard","text":"<p>Once you log in, you will see the OnDemand Dashboard. You will see the menu bar on the top where you can access all the tools available including Files Manager, Shell Access, Job Composer, and Interactive Apps. You will also see several pinned apps highlighted on the Dashboard.</p>"},{"location":"OnDemand/#files","title":"Files","text":"<p>The File Manager tool is available under Files from the Dashboard. Here you can view, edit, and transfer files between your local computer and the cluster. You can access any of the shared filesystems on Wulver including your <code>$HOME</code> directory as well as Project, Research, and Scratch. This graphical interface makes it easy to navigate your directories and transfer files to the cluster. (This transfer is only for small files such as job scripts or input scripts, please use command line tools, such as rsync for larger datasets).</p> <p>Use the Upload and Download buttons to transfer files between your local computer and the cluster. You can navigate to any of your directories through the Change Directory button where you can enter the path for your desired location. You can also create new folders with the New Directory button.</p>"},{"location":"OnDemand/#jobs","title":"Jobs","text":"<p>The Jobs menu on the menu bar includes both the Job Composer and the Active Jobs tools. The Job Composer assists you to set up and submit jobs to the cluster through a graphical interface using file management tools and access to job templates. </p>"},{"location":"OnDemand/#job-composer","title":"Job Composer","text":""},{"location":"OnDemand/#active-jobs","title":"Active Jobs","text":"<p>The Active Jobs tool will allow you to view all the jobs you\u2019ve submitted that are currently in the queue, via OnDemand or not, so you can check on their status.  </p> <p></p> <p></p>"},{"location":"OnDemand/Apps/Matlab/","title":"MATLAB","text":""},{"location":"OnDemand/Apps/Matlab/#launching-matlab","title":"Launching MATLAB","text":"<ul> <li>Navigate to the Interactive Apps section.</li> <li>Select <code>MATLAB</code> from the list of available applications.</li> </ul>"},{"location":"OnDemand/Apps/Matlab/#loading-the-matlab-version","title":"Loading the Matlab Version","text":"<ul> <li>Select the dropdown option in <code>MATLAB Version</code>. The current versions installed on Wulver are <code>2023a</code> and <code>2024a</code></li> </ul>"},{"location":"OnDemand/Apps/Matlab/#configuring-resources","title":"Configuring Resources","text":"<ul> <li>Specify your Slurm account/partition/qos.</li> <li>Set the maximum wall time requested.</li> <li>Choose the number of cores you need.</li> <li>If required, specify the number of GPUs.</li> </ul>"},{"location":"OnDemand/Apps/Matlab/#launching-the-session","title":"Launching the Session","text":"<ul> <li>Select the <code>Launch</code> option after finalizing the resources. Once clicking Launch, the request will be queued, and when resources have been allocated, you will be presented with the option to connect to the session by clicking on the blue <code>Launch MATLAB</code> option.</li> </ul> <ul> <li>You might see <code>Unable to contact settings server</code> message. It does not mean the Matlab session is terminated. You need to wait a few minutes to see the Matlab popup window.</li> </ul>"},{"location":"OnDemand/Apps/Notebook/","title":"Jupyter","text":""},{"location":"OnDemand/Apps/Notebook/#launching-jupyter-session","title":"Launching Jupyter Session","text":"<ul> <li>Navigate to the Interactive Apps section.</li> <li>Select <code>Jupyter</code> from the list of available applications.</li> </ul>"},{"location":"OnDemand/Apps/Notebook/#loading-the-environment","title":"Loading the Environment","text":"<ul> <li> <p>Choose the <code>Mode</code> option, where you can select the interface:</p> <ul> <li>Jupyterlab </li> <li>Jupyter Notebook</li> </ul> </li> <li> <p>In the <code>Conda Environment</code> section, mention the environment name where you have installed Jupyter Notebook or Jupyterlab. For example, if the name of the environment is <code>torch_cuda</code>, then just type <code>torch_cuda</code> in the box. If you don't know how to install Jupyter Notebook or Jupyterlab in the Conda environment, check Conda Documentation and Jupyter Installation.</p> </li> <li> <p>Choose the path where you want to start the Jupyter session in <code>Enter the full path of the case directory</code>. For session in <code>$HOME</code> directory keep this blank. </p> </li> </ul> <p></p>"},{"location":"OnDemand/Apps/Notebook/#configuring-resources","title":"Configuring Resources","text":"<ul> <li>Specify your Slurm account/partition/qos.</li> <li>Set the maximum wall time requested.</li> <li>Choose the number of cores you need.</li> <li>If required, specify the number of GPUs.</li> </ul>"},{"location":"OnDemand/Apps/Notebook/#launching-the-session","title":"Launching the Session","text":"<p>Select the <code>Launch</code> option after finalizing the resources. Once clicking Launch, the request will be queued, and when resources have been allocated, you will be presented with the option to connect to the session by clicking on the blue <code>Connect to Jupyter</code> option.</p> <p></p> <p></p>"},{"location":"OnDemand/Apps/Rstudio/","title":"RStudio","text":""},{"location":"OnDemand/Apps/Rstudio/#launching-rstudio","title":"Launching RStudio","text":"<ul> <li>Navigate to the Interactive Apps section.</li> <li>Select <code>RStudio</code> from the list of available applications.</li> </ul>"},{"location":"OnDemand/Apps/Rstudio/#loading-the-r-environment","title":"Loading the R Environment","text":"<ul> <li>By default, <code>R 4.2.2</code> is loaded. If you want to use this version, you can proceed without any additional commands.</li> <li>To use a different version or environment, enter the necessary commands: For example, to use a Conda environment, enter: <code>module load Anaconda3/2023.09-0; source conda.sh; conda activate my_conda_r_env</code></li> <li>To use a different R environment, enter the following commands: <code>module load GCC/11.2.0 OpenMPI/4.1.1 R/4.2.0</code></li> </ul>"},{"location":"OnDemand/Apps/Rstudio/#configuring-resources","title":"Configuring Resources","text":"<ul> <li>Specify your Slurm account/partition/qos.</li> <li>Set the maximum wall time requested.</li> <li>Choose the number of cores you need.</li> <li>If required, specify the number of GPUs.</li> </ul>"},{"location":"OnDemand/Apps/Rstudio/#launching-the-session","title":"Launching the Session","text":"<p>Once clicking Launch, the request will be queued, and when resources have been allocated, you will be presented with the option to connect to the session by clicking on the blue Connect to R Studio Server button. </p> <p></p> <p></p> <p>Once connected, the familiar R Studio interface is presented, and you will be able to use the allocated resources, and access your research data located on Wulver. Installing packages It's likely your scripts will require additional R libraries; these can be installed using the <code>install.packages()</code> command in the console window of R Studio. </p>"},{"location":"OnDemand/Apps/Rstudio/#exiting-the-session","title":"Exiting the session","text":"<p>If a session exceeds the requested running time, it will be killed. You may receive a message \"The previous R session was abnormally terminated...\". Click OK to acknowledge the message and continue. To avoid this message, it's good practice to exit the session cleanly when you have finished. To cleanly exit R Studio, click <code>File -&gt; Quit Session...</code> and then release resources back to the cluster queues by clicking the red Delete button for the relevant session on the My Interactive Sessions page.</p>"},{"location":"Policies/lochness_policies/","title":"Lochness User Policies","text":"<p>Access and usages policies for the lochness cluster</p> <ul> <li> <p>Access</p> <ul> <li>Faculty members can request access to lochness for themselves and their students by sending an email to hpc@njit.edu.  There is no charge for using lochness.</li> <li>For courses, send an email to hpc@njit.edu to discuss requirements</li> </ul> </li> <li> <p>Storage</p> <ul> <li>Users are given a quota of 100GB</li> <li>Shared research directory with 500 GB quota is available upon request</li> </ul> </li> <li> <p>Scheduling</p> <ul> <li>The SLURM scheduler uses a \u201cfair share\u201d algorithm to assign priority to jobs. The \u201cfair share\u201d algorithm takes into account factors such as user history, wall time request, etc \u2026 to assign priority. The \u201cfair share\u201d algorithm is configure to favor smaller jobs. </li> <li>Limits<ul> <li>Wall time: 30 Days. Note that shorter wall time receive higher priority.</li> </ul> </li> <li>Cores/RAM: No limit but we request users be considerate. There are limited resources for public access.</li> </ul> </li> <li> <p>Condo</p> <ul> <li>Lochness has a private pool ownership model. Users buy nodes which are then incorporated into the cluster as a private partition.</li> </ul> </li> </ul> <p>Warning</p> <p>As of Jan 16, no new Lochness accounts will be created and Lochness will be decommissioned once the Wulver migration is complete.</p>"},{"location":"Policies/wulver_policies/","title":"Wulver Usage and Condo Policies","text":"<p>Proposed Wulver Usage and Condo Policies</p>"},{"location":"Policies/wulver_policies/#faculty-computing-allowance","title":"Faculty Computing Allowance","text":"<p>Faculty PIs are allocated 300,000 Service Units (SU) per year on request at no cost. An SU is equal to 1 core hour on a standard node. For more details on calculating SUs for GPUs, see Wulver SLURM. All users working as part of the PIs project will use this allocation. Multiple PIs working on the same project may pool SUs. The SUs can be renewed annually by providing a brief report describing how the SUs were used and a list of publications, presentations, and awards generated from research conducted. Additional SUs may be purchased at a cost of 0.005/SU. The minimum purchase is 50,000 SU (500). </p> <p>Note</p> <p>The 300,000 SUs are available on <code>--qos=standard</code> only. If PI does not want to buy more SUs, PI's group members can use <code>--qos=low</code> which does not have any SU charges. For more details, see SLURM QOS </p>"},{"location":"Policies/wulver_policies/#user-storage-allowance","title":"User Storage Allowance","text":"<p>Users will be provided with 50GB of <code>$HOME</code> directories. Home directories are backed up. PIs are additionally provided 2TB project directories. These project directories are backed up. Very fast NVME scratch is available to users. This scratch space is for temporary files generated during a run and will be deleted after 30 days. Additional project storage can be purchased if needed. This additional project space will also be backed up. Users need to manage data so that backed-up data fits in the project directory space. Transient, or rapidly changing data should be stored in the scratch directory. Long-term storage with backups or archival storage for research data will be stored in a yet to be determined campus wide storage resource.</p>"},{"location":"Policies/wulver_policies/#shared-condo-partnership","title":"Shared Condo Partnership","text":"<p>Faculty who routinely need more resources than the initial allocation may buy nodes and contribute to the cluster. A catalog of select hardware for inclusion in the cluster will be made available. The PI and associated users will be able to submit jobs with a higher priority up to the resources contributed. Note that these jobs may or may not run on the actual nodes purchased. The allocated resources will be available via a floating reservation for the amount of resources purchased. Contributors will be able to additionally submit jobs using SUs as well as lower priority. The university will subsidize all infrastructure costs for these nodes. This floating reservation will be available for five years.</p>"},{"location":"Policies/wulver_policies/#private-pool","title":"Private Pool","text":"<p>If the shared condo module does not satisfy the needs of the PI, a private pool may be set up. In addition to the nodes, the PI will be charged for all infrastructure costs, including but not limited to electricity, HVAC, system administration, etc. It is strongly recommended to first try the shared condo model. If the shared condo model does not work, the nodes can be converted to a private pool.</p>"},{"location":"Policies/wulver_policies/#job-priorities","title":"Job Priorities","text":"<ul> <li> <p>Standard Priority</p> <ul> <li>Faculty PIs are allocated 300,000 Service Units (SU) per year on request at no cost</li> <li>Wall time maximum - 72 hours</li> <li>Additional SUs may be purchased at a cost of $0.005/SU</li> <li>The minimum purchase is 50,000 SU ($500)</li> <li>Jobs can be superseded by those with higher priority jobs</li> </ul> </li> <li> <p>Low Priority</p> <ul> <li>Not charged against SU allocation</li> <li>Wall time maximum - 72 hours</li> <li>Jobs can be preempted by those with higher and standard priority jobs when they are in the queue</li> </ul> </li> <li> <p>High Priority</p> <ul> <li>Not charged against SU allocation</li> <li>Wall time: 72 hours (default), PI can request longer walltimes up to 30 days. ARCS HPC reserves the right to reboot nodes once a month for maintenance \u2014 the second Tuesday of each month. See Cluster Maintenance Updates and News for details</li> <li>Only available to contributors</li> </ul> </li> </ul>"},{"location":"Services/hpc-course/","title":"HPC Resources for Courses","text":""},{"location":"Services/hpc-course/#introduction","title":"Introduction","text":"<p>Instructors can utilize High-Performance Computing (HPC) resources for academic courses. Whether faculty are planning a course that involves computationally intensive tasks or introducing students to parallel computing concepts, the HPC environment can offer valuable resources.</p>"},{"location":"Services/hpc-course/#course-details","title":"Course Details","text":"<p>To request HPC resources for course, you need to provide the following information to hpc@njit.edu. </p>"},{"location":"Services/hpc-course/#name-of-course","title":"Name of Course:","text":"<p>[Please provide the name of the course and a short description]</p>"},{"location":"Services/hpc-course/#is-this-one-section-of-a-larger-course","title":"Is this one section of a larger course?","text":"<p>[Yes or No]</p>"},{"location":"Services/hpc-course/#estimated-number-of-students","title":"Estimated Number of Students:","text":"<p>[Enter the estimated number of students for the course]</p>"},{"location":"Services/hpc-course/#activities-on-hpc","title":"Activities on HPC:","text":"<p>[Describe the specific activities that will involve the use of HPC resources. For example, simulations, data analysis, modeling, etc.]</p>"},{"location":"Services/hpc-course/#software-needed","title":"Software Needed:","text":"<p>[Specify the software required for the course. Check the list of software installed on Wulver in Software. Include any specific versions or software not already available on the HPC cluster] </p> <p>Notice Period</p> <p>A minimum of 30 days' notice is required for requesting specific software installations or substantial resource allocations.</p>"},{"location":"Services/hpc-course/#utilizing-hpc-in-a-course","title":"Utilizing HPC in a Course","text":"<p>HPC resources are good for the courses if they satisfy the following requirements.</p> <ul> <li>Simulations and Modeling: Perform complex simulations and modeling exercises that require significant computational power.</li> <li>Data Analysis: Conduct large-scale data analysis projects, exploring real-world datasets with efficiency.</li> <li>Parallel Computing: Teach parallel computing concepts and applications by leveraging the cluster's parallel processing capabilities.</li> <li>Optimization Problems: Solve optimization problems that benefit from parallel processing and distributed computing.</li> <li>Scientific Research Projects: Enable students to work on scientific research projects that demand high-performance computing resources.</li> </ul>"},{"location":"Services/hpc-course/#hpc-introduction","title":"HPC Introduction","text":"<p>The HPC Facilitator is available to provide an introduction to High-Performance Computing. This introduction can be conducted in person or online based on the preferences and requirements of the course. The session covers:</p> <ul> <li>Overview of HPC concepts</li> <li>Accessing and navigating the HPC cluster</li> <li>Basic job submission and monitoring</li> <li>Filesystems on HPC cluster</li> <li>Conclusion</li> </ul> <p>By incorporating HPC resources into your course, you provide students with the opportunity to engage in hands-on, real-world applications of computational concepts. The HPC environment enhances the learning experience and prepares students for challenges in data-driven and computationally intensive fields. For specific requests or to schedule an HPC introduction session, please contact the HPC Facilitator.</p>"},{"location":"Software/","title":"Software Environment","text":"<p>All software and numerical libraries available at the cluster can be found at <code>/apps/easybuild/software/</code>. We use EasyBuild to install, build and manage different version of packages. </p> <p>If you could not find software or libraries on HPC cluster, please submit a request for HPC Software Installation by visiting the Service Catalog. The list of installed software or packages on HPC cluster can be found in Software List.</p>"},{"location":"Software/#modules","title":"Modules","text":"<p>We use Environment Modules to manage the user environment in HPC, which help users to easily load and unload software packages, switch between different versions of software, and manage complex software dependencies. Lmod is an extension of the Environment Modules system, implemented in Lua. It enhances the functionality of traditional Environment Modules by introducing features such as hierarchical module naming, module caching, and improved flexibility in managing environment variables.</p>"},{"location":"Software/#search-for-specific-package","title":"Search for Specific Package","text":"<p>You can check specific packages and list of their versions using <code>module spider</code>. For example, the list of Python installed on cluster can be checked by using</p> <p><pre><code>module spider Python\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n\n     Versions:\n        Python/2.7.18-bare\n        Python/3.9.6-bare\n        Python/3.9.6\n        Python/3.10.4-bare\n        Python/3.10.4\n        Python/3.10.8-bare\n        Python/3.10.8\n        Python/3.11.5\n     Other possible modules matches:\n        Biopython  Boost.Python  Python-bundle-PyPI  meson-python  python3  python39\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*Python.*'\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"Python\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider Python/3.11.5\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> This will show the different versions of Python available on Wulver. </p> <p>To see how to load the specific version of software (for example <code>Python/3.9.6</code>), the following command needs to be used.</p> <p><pre><code>module spider Python/3.9.6\n</code></pre> This will show the which prerequisite modules need to be loaded prior to loading <code>Python/3.9.6</code></p> <p><pre><code>---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  Python: Python/3.9.6\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n\n\n    You will need to load all module(s) on any one of the lines below before the \"Python/3.9.6\" module is available to load.\n\n      easybuild  GCCcore/11.2.0\n      slurm/wulver  GCCcore/11.2.0\n\n    Help:\n      Description\n      ===========\n      Python is a programming language that lets you work more quickly and integrate your systems\n       more effectively.\n\n\n      More information\n      ================\n       - Homepage: https://python.org/\n</code></pre> If you are unsure about the version, you can also use <code>module spider Python</code> to see the different versions of Python and prerequisite modules to be loaded. </p>"},{"location":"Software/#load-modules","title":"Load Modules","text":"<p>To use specific package, you need to use <code>module load</code> command which modified the environment to load the software package(s).</p> <p>Note</p> <ul> <li>The <code>module load</code> command will load dependencies automatically as needed, however you may still need to load prerequisite modules to load specific software package(s). For that you need to use <code>module spider</code> command as described above.</li> <li>For running jobs via batch script, you need to add module load command(s) to your submission script.</li> </ul> <p>For example, to load <code>Python</code> version <code>3.9.6</code> as shown in the above example, you need to load <code>GCCcore/11.2.0</code> module first before loading the Python module is available to load. To use <code>Python 3.9.6</code>, use the following command <pre><code>module load GCCcore/11.2.0 Python\n</code></pre> You can verify whether Python is loaded using</p> <p><pre><code>module li\n</code></pre> and this will result is the following output <pre><code>Currently Loaded Modules:\n  1) easybuild   2) wulver   3) slurm/wulver   4) null   5) GCCcore/11.2.0   6) zlib/1.2.11   7) binutils/2.37   8) bzip2/1.0.8   9) ncurses/6.2  10) libreadline/8.1  11) Tcl/8.6.11  12) SQLite/3.36  13) XZ/5.2.5  14) GMP/6.2.1  15) libffi/3.4.2  16) OpenSSL/1.1  17) Python/3.9.6\n</code></pre></p>"},{"location":"Software/#module-unload","title":"Module unload","text":"<p>To unload a specific module that you've previously loaded: <pre><code>module unload Python\n</code></pre> You can unload all modules at once with <pre><code>module purge\n</code></pre></p>"},{"location":"Software/#module-save-collections","title":"Module Save Collections","text":"<p>Since some package(s) require to load prerequisite modules to load, every time it might be inconvenient to users to load those modules everytime. Therefore, you can save those modules in particular environment after loading those modules. For example <pre><code>module save environment_name\n</code></pre> This will save the collection of  modules in <code>environment_name</code>. </p>"},{"location":"Software/#module-list-collections","title":"Module List Collections","text":"<p>To get a list of your collections, run: <pre><code>module savelist\n</code></pre></p>"},{"location":"Software/#module-command-summary","title":"Module Command Summary","text":"<p>Here are the list of <code>module</code> commands</p> Command Description <code>module list</code> Show active modules loaded in the user environment <code>module av [module]</code> Show list of available modules in MODULEPATH <code>module spider [module]</code> Query all modules in MODULEPATH and any module hierarchy <code>module overview [module]</code> List all modules with count of each module <code>module load [module]</code> Load a module file in the users environment <code>module unload [module]</code> Remove a loaded module from the user environment <code>module purge</code> Remove all modules from the user environment <code>module swap [module1] [module2]</code> Replace module1 with module2 <code>module show [module]</code> Show content of commands performed by loading module file <code>module --raw show [module]</code> Show raw content of module file <code>module help [module]</code> Show help for a given module <code>module whatis [module]</code> A brief description of the module, generally single line <code>module savelist</code> List all user collections <code>module save [collection]</code> Save active modules in a user collection <code>module describe [collection]</code> Show content of user collection <code>module restore [collection]</code> Load modules from a collection <code>module disable [collection]</code> Disable a user collection <code>module --config</code> Show Lmod configuration <code>module use [-a] [path]</code> Prepend or Append path to MODULEPATH <code>module unuse [path]</code> Remove path from MODULEPATH <code>module --show_hidden av</code> Show all available modules in MODULEPATH including hidden modules <code>module --show_hidden spider</code> Show all possible modules in MODULEPATH and module hierarchy including hidden modules"},{"location":"Software/#further-reading","title":"Further Reading","text":"<p>You can check the documentation of <code>module</code> on the cluster using the following command: <pre><code>man module\n</code></pre></p>"},{"location":"Software/#software-list","title":"Software List","text":"<p>The following applications are installed on Wulver and Lochness.</p> Wulver Software Version Dependent Toolchain Module Load Command ACTC 1.1 foss/2021b <code>module load foss/2021b ACTC/1.1</code> ACTC 1.1 foss/2022b <code>module load foss/2022b ACTC/1.1</code> AFNI 23.0.04-Python-3.9.6 foss/2021b <code>module load foss/2021b AFNI/23.0.04-Python-3.9.6</code> ANSYS 2023R1 foss/2021b <code>module load foss/2021b ANSYS/2023R1</code> ANSYS 2022 - <code>module load ANSYS/2022</code> ATK 2.38.0 foss/2022b <code>module load foss/2022b ATK/2.38.0</code> Abseil 20240116.1 foss/2023b <code>module load foss/2023b Abseil/20240116.1</code> Altair 2023 - <code>module load Altair/2023</code> Anaconda3 2023.09-0 - <code>module load Anaconda3/2023.09-0</code> Anaconda3 5.3.0 - <code>module load Anaconda3/5.3.0</code> Arb 2.23.0 foss/2022b <code>module load foss/2022b Arb/2.23.0</code> Armadillo 11.4.3 foss/2022b <code>module load foss/2022b Armadillo/11.4.3</code> Arrow 11.0.0 foss/2022b <code>module load foss/2022b Arrow/11.0.0</code> AutoDock-GPU 1.5.3-CUDA-11.4.1 foss/2021b <code>module load foss/2021b AutoDock-GPU/1.5.3-CUDA-11.4.1</code> Autoconf 2.71 foss/2022b <code>module load foss/2022b Autoconf/2.71</code> Autoconf 2.71 foss/2023b <code>module load foss/2023b Autoconf/2.71</code> Autoconf 2.71 foss/2020a <code>module load foss/2020a Autoconf/2.71</code> Autoconf 2.71 foss/2021b <code>module load foss/2021b Autoconf/2.71</code> Autoconf 2.71 - <code>module load Autoconf/2.71</code> Automake 1.16.5 - <code>module load Automake/1.16.5</code> Automake 1.16.4 foss/2021b <code>module load foss/2021b Automake/1.16.4</code> Automake 1.16.5 foss/2020a <code>module load foss/2020a Automake/1.16.5</code> Automake 1.16.5 foss/2022b <code>module load foss/2022b Automake/1.16.5</code> Automake 1.16.5 foss/2023b <code>module load foss/2023b Automake/1.16.5</code> Autotools 20220317 - <code>module load Autotools/20220317</code> Autotools 20220317 foss/2020a <code>module load foss/2020a Autotools/20220317</code> Autotools 20210726 foss/2021b <code>module load foss/2021b Autotools/20210726</code> Autotools 20220317 foss/2022b <code>module load foss/2022b Autotools/20220317</code> Autotools 20220317 foss/2023b <code>module load foss/2023b Autotools/20220317</code> Avogadro2 1.97.0-linux-x86_64 - <code>module load Avogadro2/1.97.0-linux-x86_64</code> BLIS 0.9.0 foss/2023b <code>module load foss/2023b BLIS/0.9.0</code> BLIS 0.9.0 foss/2022b <code>module load foss/2022b BLIS/0.9.0</code> BLIS 0.8.1 foss/2021b <code>module load foss/2021b BLIS/0.8.1</code> BeautifulSoup 4.12.2 foss/2023b <code>module load foss/2023b BeautifulSoup/4.12.2</code> Biopython 1.79 foss/2021b <code>module load foss/2021b Biopython/1.79</code> Biopython 1.81 foss/2022b <code>module load foss/2022b Biopython/1.81</code> Bison 3.8.2 foss/2020a <code>module load foss/2020a Bison/3.8.2</code> Bison 3.7.6 foss/2021b <code>module load foss/2021b Bison/3.7.6</code> Bison 3.8.2 foss/2023b <code>module load foss/2023b Bison/3.8.2</code> Bison 3.8.2 - <code>module load Bison/3.8.2</code> Bison 3.8.2 foss/2022b <code>module load foss/2022b Bison/3.8.2</code> Blosc 1.21.3 foss/2022b <code>module load foss/2022b Blosc/1.21.3</code> Blosc2 2.8.0 foss/2022b <code>module load foss/2022b Blosc2/2.8.0</code> Boost 1.79.0 foss/2021b <code>module load foss/2021b Boost/1.79.0</code> Boost 1.83.0 foss/2023b <code>module load foss/2023b Boost/1.83.0</code> Boost 1.81.0 foss/2022b <code>module load foss/2022b Boost/1.81.0</code> Boost 1.77.0 foss/2021b <code>module load foss/2021b Boost/1.77.0</code> Boost 1.77.0 intel/2021b <code>module load intel/2021b Boost/1.77.0</code> Boost.MPI 1.77.0 foss/2021b <code>module load foss/2021b Boost.MPI/1.77.0</code> Boost.Python 1.77.0 foss/2021b <code>module load foss/2021b Boost.Python/1.77.0</code> Brotli 1.1.0 foss/2023b <code>module load foss/2023b Brotli/1.1.0</code> Brotli 1.0.9 foss/2021b <code>module load foss/2021b Brotli/1.0.9</code> Brotli 1.0.9 foss/2022b <code>module load foss/2022b Brotli/1.0.9</code> Brunsli 0.1 foss/2022b <code>module load foss/2022b Brunsli/0.1</code> CFITSIO 4.2.0 foss/2022b <code>module load foss/2022b CFITSIO/4.2.0</code> CGAL 5.5.2 foss/2022b <code>module load foss/2022b CGAL/5.5.2</code> CGAL 4.14.3 foss/2021b <code>module load foss/2021b CGAL/4.14.3</code> CMake 3.22.1 foss/2021b <code>module load foss/2021b CMake/3.22.1</code> CMake 3.24.3 foss/2022b <code>module load foss/2022b CMake/3.24.3</code> CMake 3.27.6 foss/2023b <code>module load foss/2023b CMake/3.27.6</code> CMake 3.21.1 foss/2021b <code>module load foss/2021b CMake/3.21.1</code> CMake 3.23.1 foss/2020a <code>module load foss/2020a CMake/3.23.1</code> CMake 3.29.3 foss/2023b <code>module load foss/2023b CMake/3.29.3</code> COMSOL 6.2 - <code>module load COMSOL/6.2</code> COMSOL 5.6 - <code>module load COMSOL/5.6</code> CP2K 2023.1 foss/2022b <code>module load foss/2022b CP2K/2023.1</code> CP2K 8.2 intel/2021b <code>module load intel/2021b CP2K/8.2</code> CREST 2.12 foss/2022b <code>module load foss/2022b CREST/2.12</code> CUDA 11.4.1 - <code>module load CUDA/11.4.1</code> CUDA 12.4.0 - <code>module load CUDA/12.4.0</code> CUDA 12.0.0 - <code>module load CUDA/12.0.0</code> Catch2 2.13.9 foss/2023b <code>module load foss/2023b Catch2/2.13.9</code> Cbc 2.10.11 foss/2023b <code>module load foss/2023b Cbc/2.10.11</code> Cgl 0.60.8 foss/2023b <code>module load foss/2023b Cgl/0.60.8</code> Chapel 2.1.0 foss/2023b <code>module load foss/2023b Chapel/2.1.0</code> Clang 12.0.1 foss/2021b <code>module load foss/2021b Clang/12.0.1</code> Clp 1.17.9 foss/2023b <code>module load foss/2023b Clp/1.17.9</code> CoinUtils 2.11.10 foss/2023b <code>module load foss/2023b CoinUtils/2.11.10</code> DB 18.1.40 foss/2020a <code>module load foss/2020a DB/18.1.40</code> DB 18.1.40 foss/2021b <code>module load foss/2021b DB/18.1.40</code> DB 18.1.40 foss/2022b <code>module load foss/2022b DB/18.1.40</code> DBus 1.15.8 foss/2023b <code>module load foss/2023b DBus/1.15.8</code> DBus 1.15.2 foss/2022b <code>module load foss/2022b DBus/1.15.2</code> DBus 1.13.18 foss/2021b <code>module load foss/2021b DBus/1.13.18</code> DMTCP 3.0.0 - <code>module load DMTCP/3.0.0</code> DMTCP 2.6.0 - <code>module load DMTCP/2.6.0</code> Doxygen 1.9.5 foss/2022b <code>module load foss/2022b Doxygen/1.9.5</code> Doxygen 1.9.1 foss/2021b <code>module load foss/2021b Doxygen/1.9.1</code> Doxygen 1.9.8 foss/2023b <code>module load foss/2023b Doxygen/1.9.8</code> ELPA 2021.05.001 foss/2021b <code>module load foss/2021b ELPA/2021.05.001</code> ELPA 2021.11.001 intel/2021b <code>module load intel/2021b ELPA/2021.11.001</code> EasyBuild 4.9.0 - <code>module load EasyBuild/4.9.0</code> EasyBuild 4.9.1 - <code>module load EasyBuild/4.9.1</code> EasyBuild 4.8.0 - <code>module load EasyBuild/4.8.0</code> EasyBuild 4.8.2 - <code>module load EasyBuild/4.8.2</code> Eigen 3.4.0 foss/2021b <code>module load foss/2021b Eigen/3.4.0</code> Eigen 3.4.0 foss/2022b <code>module load foss/2022b Eigen/3.4.0</code> Eigen 3.3.9 foss/2021b <code>module load foss/2021b Eigen/3.3.9</code> Eigen 3.4.0 foss/2023b <code>module load foss/2023b Eigen/3.4.0</code> Eigen 3.4.0 foss/2020a <code>module load foss/2020a Eigen/3.4.0</code> FFTW 3.3.10 foss/2023b <code>module load foss/2023b FFTW/3.3.10</code> FFTW 3.3.10 foss/2022b <code>module load foss/2022b FFTW/3.3.10</code> FFTW 3.3.10 foss/2021b <code>module load foss/2021b FFTW/3.3.10</code> FFTW.MPI 3.3.10 foss/2022b <code>module load foss/2022b FFTW.MPI/3.3.10</code> FFTW.MPI 3.3.10 foss/2023b <code>module load foss/2023b FFTW.MPI/3.3.10</code> FFmpeg 6.1.1 foss/2022b <code>module load foss/2022b FFmpeg/6.1.1</code> FFmpeg 4.3.2 foss/2021b <code>module load foss/2021b FFmpeg/4.3.2</code> FFmpeg 5.1.2 foss/2022b <code>module load foss/2022b FFmpeg/5.1.2</code> FLAC 1.3.3 foss/2021b <code>module load foss/2021b FLAC/1.3.3</code> FLAC 1.4.2 foss/2022b <code>module load foss/2022b FLAC/1.4.2</code> FLINT 2.9.0 foss/2022b <code>module load foss/2022b FLINT/2.9.0</code> FLINT 3.0.1 foss/2022b <code>module load foss/2022b FLINT/3.0.1</code> FLTK 1.3.8 foss/2022b <code>module load foss/2022b FLTK/1.3.8</code> FLTK 1.3.7 foss/2021b <code>module load foss/2021b FLTK/1.3.7</code> FlexiBLAS 3.2.1 foss/2022b <code>module load foss/2022b FlexiBLAS/3.2.1</code> FlexiBLAS 3.0.4 foss/2021b <code>module load foss/2021b FlexiBLAS/3.0.4</code> FlexiBLAS 3.3.1 foss/2023b <code>module load foss/2023b FlexiBLAS/3.3.1</code> FriBidi 1.0.10 foss/2021b <code>module load foss/2021b FriBidi/1.0.10</code> FriBidi 1.0.12 foss/2022b <code>module load foss/2022b FriBidi/1.0.12</code> GCC 11.2.0 foss/2021b <code>module load GCC/11.2.0</code> GCC 12.2.0 foss/2022b <code>module load GCC/12.2.0</code> GCC 13.2.0 foss/2023b <code>module load GCC/13.2.0</code> GCCcore 12.2.0 - <code>module load GCCcore/12.2.0</code> GCCcore 11.2.0 - <code>module load GCCcore/11.2.0</code> GCCcore 13.2.0 - <code>module load GCCcore/13.2.0</code> GCCcore 11.3.0 - <code>module load GCCcore/11.3.0</code> GDAL 3.3.2 foss/2021b <code>module load foss/2021b GDAL/3.3.2</code> GDAL 3.6.2 foss/2022b <code>module load foss/2022b GDAL/3.6.2</code> GDRCopy 2.3 foss/2022b <code>module load foss/2022b GDRCopy/2.3</code> GDRCopy 2.3 foss/2021b <code>module load foss/2021b GDRCopy/2.3</code> GEOS 3.9.1 foss/2021b <code>module load foss/2021b GEOS/3.9.1</code> GEOS 3.11.1 foss/2022b <code>module load foss/2022b GEOS/3.11.1</code> GLPK 5.0 foss/2021b <code>module load foss/2021b GLPK/5.0</code> GLPK 5.0 foss/2023b <code>module load foss/2023b GLPK/5.0</code> GLPK 5.0 foss/2022b <code>module load foss/2022b GLPK/5.0</code> GLib 2.69.1 foss/2021b <code>module load foss/2021b GLib/2.69.1</code> GLib 2.75.0 foss/2022b <code>module load foss/2022b GLib/2.75.0</code> GLib 2.78.1 foss/2023b <code>module load foss/2023b GLib/2.78.1</code> GMP 6.3.0 foss/2023b <code>module load foss/2023b GMP/6.3.0</code> GMP 6.2.1 foss/2021b <code>module load foss/2021b GMP/6.2.1</code> GMP 6.2.1 foss/2022b <code>module load foss/2022b GMP/6.2.1</code> GMP 6.2.1 foss/2020a <code>module load foss/2020a GMP/6.2.1</code> GObject-Introspection 1.78.1 foss/2023b <code>module load foss/2023b GObject-Introspection/1.78.1</code> GObject-Introspection 1.68.0 foss/2021b <code>module load foss/2021b GObject-Introspection/1.68.0</code> GObject-Introspection 1.74.0 foss/2022b <code>module load foss/2022b GObject-Introspection/1.74.0</code> GROMACS 2024.1 foss/2023b <code>module load foss/2023b GROMACS/2024.1</code> GROMACS 2023.1-CUDA-12.0.0 foss/2022b <code>module load foss/2022b GROMACS/2023.1-CUDA-12.0.0</code> GROMACS 2024.4-CUDA-12.4.0 foss/2022b <code>module load foss/2022b GROMACS/2024.4-CUDA-12.4.0</code> GROMACS 2021.5-CUDA-11.4.1 foss/2021b <code>module load foss/2021b GROMACS/2021.5-CUDA-11.4.1</code> GROMACS 2021.5 foss/2021b <code>module load foss/2021b GROMACS/2021.5</code> GSL 2.7 foss/2022b <code>module load foss/2022b GSL/2.7</code> GSL 2.7 foss/2022b <code>module load foss/2022b GSL/2.7</code> GSL 2.7 foss/2021b <code>module load foss/2021b GSL/2.7</code> GSL 2.7 intel/2021b <code>module load intel/2021b GSL/2.7</code> GST-plugins-base 1.22.1 foss/2022b <code>module load foss/2022b GST-plugins-base/1.22.1</code> GStreamer 1.22.1 foss/2022b <code>module load foss/2022b GStreamer/1.22.1</code> GTK3 3.24.35 foss/2022b <code>module load foss/2022b GTK3/3.24.35</code> Gaussian 16.C.01-AVX2 - <code>module load Gaussian/16.C.01-AVX2</code> Gdk-Pixbuf 2.42.10 foss/2022b <code>module load foss/2022b Gdk-Pixbuf/2.42.10</code> Ghostscript 9.54.0 foss/2021b <code>module load foss/2021b Ghostscript/9.54.0</code> Ghostscript 10.0.0 foss/2022b <code>module load foss/2022b Ghostscript/10.0.0</code> Go 1.17.3 - <code>module load Go/1.17.3</code> Go 1.17.6 - <code>module load Go/1.17.6</code> Grace 5.1.25 foss/2022b <code>module load foss/2022b Grace/5.1.25</code> Graphene 1.10.8 foss/2022b <code>module load foss/2022b Graphene/1.10.8</code> Gurobi 11.0.1 foss/2023b <code>module load foss/2023b Gurobi/11.0.1</code> Gurobi 10.0.1 foss/2022b <code>module load foss/2022b Gurobi/10.0.1</code> Gurobi_license license - <code>module load Gurobi_license/license</code> HDF 4.2.15 foss/2022b <code>module load foss/2022b HDF/4.2.15</code> HDF 4.2.15 foss/2021b <code>module load foss/2021b HDF/4.2.15</code> HDF5 1.14.3 intel/2023b <code>module load intel/2023b HDF5/1.14.3</code> HDF5 1.12.1 intel/2021b <code>module load intel/2021b HDF5/1.12.1</code> HDF5 1.14.3 foss/2023b <code>module load foss/2023b HDF5/1.14.3</code> HDF5 1.14.0 intel/2022b <code>module load intel/2022b HDF5/1.14.0</code> HDF5 1.14.0 foss/2022b <code>module load foss/2022b HDF5/1.14.0</code> HDF5 1.12.1 foss/2021b <code>module load foss/2021b HDF5/1.12.1</code> HPL 2.3 foss/2022b <code>module load foss/2022b HPL/2.3</code> HPL 2.3 intel/2022b <code>module load intel/2022b HPL/2.3</code> HarfBuzz 2.8.2 foss/2021b <code>module load foss/2021b HarfBuzz/2.8.2</code> HarfBuzz 8.2.2 foss/2023b <code>module load foss/2023b HarfBuzz/8.2.2</code> HarfBuzz 5.3.1 foss/2022b <code>module load foss/2022b HarfBuzz/5.3.1</code> HiGHS 1.7.0 foss/2023b <code>module load foss/2023b HiGHS/1.7.0</code> Highway 1.0.3 foss/2022b <code>module load foss/2022b Highway/1.0.3</code> Hypre 2.27.0 foss/2022b <code>module load foss/2022b Hypre/2.27.0</code> ICU 69.1 foss/2021b <code>module load foss/2021b ICU/69.1</code> ICU 74.1 foss/2023b <code>module load foss/2023b ICU/74.1</code> ICU 72.1 foss/2022b <code>module load foss/2022b ICU/72.1</code> IPython 8.17.2 foss/2023b <code>module load foss/2023b IPython/8.17.2</code> IQ-TREE 2.2.2.6 foss/2022b <code>module load foss/2022b IQ-TREE/2.2.2.6</code> ImageMagick 7.1.0-53 foss/2022b <code>module load foss/2022b ImageMagick/7.1.0-53</code> ImageMagick 7.1.0-4 foss/2021b <code>module load foss/2021b ImageMagick/7.1.0-4</code> Imath 3.1.6 foss/2022b <code>module load foss/2022b Imath/3.1.6</code> JasPer 2.0.33 foss/2021b <code>module load foss/2021b JasPer/2.0.33</code> JasPer 4.0.0 foss/2023b <code>module load foss/2023b JasPer/4.0.0</code> JasPer 4.0.0 foss/2022b <code>module load foss/2022b JasPer/4.0.0</code> Java 11.0.16 - <code>module load Java/11.0.16</code> Java .modulerc - <code>module load Java/.modulerc</code> JupyterLab 4.2.0 foss/2023b <code>module load foss/2023b JupyterLab/4.2.0</code> JupyterNotebook 7.2.0 foss/2023b <code>module load foss/2023b JupyterNotebook/7.2.0</code> KaHIP 3.14 foss/2022b <code>module load foss/2022b KaHIP/3.14</code> LAME 3.100 foss/2021b <code>module load foss/2021b LAME/3.100</code> LAME 3.100 foss/2022b <code>module load foss/2022b LAME/3.100</code> LAMMPS 23Jun2022-kokkos foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos</code> LAMMPS 23Jun2022-kokkos-CUDA-11.4.1 foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos-CUDA-11.4.1</code> LEMON 1.3.1 foss/2023b <code>module load foss/2023b LEMON/1.3.1</code> LERC 4.0.0 foss/2022b <code>module load foss/2022b LERC/4.0.0</code> LIGGGHTS-PUBLIC 3.8.0-kokkos-CUDA-11.4.1 foss/2021b <code>module load foss/2021b LIGGGHTS-PUBLIC/3.8.0-kokkos-CUDA-11.4.1</code> LLVM 12.0.1 foss/2021b <code>module load foss/2021b LLVM/12.0.1</code> LLVM 16.0.6 foss/2023b <code>module load foss/2023b LLVM/16.0.6</code> LLVM 15.0.5 foss/2022b <code>module load foss/2022b LLVM/15.0.5</code> LSD2 2.4.1 foss/2022b <code>module load foss/2022b LSD2/2.4.1</code> LZO 2.10 foss/2022b <code>module load foss/2022b LZO/2.10</code> LibTIFF 4.4.0 foss/2022b <code>module load foss/2022b LibTIFF/4.4.0</code> LibTIFF 4.3.0 foss/2021b <code>module load foss/2021b LibTIFF/4.3.0</code> LibTIFF 4.6.0 foss/2023b <code>module load foss/2023b LibTIFF/4.6.0</code> Libint 2.7.2-lmax-6-cp2k foss/2022b <code>module load foss/2022b Libint/2.7.2-lmax-6-cp2k</code> Libint 2.7.2-lmax-6-cp2k intel/2021b <code>module load intel/2021b Libint/2.7.2-lmax-6-cp2k</code> LittleCMS 2.15 foss/2023b <code>module load foss/2023b LittleCMS/2.15</code> LittleCMS 2.14 foss/2022b <code>module load foss/2022b LittleCMS/2.14</code> LittleCMS 2.12 foss/2021b <code>module load foss/2021b LittleCMS/2.12</code> Lua 5.4.3 foss/2021b <code>module load foss/2021b Lua/5.4.3</code> Lua 5.4.4 foss/2022b <code>module load foss/2022b Lua/5.4.4</code> M4 1.4.19 foss/2022b <code>module load foss/2022b M4/1.4.19</code> M4 1.4.19 - <code>module load M4/1.4.19</code> M4 1.4.19 foss/2020a <code>module load foss/2020a M4/1.4.19</code> M4 1.4.19 foss/2021b <code>module load foss/2021b M4/1.4.19</code> M4 1.4.19 foss/2023b <code>module load foss/2023b M4/1.4.19</code> M4 1.4.18 - <code>module load M4/1.4.18</code> MATLAB 2023a - <code>module load MATLAB/2023a</code> MATLAB 2024a - <code>module load MATLAB/2024a</code> MDAnalysis 2.0.0 foss/2021b <code>module load foss/2021b MDAnalysis/2.0.0</code> MDAnalysis 2.4.2 foss/2022b <code>module load foss/2022b MDAnalysis/2.4.2</code> METIS 5.1.0 foss/2021b <code>module load foss/2021b METIS/5.1.0</code> METIS 5.1.0 foss/2023b <code>module load foss/2023b METIS/5.1.0</code> METIS 5.1.0 foss/2022b <code>module load foss/2022b METIS/5.1.0</code> MPAS 8.0.1 foss/2022b <code>module load foss/2022b MPAS/8.0.1</code> MPFR 4.2.0 foss/2022b <code>module load foss/2022b MPFR/4.2.0</code> MPFR 4.1.0 foss/2021b <code>module load foss/2021b MPFR/4.1.0</code> MUMPS 5.6.1-metis foss/2023b <code>module load foss/2023b MUMPS/5.6.1-metis</code> Mako 1.1.4 foss/2021b <code>module load foss/2021b Mako/1.1.4</code> Mako 1.2.4 foss/2022b <code>module load foss/2022b Mako/1.2.4</code> Mako 1.2.4 foss/2023b <code>module load foss/2023b Mako/1.2.4</code> Mamba 4.14.0-0 - <code>module load Mamba/4.14.0-0</code> Maven 3.6.3 - <code>module load Maven/3.6.3</code> Mesa 21.1.7 foss/2021b <code>module load foss/2021b Mesa/21.1.7</code> Mesa 22.2.4 foss/2022b <code>module load foss/2022b Mesa/22.2.4</code> Mesa 23.1.9 foss/2023b <code>module load foss/2023b Mesa/23.1.9</code> Meson 0.64.0 foss/2022b <code>module load foss/2022b Meson/0.64.0</code> Meson 0.58.2 foss/2021b <code>module load foss/2021b Meson/0.58.2</code> Meson 1.2.3 foss/2023b <code>module load foss/2023b Meson/1.2.3</code> Miniforge3 24.1.2-0 - <code>module load Miniforge3/24.1.2-0</code> NASM 2.16.01 foss/2023b <code>module load foss/2023b NASM/2.16.01</code> NASM 2.15.05 foss/2021b <code>module load foss/2021b NASM/2.15.05</code> NASM 2.15.05 foss/2022b <code>module load foss/2022b NASM/2.15.05</code> NCCL 2.10.3-CUDA-11.4.1 foss/2021b <code>module load foss/2021b NCCL/2.10.3-CUDA-11.4.1</code> NLopt 2.7.1 foss/2022b <code>module load foss/2022b NLopt/2.7.1</code> NLopt 2.7.0 foss/2021b <code>module load foss/2021b NLopt/2.7.0</code> NSPR 4.35 foss/2022b <code>module load foss/2022b NSPR/4.35</code> NSPR 4.35 foss/2023b <code>module load foss/2023b NSPR/4.35</code> NSPR 4.32 foss/2021b <code>module load foss/2021b NSPR/4.32</code> NSS 3.69 foss/2021b <code>module load foss/2021b NSS/3.69</code> NSS 3.94 foss/2023b <code>module load foss/2023b NSS/3.94</code> NSS 3.85 foss/2022b <code>module load foss/2022b NSS/3.85</code> NTL 11.5.1 foss/2022b <code>module load foss/2022b NTL/11.5.1</code> NVHPC 23.1-CUDA-12.0.0 - <code>module load NVHPC/23.1-CUDA-12.0.0</code> Ninja 1.11.1 foss/2022b <code>module load foss/2022b Ninja/1.11.1</code> Ninja 1.10.2 foss/2021b <code>module load foss/2021b Ninja/1.10.2</code> Ninja 1.11.1 foss/2023b <code>module load foss/2023b Ninja/1.11.1</code> OR-Tools 9.9 foss/2023b <code>module load foss/2023b OR-Tools/9.9</code> ORCA 5.0.4 foss/2022b <code>module load foss/2022b ORCA/5.0.4</code> OVITO 3.7.11-basic foss/2021b <code>module load foss/2021b OVITO/3.7.11-basic</code> OpenBLAS 0.3.24 foss/2023b <code>module load foss/2023b OpenBLAS/0.3.24</code> OpenBLAS 0.3.18 foss/2021b <code>module load foss/2021b OpenBLAS/0.3.18</code> OpenBLAS 0.3.21 foss/2022b <code>module load foss/2022b OpenBLAS/0.3.21</code> OpenEXR 3.1.5 foss/2022b <code>module load foss/2022b OpenEXR/3.1.5</code> OpenEXR 3.1.1 foss/2021b <code>module load foss/2021b OpenEXR/3.1.1</code> OpenFOAM v2306 foss/2022b <code>module load foss/2022b OpenFOAM/v2306</code> OpenFOAM v2112 foss/2021b <code>module load foss/2021b OpenFOAM/v2112</code> OpenJPEG 2.5.0 foss/2023b <code>module load foss/2023b OpenJPEG/2.5.0</code> OpenJPEG 2.5.0 foss/2022b <code>module load foss/2022b OpenJPEG/2.5.0</code> OpenJPEG 2.4.0 foss/2021b <code>module load foss/2021b OpenJPEG/2.4.0</code> OpenMPI 4.1.6 foss/2023b <code>module load foss/2023b</code> OpenMPI 4.1.1 foss/2021b <code>module load foss/2021b</code> OpenMPI 4.1.4 foss/2022b <code>module load foss/2022b</code> OpenPGM 5.2.122 foss/2023b <code>module load foss/2023b OpenPGM/5.2.122</code> OpenPGM 5.2.122 foss/2022b <code>module load foss/2022b OpenPGM/5.2.122</code> OpenSSL 1.1 - <code>module load OpenSSL/1.1</code> OptiX 6.5.0 - <code>module load OptiX/6.5.0</code> Osi 0.108.10 foss/2023b <code>module load foss/2023b Osi/0.108.10</code> PCRE 8.45 foss/2022b <code>module load foss/2022b PCRE/8.45</code> PCRE 8.45 foss/2021b <code>module load foss/2021b PCRE/8.45</code> PCRE2 10.42 foss/2023b <code>module load foss/2023b PCRE2/10.42</code> PCRE2 10.37 foss/2021b <code>module load foss/2021b PCRE2/10.37</code> PCRE2 10.40 foss/2022b <code>module load foss/2022b PCRE2/10.40</code> PLUMED 2.7.3 foss/2021b <code>module load foss/2021b PLUMED/2.7.3</code> PLUMED 2.8.0 intel/2021b <code>module load intel/2021b PLUMED/2.8.0</code> PLUMED 2.9.0 foss/2022b <code>module load foss/2022b PLUMED/2.9.0</code> PMIx 4.2.6 foss/2023b <code>module load foss/2023b PMIx/4.2.6</code> PMIx 4.1.0 foss/2021b <code>module load foss/2021b PMIx/4.1.0</code> PMIx 4.2.2 foss/2022b <code>module load foss/2022b PMIx/4.2.2</code> POV-Ray 3.7.0.10 foss/2022b <code>module load foss/2022b POV-Ray/3.7.0.10</code> POV-Ray 3.7.0.10 foss/2021b <code>module load foss/2021b POV-Ray/3.7.0.10</code> PROJ 8.1.0 foss/2021b <code>module load foss/2021b PROJ/8.1.0</code> PROJ 9.1.1 foss/2022b <code>module load foss/2022b PROJ/9.1.1</code> Pango 1.50.12 foss/2022b <code>module load foss/2022b Pango/1.50.12</code> Pango 1.48.8 foss/2021b <code>module load foss/2021b Pango/1.48.8</code> ParMETIS 4.0.3 foss/2022b <code>module load foss/2022b ParMETIS/4.0.3</code> ParaView 5.11.0-osmesa - <code>module load ParaView/5.11.0-osmesa</code> ParaView 5.11.2-egl - <code>module load ParaView/5.11.2-egl</code> ParaView 5.11.2-osmesa - <code>module load ParaView/5.11.2-osmesa</code> ParaView 5.9.1-mpi foss/2021b <code>module load foss/2021b ParaView/5.9.1-mpi</code> ParaView 5.11.0-egl - <code>module load ParaView/5.11.0-egl</code> ParaView 5.11.1 foss/2022b <code>module load foss/2022b ParaView/5.11.1</code> ParaView 5.11.0-mpi foss/2022b <code>module load foss/2022b ParaView/5.11.0-mpi</code> ParallelIO 2.5.10 foss/2022b <code>module load foss/2022b ParallelIO/2.5.10</code> Perl 5.34.0 foss/2021b <code>module load foss/2021b Perl/5.34.0</code> Perl 5.36.0-minimal foss/2022b <code>module load foss/2022b Perl/5.36.0-minimal</code> Perl 5.38.0 foss/2023b <code>module load foss/2023b Perl/5.38.0</code> Perl 5.36.0 foss/2022b <code>module load foss/2022b Perl/5.36.0</code> Perl 5.34.1 foss/2020a <code>module load foss/2020a Perl/5.34.1</code> Perl-bundle-CPAN 5.38.0 foss/2023b <code>module load foss/2023b Perl-bundle-CPAN/5.38.0</code> Perl-bundle-njit 5.38.0 foss/2023b <code>module load foss/2023b Perl-bundle-njit/5.38.0</code> Pillow 10.2.0 foss/2023b <code>module load foss/2023b Pillow/10.2.0</code> Pillow 9.4.0 foss/2022b <code>module load foss/2022b Pillow/9.4.0</code> Pillow 8.3.2 foss/2021b <code>module load foss/2021b Pillow/8.3.2</code> PnetCDF 1.12.3 foss/2022b <code>module load foss/2022b PnetCDF/1.12.3</code> PyCharm 2022.3.2 - <code>module load PyCharm/2022.3.2</code> PyInstaller 6.3.0 foss/2023b <code>module load foss/2023b PyInstaller/6.3.0</code> PyQt5 5.15.4 foss/2021b <code>module load foss/2021b PyQt5/5.15.4</code> PyTables 3.8.0 foss/2022b <code>module load foss/2022b PyTables/3.8.0</code> PyYAML 6.0.1 foss/2023b <code>module load foss/2023b PyYAML/6.0.1</code> PyZMQ 25.1.0 foss/2022b <code>module load foss/2022b PyZMQ/25.1.0</code> PyZMQ 25.1.2 foss/2023b <code>module load foss/2023b PyZMQ/25.1.2</code> Python 3.11.5 foss/2023b <code>module load foss/2023b Python/3.11.5</code> Python 3.10.4 foss/2020a <code>module load foss/2020a Python/3.10.4</code> Python 2.7.18-bare foss/2022b <code>module load foss/2022b Python/2.7.18-bare</code> Python 3.10.4-bare foss/2020a <code>module load foss/2020a Python/3.10.4-bare</code> Python 3.10.8 foss/2022b <code>module load foss/2022b Python/3.10.8</code> Python 3.10.8-bare foss/2022b <code>module load foss/2022b Python/3.10.8-bare</code> Python 2.7.18-bare foss/2021b <code>module load foss/2021b Python/2.7.18-bare</code> Python 3.9.6 foss/2021b <code>module load foss/2021b Python/3.9.6</code> Python 3.9.6-bare foss/2021b <code>module load foss/2021b Python/3.9.6-bare</code> Python-bundle-PyPI 2023.10 foss/2023b <code>module load foss/2023b Python-bundle-PyPI/2023.10</code> Qhull 2020.2 foss/2021b <code>module load foss/2021b Qhull/2020.2</code> Qhull 2020.2 foss/2022b <code>module load foss/2022b Qhull/2020.2</code> Qhull 2020.2 foss/2023b <code>module load foss/2023b Qhull/2020.2</code> Qt5 5.15.2 foss/2021b <code>module load foss/2021b Qt5/5.15.2</code> Qt5 5.15.13 foss/2023b <code>module load foss/2023b Qt5/5.15.13</code> Qt5 5.15.7 foss/2022b <code>module load foss/2022b Qt5/5.15.7</code> QuantumESPRESSO 7.1 foss/2021b <code>module load foss/2021b QuantumESPRESSO/7.1</code> R 4.2.2 foss/2022b <code>module load foss/2022b R/4.2.2</code> R 4.2.0 foss/2021b <code>module load foss/2021b R/4.2.0</code> RASPA2 2.0.47 foss/2022b <code>module load foss/2022b RASPA2/2.0.47</code> RE2 2023-03-01 foss/2022b <code>module load foss/2022b RE2/2023-03-01</code> RE2 2024-03-01 foss/2023b <code>module load foss/2023b RE2/2024-03-01</code> RIP-MD master foss/2021b <code>module load foss/2021b RIP-MD/master</code> RapidJSON 1.1.0 foss/2022b <code>module load foss/2022b RapidJSON/1.1.0</code> Rust 1.54.0 foss/2021b <code>module load foss/2021b Rust/1.54.0</code> Rust 1.73.0 foss/2023b <code>module load foss/2023b Rust/1.73.0</code> Rust 1.60.0 foss/2020a <code>module load foss/2020a Rust/1.60.0</code> Rust 1.65.0 foss/2022b <code>module load foss/2022b Rust/1.65.0</code> SCOTCH 6.1.2 foss/2021b <code>module load foss/2021b SCOTCH/6.1.2</code> SCOTCH 7.0.3 foss/2022b <code>module load foss/2022b SCOTCH/7.0.3</code> SCOTCH 7.0.4 foss/2023b <code>module load foss/2023b SCOTCH/7.0.4</code> SCons 4.6.0 foss/2023b <code>module load foss/2023b SCons/4.6.0</code> SDL2 2.28.5 foss/2023b <code>module load foss/2023b SDL2/2.28.5</code> SDL2 2.0.20 foss/2021b <code>module load foss/2021b SDL2/2.0.20</code> SDL2 2.26.3 foss/2022b <code>module load foss/2022b SDL2/2.26.3</code> SQLite 3.36 foss/2021b <code>module load foss/2021b SQLite/3.36</code> SQLite 3.43.1 foss/2023b <code>module load foss/2023b SQLite/3.43.1</code> SQLite 3.39.4 foss/2022b <code>module load foss/2022b SQLite/3.39.4</code> SQLite 3.38.3 foss/2020a <code>module load foss/2020a SQLite/3.38.3</code> SWIG 4.1.1 foss/2023b <code>module load foss/2023b SWIG/4.1.1</code> SWIG 4.0.2 foss/2021b <code>module load foss/2021b SWIG/4.0.2</code> ScaFaCoS 1.0.1 foss/2021b <code>module load foss/2021b ScaFaCoS/1.0.1</code> ScaLAPACK 2.2.0-fb foss/2022b <code>module load foss/2022b ScaLAPACK/2.2.0-fb</code> ScaLAPACK 2.1.0-fb foss/2021b <code>module load foss/2021b ScaLAPACK/2.1.0-fb</code> ScaLAPACK 2.2.0-fb foss/2023b <code>module load foss/2023b ScaLAPACK/2.2.0-fb</code> SciPy-bundle 2022.05 intel/2021b <code>module load intel/2021b SciPy-bundle/2022.05</code> SciPy-bundle 2021.10 foss/2021b <code>module load foss/2021b SciPy-bundle/2021.10</code> SciPy-bundle 2021.10 intel/2021b <code>module load intel/2021b SciPy-bundle/2021.10</code> SciPy-bundle 2023.02 foss/2022b <code>module load foss/2022b SciPy-bundle/2023.02</code> SciPy-bundle 2023.11 foss/2023b <code>module load foss/2023b SciPy-bundle/2023.11</code> Spack 0.17.2 - <code>module load Spack/0.17.2</code> Spack 0.20.0 - <code>module load Spack/0.20.0</code> SuperLU 5.3.0 intel/2021b <code>module load intel/2021b SuperLU/5.3.0</code> SuperLU 5.3.0 foss/2022b <code>module load foss/2022b SuperLU/5.3.0</code> SuperLU_DIST 8.1.0 foss/2022b <code>module load foss/2022b SuperLU_DIST/8.1.0</code> Szip 2.1.1 foss/2022b <code>module load foss/2022b Szip/2.1.1</code> Szip 2.1.1 foss/2021b <code>module load foss/2021b Szip/2.1.1</code> Szip 2.1.1 foss/2023b <code>module load foss/2023b Szip/2.1.1</code> Tcl 8.6.13 foss/2023b <code>module load foss/2023b Tcl/8.6.13</code> Tcl 8.6.12 foss/2020a <code>module load foss/2020a Tcl/8.6.12</code> Tcl 8.6.12 foss/2022b <code>module load foss/2022b Tcl/8.6.12</code> Tcl 8.6.11 foss/2021b <code>module load foss/2021b Tcl/8.6.11</code> Tk 8.6.13 foss/2023b <code>module load foss/2023b Tk/8.6.13</code> Tk 8.6.12 foss/2022b <code>module load foss/2022b Tk/8.6.12</code> Tk 8.6.11 foss/2021b <code>module load foss/2021b Tk/8.6.11</code> Tkinter 3.11.5 foss/2023b <code>module load foss/2023b Tkinter/3.11.5</code> Tkinter 3.9.6 foss/2021b <code>module load foss/2021b Tkinter/3.9.6</code> Tkinter 3.10.8 foss/2022b <code>module load foss/2022b Tkinter/3.10.8</code> UCC 1.2.0 foss/2023b <code>module load foss/2023b UCC/1.2.0</code> UCC 1.1.0 foss/2022b <code>module load foss/2022b UCC/1.1.0</code> UCX 1.12.1 foss/2020a <code>module load foss/2020a UCX/1.12.1</code> UCX 1.13.1 foss/2022b <code>module load foss/2022b UCX/1.13.1</code> UCX 1.11.2 foss/2021b <code>module load foss/2021b UCX/1.11.2</code> UCX 1.15.0 foss/2023b <code>module load foss/2023b UCX/1.15.0</code> UCX-CUDA 1.13.1-CUDA-12.0.0 foss/2022b <code>module load foss/2022b UCX-CUDA/1.13.1-CUDA-12.0.0</code> UCX-CUDA 1.11.2-CUDA-11.4.1 foss/2021b <code>module load foss/2021b UCX-CUDA/1.11.2-CUDA-11.4.1</code> UCX-CUDA 1.13.1-CUDA-12.4.0 foss/2022b <code>module load foss/2022b UCX-CUDA/1.13.1-CUDA-12.4.0</code> UDUNITS 2.2.28 foss/2021b <code>module load foss/2021b UDUNITS/2.2.28</code> UDUNITS 2.2.28 foss/2022b <code>module load foss/2022b UDUNITS/2.2.28</code> UnZip 6.0 foss/2022b <code>module load foss/2022b UnZip/6.0</code> UnZip 6.0 foss/2021b <code>module load foss/2021b UnZip/6.0</code> UnZip 6.0 foss/2023b <code>module load foss/2023b UnZip/6.0</code> UnZip 6.0 foss/2020a <code>module load foss/2020a UnZip/6.0</code> VASP 6.5.0 intel/2022b <code>module load intel/2022b VASP/6.5.0</code> VASP 6.4.2 intel/2022b <code>module load intel/2022b VASP/6.4.2</code> VMD 1.9.4a57 foss/2022b <code>module load foss/2022b VMD/1.9.4a57</code> VTK 9.1.0 foss/2021b <code>module load foss/2021b VTK/9.1.0</code> Valgrind 3.21.0 foss/2022b <code>module load foss/2022b Valgrind/3.21.0</code> VirtualGL 3.0 foss/2021b <code>module load foss/2021b VirtualGL/3.0</code> Voro++ 0.4.6 foss/2021b <code>module load foss/2021b Voro++/0.4.6</code> Wayland 1.22.0 foss/2023b <code>module load foss/2023b Wayland/1.22.0</code> X11 20210802 foss/2021b <code>module load foss/2021b X11/20210802</code> X11 20231019 foss/2023b <code>module load foss/2023b X11/20231019</code> X11 20221110 foss/2022b <code>module load foss/2022b X11/20221110</code> XZ 5.4.4 foss/2023b <code>module load foss/2023b XZ/5.4.4</code> XZ 5.2.5 foss/2021b <code>module load foss/2021b XZ/5.2.5</code> XZ 5.2.5 foss/2020a <code>module load foss/2020a XZ/5.2.5</code> XZ 5.2.7 foss/2022b <code>module load foss/2022b XZ/5.2.7</code> Xerces-C++ 3.2.4 foss/2022b <code>module load foss/2022b Xerces-C++/3.2.4</code> Xvfb 21.1.6 foss/2022b <code>module load foss/2022b Xvfb/21.1.6</code> Xvfb 1.20.13 foss/2021b <code>module load foss/2021b Xvfb/1.20.13</code> Yasm 1.3.0 foss/2021b <code>module load foss/2021b Yasm/1.3.0</code> Yasm 1.3.0 foss/2022b <code>module load foss/2022b Yasm/1.3.0</code> Z3 4.8.12 foss/2021b <code>module load foss/2021b Z3/4.8.12</code> ZeroMQ 4.3.4 foss/2022b <code>module load foss/2022b ZeroMQ/4.3.4</code> ZeroMQ 4.3.5 foss/2023b <code>module load foss/2023b ZeroMQ/4.3.5</code> apptainer 1.1.9 - <code>module load apptainer/1.1.9</code> archspec 0.1.3 foss/2021b <code>module load foss/2021b archspec/0.1.3</code> arkouda 2023.11.15 foss/2022b <code>module load foss/2022b arkouda/2023.11.15</code> arpack-ng 3.8.0 foss/2022b <code>module load foss/2022b arpack-ng/3.8.0</code> at-spi2-atk 2.38.0 foss/2022b <code>module load foss/2022b at-spi2-atk/2.38.0</code> at-spi2-core 2.46.0 foss/2022b <code>module load foss/2022b at-spi2-core/2.46.0</code> binutils 2.38 foss/2020a <code>module load foss/2020a binutils/2.38</code> binutils 2.40 foss/2023b <code>module load foss/2023b binutils/2.40</code> binutils 2.37 foss/2021b <code>module load foss/2021b binutils/2.37</code> binutils 2.36.1 - <code>module load binutils/2.36.1</code> binutils 2.37 - <code>module load binutils/2.37</code> binutils 2.39 foss/2022b <code>module load foss/2022b binutils/2.39</code> binutils 2.40 - <code>module load binutils/2.40</code> binutils 2.38 - <code>module load binutils/2.38</code> binutils 2.39 - <code>module load binutils/2.39</code> bzip2 1.0.8 foss/2022b <code>module load foss/2022b bzip2/1.0.8</code> bzip2 1.0.8 foss/2020a <code>module load foss/2020a bzip2/1.0.8</code> bzip2 1.0.8 foss/2021b <code>module load foss/2021b bzip2/1.0.8</code> bzip2 1.0.8 foss/2023b <code>module load foss/2023b bzip2/1.0.8</code> cURL 7.86.0 foss/2022b <code>module load foss/2022b cURL/7.86.0</code> cURL 8.3.0 foss/2023b <code>module load foss/2023b cURL/8.3.0</code> cURL 7.78.0 foss/2021b <code>module load foss/2021b cURL/7.78.0</code> cURL 7.83.0 foss/2020a <code>module load foss/2020a cURL/7.83.0</code> cairo 1.18.0 foss/2023b <code>module load foss/2023b cairo/1.18.0</code> cairo 1.16.0 foss/2021b <code>module load foss/2021b cairo/1.16.0</code> cairo 1.17.4 foss/2022b <code>module load foss/2022b cairo/1.17.4</code> cffi 1.15.1 foss/2023b <code>module load foss/2023b cffi/1.15.1</code> chapel 1.33.0 foss/2022b <code>module load foss/2022b chapel/1.33.0</code> cppy 1.2.1 foss/2022b <code>module load foss/2022b cppy/1.2.1</code> cppy 1.2.1 foss/2023b <code>module load foss/2023b cppy/1.2.1</code> cppy 1.1.0 foss/2021b <code>module load foss/2021b cppy/1.1.0</code> cryptography 41.0.5 foss/2023b <code>module load foss/2023b cryptography/41.0.5</code> cuDNN 8.8.0.121-CUDA-12.0.0 - <code>module load cuDNN/8.8.0.121-CUDA-12.0.0</code> double-conversion 3.3.0 foss/2023b <code>module load foss/2023b double-conversion/3.3.0</code> double-conversion 3.1.5 foss/2021b <code>module load foss/2021b double-conversion/3.1.5</code> double-conversion 3.2.1 foss/2022b <code>module load foss/2022b double-conversion/3.2.1</code> elfutils 0.189 foss/2022b <code>module load foss/2022b elfutils/0.189</code> elfutils 0.185 foss/2021b <code>module load foss/2021b elfutils/0.185</code> expat 2.5.0 foss/2023b <code>module load foss/2023b expat/2.5.0</code> expat 2.4.9 foss/2022b <code>module load foss/2022b expat/2.4.9</code> expat 2.4.8 foss/2020a <code>module load foss/2020a expat/2.4.8</code> expat 2.4.1 foss/2021b <code>module load foss/2021b expat/2.4.1</code> ffnvcodec 11.1.5.2 - <code>module load ffnvcodec/11.1.5.2</code> ffnvcodec 12.1.14.0 - <code>module load ffnvcodec/12.1.14.0</code> flex 2.6.4 foss/2023b <code>module load foss/2023b flex/2.6.4</code> flex 2.6.4 foss/2021b <code>module load foss/2021b flex/2.6.4</code> flex 2.6.4 - <code>module load flex/2.6.4</code> flex 2.6.4 foss/2020a <code>module load foss/2020a flex/2.6.4</code> flex 2.6.4 foss/2022b <code>module load foss/2022b flex/2.6.4</code> flit 3.9.0 foss/2023b <code>module load foss/2023b flit/3.9.0</code> fontconfig 2.14.1 foss/2022b <code>module load foss/2022b fontconfig/2.14.1</code> fontconfig 2.14.2 foss/2023b <code>module load foss/2023b fontconfig/2.14.2</code> fontconfig 2.13.94 foss/2021b <code>module load foss/2021b fontconfig/2.13.94</code> foss 2023b - <code>module load foss/2023b</code> foss 2022b - <code>module load foss/2022b</code> foss 2021b - <code>module load foss/2021b</code> freetype 2.11.0 foss/2021b <code>module load foss/2021b freetype/2.11.0</code> freetype 2.12.1 foss/2022b <code>module load foss/2022b freetype/2.12.1</code> freetype 2.13.2 foss/2023b <code>module load foss/2023b freetype/2.13.2</code> gettext 0.21 - <code>module load gettext/0.21</code> gettext 0.21.1 - <code>module load gettext/0.21.1</code> gettext 0.21.1 foss/2022b <code>module load foss/2022b gettext/0.21.1</code> gettext 0.22 foss/2023b <code>module load foss/2023b gettext/0.22</code> gettext 0.21 foss/2021b <code>module load foss/2021b gettext/0.21</code> gettext 0.21 foss/2020a <code>module load foss/2020a gettext/0.21</code> gettext 0.22 - <code>module load gettext/0.22</code> gfbf 2022b - <code>module load gfbf/2022b</code> gfbf 2023b - <code>module load gfbf/2023b</code> giflib 5.2.1 foss/2023b <code>module load foss/2023b giflib/5.2.1</code> giflib 5.2.1 foss/2022b <code>module load foss/2022b giflib/5.2.1</code> git 2.38.1-nodocs foss/2022b <code>module load foss/2022b git/2.38.1-nodocs</code> git 2.42.0 foss/2023b <code>module load foss/2023b git/2.42.0</code> git 2.33.1 - <code>module load git/2.33.1</code> git 2.36.0-nodocs foss/2020a <code>module load foss/2020a git/2.36.0-nodocs</code> git 2.33.1-nodocs foss/2021b <code>module load foss/2021b git/2.33.1-nodocs</code> git-lfs 3.2.0 - <code>module load git-lfs/3.2.0</code> glew 2.2.0-osmesa foss/2022b <code>module load foss/2022b glew/2.2.0-osmesa</code> glew 2.2.0-egl foss/2022b <code>module load foss/2022b glew/2.2.0-egl</code> gnuplot 5.4.2 foss/2021b <code>module load foss/2021b gnuplot/5.4.2</code> gnuplot 5.4.6 foss/2022b <code>module load foss/2022b gnuplot/5.4.6</code> gompi 2021b - <code>module load gompi/2021b</code> gompi 2022b - <code>module load gompi/2022b</code> gompi 2023b - <code>module load gompi/2023b</code> googletest 1.12.1 foss/2022b <code>module load foss/2022b googletest/1.12.1</code> googletest 1.14.0 foss/2023b <code>module load foss/2023b googletest/1.14.0</code> gperf 3.1 foss/2021b <code>module load foss/2021b gperf/3.1</code> gperf 3.1 foss/2022b <code>module load foss/2022b gperf/3.1</code> gperf 3.1 foss/2023b <code>module load foss/2023b gperf/3.1</code> graphite2 1.3.14 foss/2023b <code>module load foss/2023b graphite2/1.3.14</code> graphite2 1.3.14 foss/2022b <code>module load foss/2022b graphite2/1.3.14</code> graphite2 1.3.14 foss/2021b <code>module load foss/2021b graphite2/1.3.14</code> groff 1.22.4 foss/2022b <code>module load foss/2022b groff/1.22.4</code> groff 1.23.0 foss/2023b <code>module load foss/2023b groff/1.23.0</code> groff 1.22.4 foss/2020a <code>module load foss/2020a groff/1.22.4</code> groff 1.22.4 foss/2021b <code>module load foss/2021b groff/1.22.4</code> gzip 1.10 foss/2021b <code>module load foss/2021b gzip/1.10</code> gzip 1.12 foss/2022b <code>module load foss/2022b gzip/1.12</code> gzip 1.13 foss/2023b <code>module load foss/2023b gzip/1.13</code> hatch-jupyter-builder 0.9.1 foss/2023b <code>module load foss/2023b hatch-jupyter-builder/0.9.1</code> hatchling 1.18.0 foss/2023b <code>module load foss/2023b hatchling/1.18.0</code> help2man 1.48.3 foss/2021b <code>module load foss/2021b help2man/1.48.3</code> help2man 1.49.2 foss/2022b <code>module load foss/2022b help2man/1.49.2</code> help2man 1.49.3 foss/2023b <code>module load foss/2023b help2man/1.49.3</code> help2man 1.49.2 foss/2020a <code>module load foss/2020a help2man/1.49.2</code> hwloc 2.5.0 foss/2021b <code>module load foss/2021b hwloc/2.5.0</code> hwloc 2.9.2 foss/2023b <code>module load foss/2023b hwloc/2.9.2</code> hwloc 2.8.0 foss/2022b <code>module load foss/2022b hwloc/2.8.0</code> hypothesis 6.14.6 foss/2021b <code>module load foss/2021b hypothesis/6.14.6</code> hypothesis 6.46.7 foss/2020a <code>module load foss/2020a hypothesis/6.46.7</code> hypothesis 6.68.2 foss/2022b <code>module load foss/2022b hypothesis/6.68.2</code> hypothesis 6.90.0 foss/2023b <code>module load foss/2023b hypothesis/6.90.0</code> iimpi 2021b - <code>module load iimpi/2021b</code> iimpi 2022a - <code>module load iimpi/2022a</code> iimpi 2022b - <code>module load iimpi/2022b</code> iimpi 2023b - <code>module load iimpi/2023b</code> imkl 2022.2.1 - <code>module load imkl/2022.2.1</code> imkl 2021.4.0 - <code>module load imkl/2021.4.0</code> imkl 2023.2.0 - <code>module load imkl/2023.2.0</code> imkl 2022.1.0 - <code>module load imkl/2022.1.0</code> imkl-FFTW 2021.4.0 intel/2021b <code>module load intel/2021b imkl-FFTW/2021.4.0</code> imkl-FFTW 2022.2.1 intel/2022b <code>module load intel/2022b imkl-FFTW/2022.2.1</code> imkl-FFTW 2023.2.0 intel/2023b <code>module load intel/2023b imkl-FFTW/2023.2.0</code> imkl-FFTW 2022.1.0 intel/2021b <code>module load intel/2021b imkl-FFTW/2022.1.0</code> impi 2021.6.0 intel/2021b <code>module load intel/2021b</code> impi 2021.10.0 intel/2023b <code>module load intel/2023b</code> impi 2021.7.1 intel/2022b <code>module load intel/2022b</code> impi 2021.4.0 intel/2021b <code>module load intel/2021b</code> intel 2021b - <code>module load intel/2021b</code> intel 2022a - <code>module load intel/2022a</code> intel 2023b - <code>module load intel/2023b</code> intel 2022b - <code>module load intel/2022b</code> intel-compilers 2021.4.0 intel/2021b <code>module load intel-compilers/2021.4.0</code> intel-compilers 2022.2.1 intel/2022b <code>module load intel-compilers/2022.2.1</code> intel-compilers 2023.2.1 intel/2023b <code>module load intel-compilers/2023.2.1</code> intel-compilers 2022.1.0 intel/2021b <code>module load intel-compilers/2022.1.0</code> intltool 0.51.0 foss/2021b <code>module load foss/2021b intltool/0.51.0</code> intltool 0.51.0 foss/2022b <code>module load foss/2022b intltool/0.51.0</code> intltool 0.51.0 foss/2023b <code>module load foss/2023b intltool/0.51.0</code> jbigkit 2.1 foss/2021b <code>module load foss/2021b jbigkit/2.1</code> jbigkit 2.1 foss/2023b <code>module load foss/2023b jbigkit/2.1</code> jbigkit 2.1 foss/2022b <code>module load foss/2022b jbigkit/2.1</code> jedi 0.19.1 foss/2023b <code>module load foss/2023b jedi/0.19.1</code> json-c 0.16 foss/2022b <code>module load foss/2022b json-c/0.16</code> jupyter-server 2.14.0 foss/2023b <code>module load foss/2023b jupyter-server/2.14.0</code> kim-api 2.3.0 foss/2021b <code>module load foss/2021b kim-api/2.3.0</code> klayout 0.29.1 foss/2023b <code>module load foss/2023b klayout/0.29.1</code> libGLU 9.0.3 foss/2023b <code>module load foss/2023b libGLU/9.0.3</code> libGLU 9.0.2 foss/2022b <code>module load foss/2022b libGLU/9.0.2</code> libGLU 9.0.2 foss/2021b <code>module load foss/2021b libGLU/9.0.2</code> libarchive 3.6.1 foss/2020a <code>module load foss/2020a libarchive/3.6.1</code> libarchive 3.6.1 foss/2022b <code>module load foss/2022b libarchive/3.6.1</code> libarchive 3.7.2 foss/2023b <code>module load foss/2023b libarchive/3.7.2</code> libarchive 3.5.1 foss/2021b <code>module load foss/2021b libarchive/3.5.1</code> libcerf 2.3 foss/2022b <code>module load foss/2022b libcerf/2.3</code> libcerf 1.17 foss/2021b <code>module load foss/2021b libcerf/1.17</code> libdeflate 1.19 foss/2023b <code>module load foss/2023b libdeflate/1.19</code> libdeflate 1.15 foss/2022b <code>module load foss/2022b libdeflate/1.15</code> libdrm 2.4.114 foss/2022b <code>module load foss/2022b libdrm/2.4.114</code> libdrm 2.4.107 foss/2021b <code>module load foss/2021b libdrm/2.4.107</code> libdrm 2.4.117 foss/2023b <code>module load foss/2023b libdrm/2.4.117</code> libepoxy 1.5.10 foss/2022b <code>module load foss/2022b libepoxy/1.5.10</code> libevent 2.1.12 foss/2022b <code>module load foss/2022b libevent/2.1.12</code> libevent 2.1.12 foss/2023b <code>module load foss/2023b libevent/2.1.12</code> libevent 2.1.12 - <code>module load libevent/2.1.12</code> libevent 2.1.12 foss/2021b <code>module load foss/2021b libevent/2.1.12</code> libfabric 1.16.1 foss/2022b <code>module load foss/2022b libfabric/1.16.1</code> libfabric 1.19.0 foss/2023b <code>module load foss/2023b libfabric/1.19.0</code> libfabric 1.13.2 foss/2021b <code>module load foss/2021b libfabric/1.13.2</code> libffi 3.4.4 foss/2023b <code>module load foss/2023b libffi/3.4.4</code> libffi 3.4.2 foss/2021b <code>module load foss/2021b libffi/3.4.2</code> libffi 3.4.4 foss/2022b <code>module load foss/2022b libffi/3.4.4</code> libffi 3.4.2 foss/2020a <code>module load foss/2020a libffi/3.4.2</code> libgd 2.3.3 foss/2021b <code>module load foss/2021b libgd/2.3.3</code> libgd 2.3.3 foss/2022b <code>module load foss/2022b libgd/2.3.3</code> libgeotiff 1.7.0 foss/2021b <code>module load foss/2021b libgeotiff/1.7.0</code> libgeotiff 1.7.1 foss/2022b <code>module load foss/2022b libgeotiff/1.7.1</code> libgit2 1.7.2 foss/2023b <code>module load foss/2023b libgit2/1.7.2</code> libgit2 1.5.0 foss/2022b <code>module load foss/2022b libgit2/1.5.0</code> libgit2 1.1.1 foss/2021b <code>module load foss/2021b libgit2/1.1.1</code> libglvnd 1.6.0 foss/2022b <code>module load foss/2022b libglvnd/1.6.0</code> libglvnd 1.3.3 foss/2021b <code>module load foss/2021b libglvnd/1.3.3</code> libglvnd 1.7.0 foss/2023b <code>module load foss/2023b libglvnd/1.7.0</code> libiconv 1.16 foss/2021b <code>module load foss/2021b libiconv/1.16</code> libiconv 1.17 foss/2022b <code>module load foss/2022b libiconv/1.17</code> libiconv 1.17 foss/2023b <code>module load foss/2023b libiconv/1.17</code> libidn2 2.3.2 foss/2022b <code>module load foss/2022b libidn2/2.3.2</code> libjpeg-turbo 2.1.4 foss/2022b <code>module load foss/2022b libjpeg-turbo/2.1.4</code> libjpeg-turbo 2.0.6 foss/2021b <code>module load foss/2021b libjpeg-turbo/2.0.6</code> libjpeg-turbo 3.0.1 foss/2023b <code>module load foss/2023b libjpeg-turbo/3.0.1</code> libogg 1.3.5 foss/2022b <code>module load foss/2022b libogg/1.3.5</code> libogg 1.3.5 foss/2021b <code>module load foss/2021b libogg/1.3.5</code> libopus 1.3.1 foss/2022b <code>module load foss/2022b libopus/1.3.1</code> libpciaccess 0.17 foss/2022b <code>module load foss/2022b libpciaccess/0.17</code> libpciaccess 0.16 foss/2021b <code>module load foss/2021b libpciaccess/0.16</code> libpciaccess 0.17 foss/2023b <code>module load foss/2023b libpciaccess/0.17</code> libpng 1.6.38 foss/2022b <code>module load foss/2022b libpng/1.6.38</code> libpng 1.6.40 foss/2023b <code>module load foss/2023b libpng/1.6.40</code> libpng 1.6.37 foss/2021b <code>module load foss/2021b libpng/1.6.37</code> libreadline 8.1 foss/2021b <code>module load foss/2021b libreadline/8.1</code> libreadline 8.2 foss/2023b <code>module load foss/2023b libreadline/8.2</code> libreadline 8.2 foss/2022b <code>module load foss/2022b libreadline/8.2</code> libreadline 8.1.2 foss/2020a <code>module load foss/2020a libreadline/8.1.2</code> libsndfile 1.0.31 foss/2021b <code>module load foss/2021b libsndfile/1.0.31</code> libsndfile 1.2.0 foss/2022b <code>module load foss/2022b libsndfile/1.2.0</code> libsodium 1.0.19 foss/2023b <code>module load foss/2023b libsodium/1.0.19</code> libsodium 1.0.18 foss/2022b <code>module load foss/2022b libsodium/1.0.18</code> libtirpc 1.3.3 foss/2022b <code>module load foss/2022b libtirpc/1.3.3</code> libtirpc 1.3.2 foss/2021b <code>module load foss/2021b libtirpc/1.3.2</code> libtool 2.4.7 foss/2023b <code>module load foss/2023b libtool/2.4.7</code> libtool 2.4.7 - <code>module load libtool/2.4.7</code> libtool 2.4.7 foss/2022b <code>module load foss/2022b libtool/2.4.7</code> libtool 2.4.7 foss/2020a <code>module load foss/2020a libtool/2.4.7</code> libtool 2.4.6 foss/2021b <code>module load foss/2021b libtool/2.4.6</code> libunwind 1.6.2 foss/2022b <code>module load foss/2022b libunwind/1.6.2</code> libunwind 1.5.0 foss/2021b <code>module load foss/2021b libunwind/1.5.0</code> libunwind 1.6.2 foss/2023b <code>module load foss/2023b libunwind/1.6.2</code> libvorbis 1.3.7 foss/2021b <code>module load foss/2021b libvorbis/1.3.7</code> libvorbis 1.3.7 foss/2022b <code>module load foss/2022b libvorbis/1.3.7</code> libvori 220621 foss/2022b <code>module load foss/2022b libvori/220621</code> libwebp 1.3.2 foss/2023b <code>module load foss/2023b libwebp/1.3.2</code> libxc 4.3.4 intel/2021b <code>module load intel/2021b libxc/4.3.4</code> libxc 5.1.6 intel/2021b <code>module load intel/2021b libxc/5.1.6</code> libxc 5.1.6 foss/2021b <code>module load foss/2021b libxc/5.1.6</code> libxc 5.2.3 intel/2022b <code>module load intel/2022b libxc/5.2.3</code> libxc 6.1.0 intel/2022b <code>module load intel/2022b libxc/6.1.0</code> libxc 6.1.0 foss/2022b <code>module load foss/2022b libxc/6.1.0</code> libxml2 2.9.13 foss/2020a <code>module load foss/2020a libxml2/2.9.13</code> libxml2 2.9.10 foss/2021b <code>module load foss/2021b libxml2/2.9.10</code> libxml2 2.11.5 foss/2023b <code>module load foss/2023b libxml2/2.11.5</code> libxml2 2.10.3 foss/2022b <code>module load foss/2022b libxml2/2.10.3</code> libxslt 1.1.37 foss/2022b <code>module load foss/2022b libxslt/1.1.37</code> libxslt 1.1.38 foss/2023b <code>module load foss/2023b libxslt/1.1.38</code> libxsmm 1.17 intel/2021b <code>module load intel/2021b libxsmm/1.17</code> libxsmm 1.16.2 intel/2022b <code>module load intel/2022b libxsmm/1.16.2</code> libxsmm 1.17 foss/2022b <code>module load foss/2022b libxsmm/1.17</code> libyaml 0.2.5 foss/2023b <code>module load foss/2023b libyaml/0.2.5</code> lxml 4.9.3 foss/2023b <code>module load foss/2023b lxml/4.9.3</code> lz4 1.9.4 foss/2023b <code>module load foss/2023b lz4/1.9.4</code> lz4 1.9.4 foss/2022b <code>module load foss/2022b lz4/1.9.4</code> lz4 1.9.3 foss/2021b <code>module load foss/2021b lz4/1.9.3</code> magma 2.6.2-CUDA-11.4.1 foss/2021b <code>module load foss/2021b magma/2.6.2-CUDA-11.4.1</code> make 4.4.1 foss/2023b <code>module load foss/2023b make/4.4.1</code> make 4.3 foss/2021b <code>module load foss/2021b make/4.3</code> makeinfo 7.1 foss/2023b <code>module load foss/2023b makeinfo/7.1</code> matplotlib 3.7.0 foss/2022b <code>module load foss/2022b matplotlib/3.7.0</code> matplotlib 3.8.2 foss/2023b <code>module load foss/2023b matplotlib/3.8.2</code> matplotlib 3.4.3 intel/2021b <code>module load intel/2021b matplotlib/3.4.3</code> matplotlib 3.4.3 foss/2021b <code>module load foss/2021b matplotlib/3.4.3</code> maturin 1.3.1 foss/2023b <code>module load foss/2023b maturin/1.3.1</code> meson-python 0.15.0 foss/2023b <code>module load foss/2023b meson-python/0.15.0</code> motif 2.3.8 foss/2022b <code>module load foss/2022b motif/2.3.8</code> motif 2.3.8 foss/2021b <code>module load foss/2021b motif/2.3.8</code> mpi4py 3.1.5 foss/2023b <code>module load foss/2023b mpi4py/3.1.5</code> mpi4py 3.1.4 foss/2022b <code>module load foss/2022b mpi4py/3.1.4</code> ncurses 6.3 foss/2022b <code>module load foss/2022b ncurses/6.3</code> ncurses 6.2 foss/2021b <code>module load foss/2021b ncurses/6.2</code> ncurses 6.2 - <code>module load ncurses/6.2</code> ncurses 6.4 foss/2023b <code>module load foss/2023b ncurses/6.4</code> ncurses 6.3 foss/2020a <code>module load foss/2020a ncurses/6.3</code> ncurses 6.3 - <code>module load ncurses/6.3</code> ncurses 6.4 - <code>module load ncurses/6.4</code> ncview 2.1.8 foss/2022b <code>module load foss/2022b ncview/2.1.8</code> netCDF 4.8.1 intel/2021b <code>module load intel/2021b netCDF/4.8.1</code> netCDF 4.9.2 foss/2023b <code>module load foss/2023b netCDF/4.9.2</code> netCDF 4.8.1 foss/2021b <code>module load foss/2021b netCDF/4.8.1</code> netCDF 4.9.0 foss/2022b <code>module load foss/2022b netCDF/4.9.0</code> netCDF-Fortran 4.6.0 foss/2022b <code>module load foss/2022b netCDF-Fortran/4.6.0</code> netCDF-Fortran 4.5.3 foss/2021b <code>module load foss/2021b netCDF-Fortran/4.5.3</code> netCDF-Fortran 4.5.3 intel/2021b <code>module load intel/2021b netCDF-Fortran/4.5.3</code> nettle 3.7.3 foss/2021b <code>module load foss/2021b nettle/3.7.3</code> nettle 3.8.1 foss/2022b <code>module load foss/2022b nettle/3.8.1</code> networkx 3.0 foss/2022b <code>module load foss/2022b networkx/3.0</code> networkx 3.2.1 foss/2023b <code>module load foss/2023b networkx/3.2.1</code> networkx 2.6.3 foss/2021b <code>module load foss/2021b networkx/2.6.3</code> networkx 2.8.8 foss/2022b <code>module load foss/2022b networkx/2.8.8</code> nlohmann_json 3.11.2 foss/2022b <code>module load foss/2022b nlohmann_json/3.11.2</code> nodejs 20.9.0 foss/2023b <code>module load foss/2023b nodejs/20.9.0</code> nodejs 18.12.1 foss/2022b <code>module load foss/2022b nodejs/18.12.1</code> nodejs 14.17.6 foss/2021b <code>module load foss/2021b nodejs/14.17.6</code> numactl 2.0.16 foss/2023b <code>module load foss/2023b numactl/2.0.16</code> numactl 2.0.14 foss/2020a <code>module load foss/2020a numactl/2.0.14</code> numactl 2.0.14 foss/2021b <code>module load foss/2021b numactl/2.0.14</code> numactl 2.0.16 foss/2022b <code>module load foss/2022b numactl/2.0.16</code> opendihu 2204 - <code>module load opendihu/2204</code> p7zip 17.04 - <code>module load p7zip/17.04</code> packmol 20.14.3 foss/2022b <code>module load foss/2022b packmol/20.14.3</code> parallel 20230722 foss/2022b <code>module load foss/2022b parallel/20230722</code> patchelf 0.18.0 foss/2023b <code>module load foss/2023b patchelf/0.18.0</code> perm-md-count main foss/2021b <code>module load foss/2021b perm-md-count/main</code> perm-md-count main foss/2022b <code>module load foss/2022b perm-md-count/main</code> pixman 0.42.2 foss/2023b <code>module load foss/2023b pixman/0.42.2</code> pixman 0.42.2 foss/2022b <code>module load foss/2022b pixman/0.42.2</code> pixman 0.40.0 foss/2021b <code>module load foss/2021b pixman/0.40.0</code> pkg-config 0.29.2 foss/2021b <code>module load foss/2021b pkg-config/0.29.2</code> pkgconf 1.9.3 foss/2022b <code>module load foss/2022b pkgconf/1.9.3</code> pkgconf 1.8.0 foss/2021b <code>module load foss/2021b pkgconf/1.8.0</code> pkgconf 2.0.3 foss/2023b <code>module load foss/2023b pkgconf/2.0.3</code> pkgconf 1.8.0 - <code>module load pkgconf/1.8.0</code> pkgconf 1.8.0 foss/2020a <code>module load foss/2020a pkgconf/1.8.0</code> pkgconfig 1.5.5-python foss/2021b <code>module load foss/2021b pkgconfig/1.5.5-python</code> pkgconfig 1.5.5-python foss/2023b <code>module load foss/2023b pkgconfig/1.5.5-python</code> pocl 1.8 foss/2021b <code>module load foss/2021b pocl/1.8</code> poetry 1.6.1 foss/2023b <code>module load foss/2023b poetry/1.6.1</code> poppler 22.01.0 foss/2021b <code>module load foss/2021b poppler/22.01.0</code> protobuf 25.3 foss/2023b <code>module load foss/2023b protobuf/25.3</code> protobuf-python 4.25.3 foss/2023b <code>module load foss/2023b protobuf-python/4.25.3</code> py-cpuinfo 9.0.0 foss/2022b <code>module load foss/2022b py-cpuinfo/9.0.0</code> pybind11 2.7.1 foss/2021b <code>module load foss/2021b pybind11/2.7.1</code> pybind11 2.10.3 foss/2022b <code>module load foss/2022b pybind11/2.10.3</code> pybind11 2.9.2 foss/2020a <code>module load foss/2020a pybind11/2.9.2</code> pybind11 2.11.1 foss/2023b <code>module load foss/2023b pybind11/2.11.1</code> rclone 1.57.0 - <code>module load rclone/1.57.0</code> re2c 3.0 foss/2022b <code>module load foss/2022b re2c/3.0</code> re2c 3.1 foss/2023b <code>module load foss/2023b re2c/3.1</code> re2c 2.2 foss/2021b <code>module load foss/2021b re2c/2.2</code> scikit-build 0.11.1 foss/2021b <code>module load foss/2021b scikit-build/0.11.1</code> scikit-build 0.17.2 foss/2022b <code>module load foss/2022b scikit-build/0.17.2</code> scikit-build 0.17.6 foss/2023b <code>module load foss/2023b scikit-build/0.17.6</code> setuptools-rust 1.8.0 foss/2023b <code>module load foss/2023b setuptools-rust/1.8.0</code> silo 4.10.2 foss/2022b <code>module load foss/2022b silo/4.10.2</code> snappy 1.1.9 foss/2022b <code>module load foss/2022b snappy/1.1.9</code> snappy 1.1.10 foss/2023b <code>module load foss/2023b snappy/1.1.10</code> snappy 1.1.9 foss/2021b <code>module load foss/2021b snappy/1.1.9</code> spdlog 1.12.0 foss/2023b <code>module load foss/2023b spdlog/1.12.0</code> tbb 2020.3 foss/2021b <code>module load foss/2021b tbb/2020.3</code> tcsh 6.24.01 foss/2021b <code>module load foss/2021b tcsh/6.24.01</code> tecplot360ex 2022R2 - <code>module load tecplot360ex/2022R2</code> tmux 3.2a foss/2021b <code>module load foss/2021b tmux/3.2a</code> tmux 3.3a - <code>module load tmux/3.3a</code> tornado 6.4 foss/2023b <code>module load foss/2023b tornado/6.4</code> tqdm 4.62.3 foss/2021b <code>module load foss/2021b tqdm/4.62.3</code> tqdm 4.64.1 foss/2022b <code>module load foss/2022b tqdm/4.64.1</code> utf8proc 2.8.0 foss/2022b <code>module load foss/2022b utf8proc/2.8.0</code> util-linux 2.39 foss/2023b <code>module load foss/2023b util-linux/2.39</code> util-linux 2.37 foss/2021b <code>module load foss/2021b util-linux/2.37</code> util-linux 2.38.1 foss/2022b <code>module load foss/2022b util-linux/2.38.1</code> virtualenv 20.24.6 foss/2023b <code>module load foss/2023b virtualenv/20.24.6</code> wxWidgets 3.2.2.1 foss/2022b <code>module load foss/2022b wxWidgets/3.2.2.1</code> x264 20210613 foss/2021b <code>module load foss/2021b x264/20210613</code> x264 20230226 foss/2022b <code>module load foss/2022b x264/20230226</code> x265 3.5 foss/2022b <code>module load foss/2022b x265/3.5</code> x265 3.5 foss/2021b <code>module load foss/2021b x265/3.5</code> xorg-macros 1.19.3 foss/2022b <code>module load foss/2022b xorg-macros/1.19.3</code> xorg-macros 1.19.3 foss/2021b <code>module load foss/2021b xorg-macros/1.19.3</code> xorg-macros 1.20.0 foss/2023b <code>module load foss/2023b xorg-macros/1.20.0</code> xprop 1.2.5 foss/2021b <code>module load foss/2021b xprop/1.2.5</code> xprop 1.2.5 foss/2022b <code>module load foss/2022b xprop/1.2.5</code> xtb 6.6.1 foss/2022b <code>module load foss/2022b xtb/6.6.1</code> xtb 6.6.1 foss/2022b <code>module load foss/2022b xtb/6.6.1</code> xxd 8.2.4220 foss/2021b <code>module load foss/2021b xxd/8.2.4220</code> xxd 9.0.1696 foss/2022b <code>module load foss/2022b xxd/9.0.1696</code> zlib 1.2.12 - <code>module load zlib/1.2.12</code> zlib 1.2.11 - <code>module load zlib/1.2.11</code> zlib 1.2.12 foss/2022b <code>module load foss/2022b zlib/1.2.12</code> zlib 1.2.13 foss/2023b <code>module load foss/2023b zlib/1.2.13</code> zlib 1.2.12 foss/2020a <code>module load foss/2020a zlib/1.2.12</code> zlib 1.2.11 foss/2021b <code>module load foss/2021b zlib/1.2.11</code> zlib 1.2.13 - <code>module load zlib/1.2.13</code> zstd 1.5.2 foss/2022b <code>module load foss/2022b zstd/1.5.2</code> zstd 1.5.0 foss/2021b <code>module load foss/2021b zstd/1.5.0</code> zstd 1.5.5 foss/2023b <code>module load foss/2023b zstd/1.5.5</code>"},{"location":"Software/CFD/ansys/","title":"ANSYS","text":"<p>ANSYS is a computer-aided engineering (CAE) software suite used to simulate, analyze, and design products and systems in various industries such as aerospace, automotive, energy, and healthcare. ANSYS provides a wide range of simulation tools that can be used to model and analyze various physical phenomena such as fluid dynamics, structural mechanics, electromagnetics, and thermal analysis.</p> <p>The software suite is known for its high level of accuracy and versatility, and is widely used by engineers and designers to optimize product performance, reduce costs, and improve time to market. ANSYS offers a wide range of modules and add-ons, which can be customized to suit specific engineering needs and requirements.</p>"},{"location":"Software/CFD/ansys/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command ANSYS 2022 - <code>module load ANSYS/2022</code> ANSYS 2023R1 foss/2021b <code>module load foss/2021b ANSYS/2023R1</code>"},{"location":"Software/CFD/ansys/#application-information-documentation","title":"Application Information, Documentation","text":"<p>Please download ANSYS and follow the instructions to install ANSYS on your local machine.</p>"},{"location":"Software/CFD/ansys/#using-ansys","title":"Using ANSYS","text":"<p>ANSYS workbench will be available on Wulver via Open OnDemand. We will provide the instructions soon.</p>"},{"location":"Software/CFD/ansys/#related-applications","title":"Related Applications","text":"<ul> <li>COMSOL</li> </ul>"},{"location":"Software/CFD/ansys/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/CFD/comsol/","title":"COMSOL","text":"<p>COMSOL is a commercial finite element analysis (FEA) software package used for modeling and simulation of multi-physics systems. It allows users to build and solve complex multiphysics models involving various physical phenomena such as fluid dynamics, structural mechanics, heat transfer, electromagnetics, and chemical reactions.</p> <p>The software uses a graphical user interface to create models, and offers a wide range of pre-built modeling components that can be used to quickly create models. It also supports user-defined models and can handle a variety of boundary conditions and physics.</p> <p>COMSOL is widely used in engineering and science fields, such as mechanical engineering, chemical engineering, electrical engineering, physics, and materials science, among others. Its multiphysics capabilities make it a powerful tool for simulating complex systems and optimizing designs.</p>"},{"location":"Software/CFD/comsol/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command COMSOL 6.2 - <code>module load COMSOL/6.2</code> COMSOL 5.6 - <code>module load COMSOL/5.6</code>"},{"location":"Software/CFD/comsol/#application-information-documentation","title":"Application Information, Documentation","text":""},{"location":"Software/CFD/comsol/#using-comsol","title":"Using COMSOL","text":""},{"location":"Software/CFD/comsol/#related-applications","title":"Related Applications","text":"<ul> <li>ANSYS </li> </ul>"},{"location":"Software/CFD/comsol/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/CFD/fluent/","title":"FLUENT","text":""},{"location":"Software/CFD/fluent/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command ANSYS 2022 - <code>module load ANSYS/2022</code> ANSYS 2023R1 foss/2021b <code>module load foss/2021b ANSYS/2023R1</code>"},{"location":"Software/CFD/fluent/#application-information-documentation","title":"Application Information, Documentation","text":"<p>To use Fluent on cluster, Users need to prepare the case using ANSYS on their local machine first. Please download ANSYS and follow the instructions to install ANSYS on your local machine.</p>"},{"location":"Software/CFD/fluent/#using-fluent","title":"Using Fluent","text":"<p>To use Fluent in cluster, users first need to prepare the fluent case (meshing, setting boundary conditions) on their local machine  and save the case and data in <code>.cas</code> and <code>.dat</code> format respectively.  If you are running transient problem and want to save the data at particular timestep or time interval, please see the steps below.</p> <ul> <li>Go to <code>Calculation Activities</code> option in the left pane and double-click the <code>Autosave (Every  Flow Time)</code> option, you will notice a separate dialogue box <code>Autosave</code>, where you need to specify how frequently you want to save the data, you can choose eiter <code>timestep</code> or <code>Flow Time</code> interval. In the <code>File name</code> option you need to specify the subdirectory where you want to save the data and the case name. In the example shown below the subdirectory is <code>data</code> and the problem name is heatpipe. You need to make sure to create the subdirectoy (<code>data</code> in this example) in the cluster where you want to intend to submit the job script.</li> </ul> <p></p> <p>This is required if your job is somehow cancelled, or you need to restart from specific flow time.</p> <ul> <li>If you want  to ppstprocess the data using a different software , e.g. Tecplot or ParaView, you need to save the data in different file format at certain flow time or time step interval.  To set up the postprocessing configuration, select <code>File --&gt; Export --&gt; During Calculation --&gt; Solution Data</code> option (see the figure below)</li> </ul> <p></p> <ul> <li>Once you open the configuration, you will notice separate dialogue box <code>Automatic Export</code>. You need the file format in <code>File Type</code> option. Select the drop-down option to see different options. In this example above, <code>ensight</code> format has been selected. In right pane, you need to select which parameters you want to visualize. In the example above, <code>Volume Fraction</code> for different phases have been selected. </li> <li>Next, you to select the file name and the subdirectory on cluster where you intend to write the post processed data. In this example, we choose to write the files in <code>ensi</code> subdirectory. Please make sure to create this subdirectory inside case directory on cluster before running the simulation. </li> <li>You can also set how often you want to write your data. You need to select <code>Export Data Everty (s)</code> option in terms of time step interval or flow time interval.  </li> <li>Once you set up your case and initialize the problem, you need to go to <code>File --&gt; write --&gt; case and data</code> option to write the case and data in <code>.cas</code> and <code>.dat</code> respectively. </li> <li>Transfer the <code>.cas</code> and <code>.dat</code> files to the cluster. See the steps to transfer the data for transferring the files from local machine to cluster.</li> <li>Once you transfer the files, log on the cluster and go the directory where you transferred the <code>.cas</code> and <code>.dat</code> files. </li> <li>Create a journal file which defines the input data file and some additional settings required by Fluent.  You can use the following journal file</li> </ul> Sample journal file : journal.JOU <pre><code>   /file/set-tui-version \"21.1\"\n   /file/read-case-data tube_vof.cas.h5\n   solve/dual-time-iterate 20 50\n   (print-case-timer)\n   parallel/timer/usage\n</code></pre> <p>In the above <code>Journal</code> script, the full name of case file (<code>tube_vof.cas.h5</code>) is mentioned. You need to modify based on the case file based on the problem. The <code>solve/dual-time-iterate</code> specifies the end flow time and number of iterations. In the above example, <code>20</code> is the end flow time while the maximum number of iterations are <code>50</code>. The \"dual-time\" approach allows for a larger time step size by introducing an additional iteration loop within each time step. Users can select different approach based on their problems and need to modify it accordingly. </p> Sample Batch Script to Run FLUENT : fluent.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH --job-name=fluent\n#SBATCH --output=%x.%j.out # i%x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n# Use \"sinfo\" to see what partitions are available to you\n#SBATCH --partition=general\n#SBATCH --ntasks=8\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n# Memory required; lower amount gets scheduling priority\n#SBATCH --mem-per-cpu=2G\n\n# Time required in d-hh:mm:ss format; lower time gets scheduling priority\n#SBATCH --time=71:59:00\n\n# Purge and load the correct modules\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver # Load the slurm, easybuild \nmodule load ANSYS\n\n# Run the mpi program\n\nmachines=hosts.$SLURM_JOB_ID\ntouch $machines\nfor node in `scontrol show hostnames`\n    do\n        echo \"$node\"  &gt;&gt; $machines\n    done\n\nfluent 3ddp -affinity=off -ssh -t$SLURM_NTASKS -pib -mpi=intel -cnf=\"$machines\" -g -i journal.JOU\n</code></pre> <p>Submit the job using <code>sbatch fluent.submit.sh</code> command.</p>"},{"location":"Software/CFD/fluent/#related-applications","title":"Related Applications","text":"<ul> <li>OpenFOAM</li> </ul>"},{"location":"Software/CFD/fluent/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/CFD/openfoam/","title":"OpenFOAM","text":"<p>OpenFOAM (Open Field Operation and Manipulation) is a free and open-source computational fluid dynamics (CFD) software package that is used to simulate and analyze fluid flow, heat transfer, and other related phenomena. It is written primarily in C++ and can be used on various operating systems such as Linux, Windows, and macOS.</p> <p>OpenFOAM offers a wide range of solvers, turbulence models, and physical models that can be used to model various engineering applications such as automotive, aerospace, chemical processing, and power generation. It also allows users to customize and extend its functionality to meet specific needs.</p> <p>The software is widely used in academia, research, and industry, and is known for its robustness, flexibility, and scalability. It offers a high degree of parallelization, which allows it to be used on large clusters of computers for complex simulations. Because it is open source, it is often used by researchers and developers to create new solvers, models, and add-ons for specific applications.</p>"},{"location":"Software/CFD/openfoam/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command OpenFOAM v2112 foss/2021b <code>module load foss/2021b OpenFOAM/v2112</code> OpenFOAM v2306 foss/2022b <code>module load foss/2022b OpenFOAM/v2306</code>"},{"location":"Software/CFD/openfoam/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of OpenFOAM is available at OpenFOAM Documentation, where you can find the tutorials in OpenFOAM meshing (blockMesh), postprocessing, setting boundary conditions etc. </p>"},{"location":"Software/CFD/openfoam/#using-openfoam","title":"Using OpenFOAM","text":"<p>OpenFOAM can be used for both serial and parallel jobs. To run OpenFOAM in parallel, you need to use the following job script.</p> Sample Batch Script to Run OpenFOAM in parallel: openfoam_parallel.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH --job-name=openfoam_parallel\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load foss/2021b OpenFOAM\n################################################\n#\n# Source OpenFOAM bashrc\n# The modulefile doesn't do this\n#\n################################################\nsource $FOAM_BASH\n################################################\n#\n# copy into cavity directory from /opt/site/examples/openFoam/parallel \n# run blockMesh and\n# icoFoam. Note: this is running on one node and\n# using all 32 cores on the node\n#\n################################################\ncp -r /apps/easybuild/examples/openFoam/parallel/cavity /path/to/destination\n# /path/to/destination is destination path where user wants to copy the cavity directory\ncd cavity\nblockMesh\ndecomposePar -force\nsrun icoFoam -parallel\nreconstructPar\n</code></pre> <p>Note</p> Wulver <p>You can copy the tutorial <code>cavity</code> mentioned in the above job script from the <code>/apps/easybuild/examples/openFoam/parallel</code> directory.  </p> <p>To run OpenFOAM in serial, the following job script can be used.</p> Sample Batch Script to Run OpenFOAM in serial: openfoam_serial.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH --job-name=openfoam_serial\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load foss/2021b OpenFOAM\n################################################\n#\n# Source OpenFOAM bashrc\n# The modulefile doesn't do this\n#\n################################################\nsource $FOAM_BASH\n################################################\n#\n# copy into cavity directory from /apps/easybuild/examples/openFoam/serial \n# run blockMesh and\n# icoFoam. Note: this is running on one node and\n# using all 32 cores on the node\n#\n################################################\ncp -r /apps/easybuild/examples/openFoam/serial/cavity /path/to/destination\n# /path/to/destination is destination path where user wants to copy the cavity directory\ncd cavity\nblockMesh\nicoFoam\n</code></pre> <p>Submit the job script using the sbatch command: <code>sbatch openfoam_parallel.submit.sh</code> or <code>sbatch openfoam_serial.submit.sh</code>.</p>"},{"location":"Software/CFD/openfoam/#building-openfoam-from-source","title":"Building OpenFOAM from source","text":"<p>Sometimes, users need to create a new solver or modify the existing solver by adding different functions for their research. In that case, users need to build openFOAM from source since user do not have the permission to add libraries in the root directory where OpenFOAM is installed. The following instructions are provided on how to build openFOAM from source on cluster. If you have any queries or issues regarding building OpenFOAM, please contact us at hpc@njit.edu.</p> <pre><code>  # This is to build a completely self contained OpenFOAM using MPICH mpi. Everything from GCC on up will be built.\n\n  # start an interactive session with compute node. Replace \"PI_UCID\" with the UCID of PI. Modify the other parameters if required.\n  srun --partition=general --nodes=1 --ntasks-per-node=16 --mem-per-cpu=2G --account=PI_UCID --qos=standard --time=2:00:00 --pty bash  \n\n  # purge all loaded modules\n  module purge\n  # Download the latest version of OpenFOAM, visit https://develop.openfoam.com/Development/openfoam/-/blob/master/doc/Build.md for details\n  # Download the source\n  wget https://dl.openfoam.com/source/v2212/OpenFOAM-v2212.tgz \n  # Download ThirdParty\n  wget https://dl.openfoam.com/source/v2212/ThirdParty-v2212.tgz\n\n  cd ThirdParty-v2212\n\n  #Packages to download :\n  wget https://ftp.gnu.org/gnu/gcc/gcc-4.8.5/gcc-4.8.5.tar.bz2\n  wget https://src.fedoraproject.org/repo/pkgs/metis/metis-5.1.0.tar.gz/5465e67079419a69e0116de24fce58fe/metis-5.1.0.tar.gz\n  wget ftp://ftp.gnu.org/gnu/gmp/gmp-6.2.0.tar.bz2\n  wget ftp://ftp.gnu.org/gnu/mpfr/mpfr-4.0.2.tar.bz2\n  wget ftp://ftp.gnu.org/gnu/mpc/mpc-1.1.0.tar.gz\n  wget http://www.mpich.org/static/downloads/3.3/mpich-3.3.tar.gz\n\n  # unpack the above packages\n\n  vi ../OpenFOAM-v2212/etc/bashrc\n  # Change the following in bashrc \n  # User needs to specify the full path of the project directory below, it can be either the research directory or $HOME directory.\n    projectDir=\"path/to/OpenFOAM/2212/OpenFOAM-$WM_PROJECT_VERSION\" \n    export WM_MPLIB=MPICH\n    export WM_LABEL_SIZE=64\n\n  vi ../OpenFOAM-v2212/etc/config.sh/compiler\n  # Change the following in compiler\n    default_gmp_version=gmp-system\n    default_mpfr_version=mpfr-system\n    default_mpc_version=mpc-system\n\n    gmp_version=\"gmp-6.2.0\"\n    mpfr_version=\"mpfr-4.0.2\"\n    mpc_version=\"mpc-1.1.0\"\n  # Source the RC script\n  source ../OpenFOAM-v2212/etc/bashrc FOAMY_HEX_MESH=yes\n  # You might see the following warning message\n  ===============================================================================\n  Warning in /opt/site/apps/OpenFOAM/2212/OpenFOAM-v2212/etc/config.sh/settings:\n  Cannot find 'Gcc' compiler installation\n    /opt/site/apps/OpenFOAM/2212/ThirdParty-v2212/platforms/linux64/gcc-4.8.5\n\n  Either install this compiler version, or use the system compiler by setting\n  WM_COMPILER_TYPE to 'system' in $WM_PROJECT_DIR/etc/bashrc.\n  ===============================================================================\n  No completions for /opt/site/apps/OpenFOAM/2212/OpenFOAM-v2212/platforms/linux64GccDPInt64Opt/bin\n  [ignore if OpenFOAM is not yet compiled]\n\n  ./makeGcc\n  wmRefresh\n  # make MPICH\n  ./makeMPICH\n  wmRefresh\n\n  # We should be able to make the rest of the utilities.\n  # load the cmake module\n  module load cmake\n\n  /Allwmake -j 8\n\n  cd ../OpenFOAM-v2212\n  wmRefresh\n\n  ./Allwmake -j 16\n</code></pre>"},{"location":"Software/CFD/openfoam/#related-applications","title":"Related Applications","text":"<ul> <li>FLUENT</li> </ul>"},{"location":"Software/CFD/openfoam/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/IDE/VSCode/","title":"VS Code","text":"<p>Visual Studio Code (often abbreviated as VS Code) is an Integrated Development Environment (IDE). It is a lightweight and highly extensible code editor developed by Microsoft. Although referred to as a code editor, VS Code offers many features. One of the important feature of VS Code is an integrated terminal that allows developers to execute commands, run scripts, and interact with the command-line interface without leaving the editor. VS Code also provides Git integration, enabling developers to manage version control operations, such as committing, branching, and merging, without switching to a separate Git client. </p>"},{"location":"Software/IDE/VSCode/#availability","title":"Availability","text":"<p>VS Code is not installed on the cluster. To use VS Code, you need to install it on your computer and connect remotely to the cluster using NJIT VPN.</p>"},{"location":"Software/IDE/VSCode/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of VS Code is available at VS Code documentation. You can download the VS Code from VS Code download page</p>"},{"location":"Software/IDE/VSCode/#using-vs-code-on-cluster","title":"Using VS Code on Cluster","text":"<p>Warning</p> <p>If you want to use VS Code on NJIT cluster, don't use VS Code installed on your machine to connect to cluster! Please use the method described belelow so that you can use VS Code not only to edit scripts, but also run your script on the cluster.</p> <p>Use the following slurm script and submit the job script using <code>sbatch vs-code.submit.sh</code> command.</p> Batch Script to use VS Code : vs-code.submit.sh <pre><code>#!/bin/bash -l\n#SBATCH --job-name=vs-code\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.out # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\nset -e\n\nmodule purge\nmodule load wulver # load slurn, easybuild\n\n# add any required module loads here, e.g. a specific Python\n\nCLI_PATH=\"${HOME}/vscode_cli\"\n\n# Install the VS Code CLI command if it doesn't exist\nif [[ ! -e ${CLI_PATH}/code ]]; then\n    echo \"Downloading and installing the VS Code CLI command\"\n    mkdir -p \"${HOME}/vscode_cli\"\n    pushd \"${HOME}/vscode_cli\"\n    # Process from: https://code.visualstudio.com/docs/remote/tunnels#_using-the-code-cli\n    curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&amp;os=cli-alpine-x64' --output vscode_cli.tar.gz\n    # unpack the code binary file\n    tar -xf vscode_cli.tar.gz\n    # clean-up\n    rm vscode_cli.tar.gz\n    popd\nfi\n\n# run the code tunnel command and accept the licence\n${CLI_PATH}/code tunnel --accept-server-license-terms\n</code></pre> <p>Once you submit the job, you will see an output file with <code>.out</code> extension. Once you open the file, you will see the following <pre><code>*\n\n* Visual Studio Code Server\n\n*\n\n* By using the software, you agree to\n\n* the Visual Studio Code Server License Terms (https://aka.ms/vscode-server-license)and\n\n* the Microsoft Privacy Statement (https://privacy.microsoft.com/en-US/privacystatement).\n\n*\n\nTo grant access to the server, please log into https://github.com/login/device and use code XXXX-XXXX\n</code></pre> You need to have the GitHub account, please open the GitHub profile and use the code printed in the output file. Once you authorize GitHub, you will see the following in the output file</p> <pre><code>Open this link in your browser https://vscode.dev/tunnel/nodeXXX\n</code></pre> <p>Now copy and paste this link in your browser and VS Code is ready to use.</p>"},{"location":"Software/IDE/VSCode/#related-applications","title":"Related Applications","text":"<ul> <li>PyCharm</li> </ul>"},{"location":"Software/IDE/VSCode/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/cp2k/","title":"CP2K","text":"<p>CP2K is a free and open-source quantum chemistry and solid-state physics software package that is designed to perform atomistic simulations of a wide range of materials. It is capable of carrying out a variety of quantum mechanical calculations, including density functional theory (DFT), time-dependent DFT, and many-body perturbation theory (MBPT).</p> <p>CP2K is optimized for running on massively parallel supercomputers, making it well-suited for large-scale simulations of complex systems. It includes a number of advanced algorithms and techniques for efficiently calculating electronic properties of materials, such as linear scaling algorithms for DFT and MBPT calculations.</p> <p>CP2K is highly modular and flexible, allowing users to customize and extend its functionality by writing their own input files or by modifying the source code. It also includes a graphical user interface (GUI) for setting up and running simulations, as well as a number of built-in tools for analyzing simulation output.</p> <p>CP2K is widely used in the fields of materials science, chemistry, and physics for studying a wide range of materials, including biomolecules, polymers, and solids.</p>"},{"location":"Software/chemistry/cp2k/#availability","title":"Availability","text":"Software Version Dependent Toolchain Module Load Command CP2K 8.2 intel/2021b <code>module load intel/2021b CP2K/8.2</code> CP2K 2023.1 foss/2022b <code>module load foss/2022b CP2K/2023.1</code>"},{"location":"Software/chemistry/cp2k/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of CP2K is available at CP2K Documentation. For any issues CP2K simulation, users can contact at CP2K Forum. </p>"},{"location":"Software/chemistry/cp2k/#using-cp2k","title":"Using CP2K","text":"<p>CP2K MPI/OpenMP-hybrid Execution (PSMP), CP2K with Population Analysis capabilities- CP2K-popt</p> Sample Batch Script to Run CP2K : cp2k.submit.sh <pre><code>#!/bin/bash -l\n#SBATCH -J CP2K\n#SBATCH -o sn1-xtb_input.out\n#SBATCH -e sn1-xtb_input.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=16\n#SBATCH --mem-per-cpu=4G\n#SBATCH --qos=standard\n#SBATCH --partition=general\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH -t 72:00:00\n\n#module load command\n\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver\nmodule load foss/2022b CP2K\n\n#Run the program\n\ninputFile=sn1-xtb_input.inp\noutputFile=sn1-xtb_input.out\nmpirun -np $SLURM_NTASKS cp2k.popt -i $inputFile -o $outputFile\n</code></pre> <p>The sample input file <code>sn1-xtb_input.inp</code> can be found in <code>/apps/testjobs/CP2K</code></p> <p>Important</p> <p>Please don't run anything on <code>/apps/testjobs/CP2K</code> as users have only read-only permission. You need to copy the submit script and input file from <code>/apps/testjobs/CP2K</code> to <code>$HOME</code> or <code>/project</code>.</p>"},{"location":"Software/chemistry/cp2k/#related-applications","title":"Related Applications","text":"<ul> <li>Gaussian</li> <li>ORCA</li> </ul>"},{"location":"Software/chemistry/cp2k/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/gaussian/","title":"Gaussian","text":""},{"location":"Software/chemistry/gaussian/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command Gaussian 16.C.01-AVX2 - <code>module load Gaussian/16.C.01-AVX2</code> <p>Important</p> <p>Due to licensing restrictions, Gaussian is not automatically accessible to all HPC users. Students are required to contact hpc@njit.edu to request access to Gaussian.</p>"},{"location":"Software/chemistry/gaussian/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of Gaussian is available at Gaussian Documentation. For any issues Gaussian simulation, users can contact at Gaussian Support. </p>"},{"location":"Software/chemistry/gaussian/#using-gaussian","title":"Using Gaussian","text":"Sample Batch Script to Run Gaussian : g16.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH -J g16\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --mem-per-cpu=3G\n#SBATCH --time=10:00:00\n#SBATCH --partition=general\n#SBATCH --account=PI_UCID # Replace PI_UCID with the UCID of PI\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --qos=standard\n\n#module load command\n\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver\nmodule load Gaussian\n\n#Run the program\n\ng16 test_g98.com\n</code></pre> <p>The sample input file <code>test_g98.com</code> can be found in <code>/apps/testjobs/Gaussian</code></p> <p>Important</p> <p>Please don't run anything on <code>/apps/testjobs/Gaussian</code> as users have only read-only permission. You need to copy the submit script and input file from <code>/apps/testjobs/Gaussian</code> to <code>$HOME</code> or <code>/project</code>.</p>"},{"location":"Software/chemistry/gaussian/#related-applications","title":"Related Applications","text":"<ul> <li>CP2K</li> <li>ORCA</li> </ul>"},{"location":"Software/chemistry/gaussian/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/orca/","title":"ORCA","text":""},{"location":"Software/chemistry/orca/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command ORCA 5.0.4 foss/2022b <code>module load foss/2022b ORCA/5.0.4</code>"},{"location":"Software/chemistry/orca/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of Gaussian is available at ORCA Documentation. For any issues Gaussian simulation, users can contact at ORCA Forum. </p>"},{"location":"Software/chemistry/orca/#using-gaussian","title":"Using Gaussian","text":"Sample Batch Script to Run ORCA : orca.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH --job-name=job_orca\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --nodes=1\n#SBATCH --account=PI_UCID # Replace PI_UCID with the UCID of PI\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=59:00  # D-HH:\n\n#module load command\n\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver\nmodule load foss/2022b ORCA\n\n#Run the program\n\nsrun orca test.inp &gt; geom.out\n</code></pre> <p>The sample input file <code>test.inp</code> can be found in <code>/apps/testjobs/ORCA</code></p> <p>Important</p> <p>Please don't run anything on <code>/apps/testjobs/ORCA</code> as users have only read-only permission. You need to copy the submit script and input file from <code>/apps/testjobs/ORCA</code> to <code>$HOME</code> or <code>/project</code>.</p>"},{"location":"Software/chemistry/orca/#related-applications","title":"Related Applications","text":"<ul> <li>Gaussian</li> <li>CP2K</li> </ul>"},{"location":"Software/chemistry/orca/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/qe/","title":"Quantum Espresso","text":""},{"location":"Software/chemistry/qe/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command QuantumESPRESSO 7.1 foss/2021b <code>module load foss/2021b QuantumESPRESSO/7.1</code>"},{"location":"Software/chemistry/qe/#related-applications","title":"Related Applications","text":""},{"location":"Software/chemistry/qe/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/siesta/","title":"SIESTA","text":"<p>SIESTA (Spanish Initiative for Electronic Simulations with Thousands of Atoms) is a free and open-source software package for performing electronic structure calculations of materials using density functional theory (DFT). It is particularly well-suited for simulating materials containing large numbers of atoms, such as nanomaterials and surfaces.</p> <p>SIESTA uses a localized basis set approach that reduces the computational cost of DFT calculations by approximating the electronic wavefunction using a small number of basis functions centered around each atom. This allows SIESTA to efficiently simulate systems containing thousands of atoms with reasonable accuracy.</p> <p>SIESTA is actively developed and maintained by a team of researchers at the Universidad Aut\u00f3noma de Madrid in Spain, and is available as a free download under an open-source license. It is widely used in the fields of material science, chemistry, and physics for studying a wide range of materials, including metals, semiconductors, and biological molecules.</p>"},{"location":"Software/chemistry/siesta/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command"},{"location":"Software/chemistry/siesta/#related-applications","title":"Related Applications","text":""},{"location":"Software/chemistry/siesta/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/math/MATLAB/","title":"MATLAB","text":"<p>MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and proprietary programming language developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages. Although MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.</p>"},{"location":"Software/math/MATLAB/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command MATLAB 2024a - <code>module load MATLAB/2024a</code> MATLAB 2023a - <code>module load MATLAB/2023a</code>"},{"location":"Software/math/MATLAB/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of MATLAB is available at MATLAB Tutorial.</p>"},{"location":"Software/math/MATLAB/#using-matlab","title":"Using MATLAB","text":""},{"location":"Software/math/MATLAB/#serial-job","title":"Serial Job","text":"Sample Batch Script to run MATLAB: matlab-serial.sh Wulver <pre><code>#!/bin/bash\n#SBATCH -J test_matlab\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Load matlab module\nmodule purge\nmodule load wulver # Load the slurm, easybuild \nmodule load MATLAB\n\nmatlab -nodisplay -nosplash -r test\n</code></pre> Sample MATLAB script <pre><code>A = [ 1 2; 3 4]\nA.**2\n</code></pre>"},{"location":"Software/math/MATLAB/#parallel-job","title":"Parallel Job","text":""},{"location":"Software/math/MATLAB/#single-node-parallelization","title":"Single node parallelization","text":"Sample Batch Script to run MATLAB: matlab_parallel.sh Wulver <pre><code>#!/bin/bash\n#SBATCH -J test_matlab\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Load matlab module\nmodule purge\nmodule load wulver # Load the slurm, easybuild\nmodule load MATLAB\n\n# Run matlab\nmatlab -nodisplay -nosplash -r 'for_loop; quit'\n</code></pre> Sample Parallel MATLAB script: for_loop.m <pre><code>poolobj = parpool('local',32);\nfprintf('Number of workers: %g\\n', poolobj.NumWorkers);\n\ntic\nn = 2000;\nA = 500;\na = zeros(n);\nparfor i = 1:n\n    a(i) = max(abs(eig(rand(A))));\nend\ntoc\n</code></pre>"},{"location":"Software/math/MATLAB/#multi-node-parallelization","title":"Multi node parallelization","text":"<p>If you want to learn how to install a version of MATLAB on your local system and use it to run jobs on Wulver, please see Using Local MATLAB on Wulver</p>"},{"location":"Software/math/MATLAB/matlab_local/","title":"Use MATLAB on NJIT HPC","text":"<p>Tip</p> <p>Since MFA is enabled, the instructions for running MATLAB via HPC resources have been modified. If you already installed MATLAB on the local machine, skip to Setup Slurm profile to run MATLAB on Wulver. </p>"},{"location":"Software/math/MATLAB/matlab_local/#installation-steps-of-matlab-on-local-machine","title":"Installation steps of MATLAB on local machine","text":"<ul> <li>Go to Mathworks Download and register with your NJIT email address.</li> <li>Select the MATLAB version installed on Wulver.</li> <li>User needs to select the correct installer based on the OS (Mac or Windows). </li> <li>Run the installer.</li> </ul> <ul> <li>Make sure to check Parallel Computing Toolbox option.</li> </ul> <ul> <li>Continue by selecting Next and MATLAB will be installed on your computer.</li> </ul>"},{"location":"Software/math/MATLAB/matlab_local/#setup-slurm-profile-to-run-matlab-on-wulver","title":"Setup Slurm profile to run MATLAB on Wulver","text":"<ul> <li>Open MATLAB \u2192 select Create and Manage Clusters.</li> </ul> <ul> <li>A new dialogue box will open and under the Add Cluster Profile, select Slurm.</li> </ul> <ul> <li>This will open a Slurm cluster Profile and select the edit option to modify the parameters</li> </ul> <ul> <li>Modify the following parameters as mentioned in the screenshot</li> </ul> <p>a. <code>Description</code> - Set the name as <code>Wulver</code></p> <p>b. <code>JobStorageLocation</code> - No Change</p> <p>c. <code>NumWorkers</code> - 512</p> <p>d. <code>NumThreads</code> - No Change</p> <p>e. <code>ClusterMatlabRoot</code> - Use <code>module av MATLAB</code> command first.</p> <p><pre><code>  login-1-45 ~ &gt;: module av MATLAB\n  ------------------------------------/apps/easybuild/modules/all/Core---------------------------------------------------------\n   MATLAB/2024a\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre> This will show you the list of MATLAB versions installed on Wulver. Next, use <code>module show MATLAB/2024a</code> to check MATLAB installation path.</p> <p><pre><code>   login-1-45 ~ &gt;: module show MATLAB/2024a\n---------------------------------------------------------------------------------------------------------------------------------\n   /apps/easybuild/modules/all/Core/MATLAB/2024a.lua:\n---------------------------------------------------------------------------------------------------------------------------------\nhelp([[\nDescription\n===========\nThe MATLAB Parallel Server Toolbox.\n\n\nMore information\n================\n - Homepage: https://www.mathworks.com/help/matlab/matlab-engine-for-python.html\n]])\nwhatis(\"Description: The MATLAB Parallel Server Toolbox.\")\nwhatis(\"Homepage: https://www.mathworks.com/help/matlab/matlab-engine-for-python.html\")\nwhatis(\"URL: https://www.mathworks.com/help/matlab/matlab-engine-for-python.html\")\nconflict(\"MATLAB\")\nprepend_path(\"CMAKE_PREFIX_PATH\",\"/apps/easybuild/software/MATLAB/2024a\")\nprepend_path(\"PATH\",\"/apps/easybuild/software/MATLAB/2024a/bin\")\nsetenv(\"EBROOTMATLAB\",\"/apps/easybuild/software/MATLAB/2024a\")\nsetenv(\"EBVERSIONMATLAB\",\"2024a\")\nsetenv(\"EBDEVELMATLAB\",\"/apps/easybuild/software/MATLAB/R2023a/easybuild/Core-MATLAB-2023a-easybuild-devel\")\nprepend_path(\"PATH\",\"/apps/easybuild/software/MATLAB/2024a/toolbox/parallel/bin\")\nprepend_path(\"PATH\",\"/apps/easybuild/software/MATLAB/2024a\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/apps/easybuild/software/MATLAB/2024a/runtime/glnxa64\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/apps/easybuild/software/MATLAB/2024a/bin/glnxa64\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/apps/easybuild/software/MATLAB/2024a/sys/os/glnxa64\")\nsetenv(\"_JAVA_OPTIONS\",\"-Xmx2048m\")\n</code></pre> The MATLAB installation path is defined by the <code>EBROOTMATLAB</code> environment variable, which, in the above example, is set to <code>/apps/easybuild/software/MATLAB/2024a</code>.</p> <p>f. <code>RequireOnlineLicensing</code> - false</p> <p>g. <code>AdditionalProperties</code> - Select add  and add the following as mentioned in the table. </p> <p></p> Name Value Type <code>ClusterHost</code> <code>wulver.njit.edu</code> String <code>AuthenticationMode</code> Multifactor String <code>UseUniqueSubfolders</code> True Logical <code>UseIdentityFile</code> False Logical <code>RemoteJobStorageLocation</code> <code>$PATH</code> String <code>user</code> <code>$UCID</code> String <p>Replace <code>$PATH</code> with the actual path of Wulver where you want to save the output file. Make sure to use <code>/project</code> directory for remote job storage as <code>$HOME</code> has fixed quota of 50GB and cannot be increased. See Wulver Filesystems for details. Replace <code>$UCID</code> with the NJIT UCID.</p>"},{"location":"Software/math/MATLAB/matlab_local/#submitting-a-serial-job","title":"Submitting a Serial Job","text":"<p>This section will demonstrate how to create a cluster object and submit a simple job to the cluster. The job will run the 'hostname' command on the node assigned to the job. The output will indicate clearly that the job ran on the cluster and not on the local computer.</p> <p>The hostname.m file used in this demonstration can be downloaded here.</p> <pre><code> &gt;&gt; c=parcluster \n</code></pre> <p>Certain arguments need to be passed to SLURM in order for the job to run properly. Here we will set values for partition, and time. In the Matlab window enter: <pre><code> &gt;&gt; c.AdditionalProperties.AdditionalSubmitArgs=['--partition=general --qos=standard --account=$PI_UCID --time=2-00:00:00'] \n</code></pre> Replace <code>$PI_UCID</code> with the UCID of PI. Check the SLURM Documentation for other SLURM parameters. To make this persistent between Matlab sessions these arguments need to be saved to the profile. In the Matlab window enter: <pre><code> &gt;&gt; c.saveProfile \n</code></pre></p> <p>We will now submit the hostname.m function to the cluster. In the Matlab window enter the following: <pre><code>&gt;&gt; j=c.batch(@hostname, 1, {}, 'AutoAddClientPath', false); \n</code></pre></p> <ul> <li> <p><code>@</code>: Submitting a function.</p> </li> <li> <p><code>1</code>: The number of output arguments from the evaluated function.</p> </li> <li> <p><code>{}</code>: Cell array of input arguments to the function. In this case empty.</p> </li> <li> <p><code>'AutoAddClientPath', false</code>: The client path is not available on the cluster.</p> </li> </ul> <p>When the job is submitted, you will be prompted for your password.</p> <p>To wait for the job to finish, enter the following in the Matlab window: <pre><code> &gt;&gt;j.wait\n</code></pre> Finally, to get the results: <pre><code> &gt;&gt;fetchOutputs(j)\n</code></pre></p>"},{"location":"Software/math/MATLAB/matlab_local/#submitting-a-parallel-function","title":"Submitting a Parallel Function","text":"<p>The <code>Job Monitor</code> is a convenient way to monitor jobs submitted to the cluster. In the Matlab window select <code>Parallel</code> and then <code>Monitor Jobs</code>.</p> <p>For more information see the Mathworks page: Job Monitor.</p> <p>Here we will submit a simple function using a \"parfor\" loop. The code for this example is as follows: <pre><code>function t = parallel_example\n\nt0 = tic;\nparfor idx = 1:16\n        A(idx) = idx;\n        pause (2)\nend\n\nt=toc(t0);\n</code></pre> To submit this job: <pre><code> &gt;&gt; c.AdditionalProperties.AdditionalSubmitArgs=['--partition=general --qos=standard --account=$PI_UCID --ntasks=8 --time=2-00:00:00'] \n &gt;&gt; c.saveProfile\n &gt;&gt; j=c.batch(@parallel_example, 1, {}, 'AutoAddClientPath', false, 'Pool', 7)\n</code></pre> Since this is a parallel job a 'Pool' must be started. The actual number of tasks started will be one more than requested in the pool. In this case, the batch command calls for a pool of seven. Eight tasks will be started on the cluster.</p> <p>The job takes a few minutes to run and the state of the job changes to \"finished.\"</p> <p>Once again to get the results enter: <pre><code> &gt;&gt; fetchOutputs(j) \n</code></pre> As can be seen the parfor loop was completed in 6.7591 seconds.</p> <p></p>"},{"location":"Software/math/MATLAB/matlab_local/#submitting-a-script-requiring-a-gpu","title":"Submitting a Script Requiring a GPU","text":"<p>In this section we will submit a matlab script using a GPU. The results will be written to the job diary. The code for this example is as follows: <pre><code>% MATLAB script that defines a random matrix and does FFT\n%\n% The first FFT is without a GPU\n% The second is with the GPU\n%\n% MATLAB knows to use the GPU the second time because it\n%   is passed a type gpuArray as an argument to FFT\n% We do the FFT a bunch of times to make using the GPU worth it,\n%   or else it spends more time offloading to the GPU\n%   than performning the calculation\n%\n% This example is meant to provide a general understanding\n%   of MATLAB GPU usage\n% Meaningful performance measurements depend on many factors\n%   beyond the scope of this example\n% Downloaded from https://projects.ncsu.edu/hpc/Software/examples/matlab/gpu/gpu_m\n\n% Define a matrix\nA1 = rand(3000,3000);\n\n% Just use the compute node, no GPU\ntic;\n% Do 1000 FFT's\nfor i = 1:1000\n      B2 = fft(A1);\nend\ntime1 = toc;\nfprintf('%s\\n',\"Time to run FFT on the node:\")\ndisp(time1);\n\n% Use GPU\ntic;\nA2 = gpuArray(A1);\n% Do 1000 FFT's\nfor i = 1:1000\n      % MALAB knows to use GPU FFT because A2 is defined by gpuArray\n        B2 = fft(A2);\nend\ntime2 = toc;\nfprintf('%s\\n',\"Time to run FFT on the GPU:\")\ndisp(time2);\n\n% Will be greater than 1 if GPU is faster\nspeedup = time1/time2 \n</code></pre> We will need to change the partition to <code>gpu</code> to request a gpu. In the Matlab window enter: <pre><code> &gt;&gt; c.AdditionalProperties.AdditionalSubmitArgs=['--partition=gpu --qos=standard --account=PI_UCID --gres=gpu:1 --mem-per-cpu=4G --time=2-00:00:00'] \n</code></pre> </p> <p>Submit the job as before. Since a script is submitted as opposed to a function, only the name of the script is included in the batch command. Do not include the <code>@</code> symbol. In a script there are no inputs or ouptuts. <pre><code> &gt;&gt; j=c.batch('gpu', 'AutoAddClientPath', false) \n</code></pre></p> <p>To get the result: <pre><code> &gt;&gt; j.diary \n</code></pre></p>"},{"location":"Software/math/MATLAB/matlab_local/#load-and-plot-results-from-a-job","title":"Load and Plot Results from A Job","text":"<p>In this section we will run a job on the cluster and then load and plot the results in the local Matlab workspace. The code for this example is as follows: <pre><code>n=100;\ndisp(\"n = \" + n);\nA = gallery('poisson',n-2);\nb = convn(([1,zeros(1,n-2),1]'|[1,zeros(1,n-1)]), 0.5*ones(3,3),'valid')';\nx = reshape(A\\b(:),n-2,n-2)';%\n</code></pre> As before submit the job: <pre><code> &gt;&gt; j=c.batch('plot_demo', 'AutoAddClientPath', false);\n</code></pre></p> <p>To load 'x' into the local Matlab workspace: <pre><code> &gt;&gt; load(j,'x') \n</code></pre></p> <p>Finally, plot the results: <pre><code> &gt;&gt; plot(x) \n</code></pre></p>"},{"location":"Software/md/gromacs/","title":"GROMACS","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.</p> <p>It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p>"},{"location":"Software/md/gromacs/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command GROMACS 2021.5 foss/2021b <code>module load foss/2021b GROMACS/2021.5</code> GROMACS 2021.5-CUDA-11.4.1 foss/2021b <code>module load foss/2021b GROMACS/2021.5-CUDA-11.4.1</code> GROMACS 2024.1 foss/2023b <code>module load foss/2023b GROMACS/2024.1</code> GROMACS 2023.1-CUDA-12.0.0 foss/2022b <code>module load foss/2022b GROMACS/2023.1-CUDA-12.0.0</code> GROMACS 2024.4-CUDA-12.4.0 foss/2022b <code>module load foss/2022b GROMACS/2024.4-CUDA-12.4.0</code>"},{"location":"Software/md/gromacs/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of GROMACS is available at GROMACS Manual, where you can find the tutorials in topologies, input file format, setting parameters, etc. </p>"},{"location":"Software/md/gromacs/#using-gromacs","title":"Using GROMACS","text":"<p>GROMACS can be used on CPU or GPU. When using GROMACS with GPUs (Graphics Processing Units), the calculations can be significantly accelerated, allowing for faster simulations. You can use GROMACS with GPU acceleration, but you need to use GPU nodes on our cluster. </p> Sample Batch Script to Run GROMACS on GPU gmx_gpu.submit.sh Wulver <pre><code>#!/bin/bash -l\n# NOTE the -l (login) flag!\n#SBATCH -J gmx2023\n#SBATCH -o test.%x.%j.out\n#SBATCH -e test.%x.%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --partition=gpu\n#SBATCH --qos=standard\n#SBATCH --time 72:00:00   # Max 3 days\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --cpus-per-task=2\n#SBATCH --gpus-per-node=4  \n#SBATCH --account=PI_ucid  # Replace PI_ucid with the UCID of PI, if you don't know PI's UCID use \"sacctmgr show user username\" on the login screen, replace \"username\" with your UCID\n\nmodule purge\nmodule load wulver\nmodule load foss/2022b GROMACS/2023.1-CUDA-12.0.0\n\nINPUT_DIR=${PWD}/INPUT\nOUTPUT_DIR=${PWD}/OUTPUT\n\ncp -r $INPUT_DIR/* $OUTPUT_DIR/\ncd $OUTPUT_DIR\n\nsrun gmx_mpi mdrun -deffnm run -cpi -v -ntomp 2 -pin on -tunepme -dlb yes -nb gpu -noappend\n</code></pre> Sample Batch Script to Run GROMACS on CPU gmx_cpu.submit.sh Wulver <pre><code>#!/bin/bash -l\n# NOTE the -l (login) flag!\n#SBATCH -J gmx2021\n#SBATCH -o test.%x.%j.out\n#SBATCH -e test.%x.%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --time 72:00:00   # Max 3 days\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --account=PI_ucid  # Replace PI_ucid with the UCID of PI, if you don't know PI's UCID use \"sacctmgr show user username\" on the login screen, replace \"username\" with your UCID\n\nmodule purge\nmodule load wulver\nmodule load foss/2021b GROMACS/2021.5\n\nINPUT_DIR=${PWD}/INPUT\nOUTPUT_DIR=${PWD}/OUTPUT\n\ncp -r $INPUT_DIR/* $OUTPUT_DIR/\ncd $OUTPUT_DIR\n\nsrun gmx_mpi mdrun -v -deffnm em -cpi -v -ntomp 1 -pin on -tunepme -dlb yes -noappend\n</code></pre> <p>The tutorial in the above-mentioned job script can be found in </p> Wulver <p><code>/apps/testjobs/gromacs</code></p>"},{"location":"Software/md/gromacs/#related-applications","title":"Related Applications","text":"<ul> <li>LAMMPS</li> </ul>"},{"location":"Software/md/gromacs/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/md/lammps/","title":"LAMMPS","text":"<p>LAMMPS is a large scale classical molecular dynamics code, and stands for Large-scale Atomic/Molecular Massively Parallel Simulator.  LAMMPS has potentials for soft materials (biomolecules, polymers), solid-state materials (metals, semiconductors) and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, meso, or continuum scale.</p>"},{"location":"Software/md/lammps/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command LAMMPS 23Jun2022-kokkos-CUDA-11.4.1 foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos-CUDA-11.4.1</code> LAMMPS 23Jun2022-kokkos foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos</code> <p>Note</p> <p>To know the deatils about dependent toolchain please go to Toolchains</p>"},{"location":"Software/md/lammps/#application-information-documentation-and-support","title":"Application Information, Documentation and Support","text":"<p>The official LAMMPS is available at LAMMPS Online Manual. LAMMPS has a large user base and a good user support. Question related to using LAMMPS can be posted to the LAMMPS User forum. Archived user mailing list are also useful to resolve some of the common user issues. </p> <p>Tip</p> <p>If after checking the above forum, if you believe that there is an issue with the module, please file a ticket with Service Now</p>"},{"location":"Software/md/lammps/#using-lammps","title":"Using LAMMPS","text":"Sample Batch Script to Run LAMMPS Wulver <pre><code>#!/bin/bash\n#SBATCH -J test_lammps\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128\n#SBATCH --mem-per-cpu=4000M # Maximum allowable memory per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n###############################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load foss/2021b LAMMPS\n\nsrun lmp -in test.in\n</code></pre> <p>Then submit the job script using the sbatch command, e.g., assuming the job script name is <code>test_lammps.slurm</code>:</p> <pre><code>sbatch test_lammps.slurm\n</code></pre>"},{"location":"Software/md/lammps/#building-lammps-from-source","title":"Building LAMMPS from source","text":"<p>Some users may be interested in building LAMMPS from source to enable more specific LAMMPS packages.  The source files for LAMMPS can be downloaded as either a tar file  or from the LAMMPS Github repository. </p> Building on Cluster <p>The following procedure was used to build LAMMPS on Wulver.  In the terminal:</p> WulverLochness <pre><code>module purge\nmodule load wulver\nmodule load foss\nmodule load CMake\n\ngit clone https://github.com/lammps/lammps.git\ncd lammps\nmkdir build\ncd build\n\ncmake -DCMAKE_INSTALL_PREFIX=$PWD/../install_hsw -DCMAKE_CXX_COMPILER=mpicxx \\\n            -DCMAKE_BUILD_TYPE=Release -D BUILD_MPI=yes -DKokkos_ENABLE_OPENMP=ON \\\n            -DKokkos_ARCH_HSW=ON -DCMAKE_CXX_STANDARD=17 -D PKG_MANYBODY=ON \\\n            -D PKG_MOLECULE=ON -D PKG_KSPACE=ON -D PKG_REPLICA=ON -D PKG_ASPHERE=ON \\\n            -D PKG_RIGID=ON -D PKG_KOKKOS=ON -D DOWNLOAD_KOKKOS=ON \\\n            -D CMAKE_POSITION_INDEPENDENT_CODE=ON -D CMAKE_EXE_FLAGS=\"-dynamic\" ../cmake\nmake -j16\nmake install\n</code></pre> <pre><code>module purge\nmodule load foss\nmodule load CMake\n\ngit clone https://github.com/lammps/lammps.git\ncd lammps\nmkdir build\ncd build\n\ncmake -DCMAKE_INSTALL_PREFIX=$PWD/../install_hsw -DCMAKE_CXX_COMPILER=mpicxx \\\n            -DCMAKE_BUILD_TYPE=Release -D BUILD_MPI=yes -DKokkos_ENABLE_OPENMP=ON \\\n            -DKokkos_ARCH_HSW=ON -DCMAKE_CXX_STANDARD=17 -D PKG_MANYBODY=ON \\\n            -D PKG_MOLECULE=ON -D PKG_KSPACE=ON -D PKG_REPLICA=ON -D PKG_ASPHERE=ON \\\n            -D PKG_RIGID=ON -D PKG_KOKKOS=ON -D DOWNLOAD_KOKKOS=ON \\\n            -D CMAKE_POSITION_INDEPENDENT_CODE=ON -D CMAKE_EXE_FLAGS=\"-dynamic\" ../cmake\nmake -j16\nmake install\n</code></pre>"},{"location":"Software/md/lammps/#related-applications","title":"Related Applications","text":"<ul> <li>GROMACS</li> </ul>"},{"location":"Software/md/lammps/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/md/plumed/","title":"PLUMED","text":""},{"location":"Software/md/plumed/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command PLUMED 2.8.0 intel/2021b <code>module load intel/2021b PLUMED/2.8.0</code> PLUMED 2.7.3 foss/2021b <code>module load foss/2021b PLUMED/2.7.3</code> PLUMED 2.9.0 foss/2022b <code>module load foss/2022b PLUMED/2.9.0</code>"},{"location":"Software/md/plumed/#related-applications","title":"Related Applications","text":""},{"location":"Software/md/plumed/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/programming/compilers/","title":"Compilers and Toolchains","text":""},{"location":"Software/programming/compilers/#gnu-and-intel-compilers","title":"GNU and Intel Compilers","text":"<p>We offer both GNU and Intel compilers. Here is the list of compilers you can find on our cluster.</p> Wulver Software Version Dependent Toolchain Module Load Command intel-compilers 2023.2.1 intel/2023b <code>module load intel-compilers/2023.2.1</code> intel-compilers 2022.1.0 intel/2021b <code>module load intel-compilers/2022.1.0</code> intel-compilers 2022.2.1 intel/2022b <code>module load intel-compilers/2022.2.1</code> intel-compilers 2021.4.0 intel/2021b <code>module load intel-compilers/2021.4.0</code> GCC 12.2.0 foss/2022b <code>module load GCC/12.2.0</code> GCC 13.2.0 foss/2023b <code>module load GCC/13.2.0</code> GCC 11.2.0 foss/2021b <code>module load GCC/11.2.0</code>"},{"location":"Software/programming/compilers/#mpi-libraries","title":"MPI Libraries","text":"<p>MPI (Message Passing Interface) libraries are a set of software tools that allow for parallel computing on distributed memory systems, such as computer clusters. These libraries provide a standardized interface for communication between processes running on different nodes of the cluster. There are several implementations of MPI libraries available, such as OpenMPI, MPICH, and Intel MPI. Currently, the following MPI libraries on our cluster.</p> Wulver Software Version Dependent Toolchain Module Load Command impi 2021.4.0 intel/2021b <code>module load intel/2021b</code> impi 2021.7.1 intel/2022b <code>module load intel/2022b</code> impi 2021.10.0 intel/2023b <code>module load intel/2023b</code> impi 2021.6.0 intel/2021b <code>module load intel/2021b</code> OpenMPI 4.1.1 foss/2021b <code>module load foss/2021b</code> OpenMPI 4.1.6 foss/2023b <code>module load foss/2023b</code> OpenMPI 4.1.4 foss/2022b <code>module load foss/2022b</code>"},{"location":"Software/programming/compilers/#toolchains","title":"Toolchains","text":"<p>We use EasyBuild to install the packages as modules and to avoid too many packages tol load as a module, we use pre-defined build environment modules called toolchains which include a combination of tools such as compilers, libraries etc. We use <code>foss</code> and <code>intel</code> toolchains in Wulver. The advantage of using toolchains is that user can load either <code>foss</code> or <code>intel</code> as base package and the additional libraries such as MPI, LAPACK and other math libraries will be automatically loaded. </p>"},{"location":"Software/programming/compilers/#free-open-source-software-foss","title":"Free Open Source Software (foss)","text":"<p>The <code>foss</code> toolchains are versioned with a yearletter scheme, e.g. <code>foss/2021b</code> is the second foss toolchain composed in 2021. The <code>foss</code> toolchain comprises the following </p> <pre><code>\ud83d\udcc1 foss\n\u251c\u2500\u2500 \ud83d\udcc1 gompi \n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 GCC\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 GCCcore\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 zlib\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 binutils\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 OpenMPI\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 numactl\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 XZ\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 libxml2\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 libpciaccess\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 hwloc\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 OpenSSL\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 libevent\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 UCX\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 libfabric\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 PMIx\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 UCC\n\u251c\u2500\u2500 \ud83d\udcc4 FFTW\n\u251c\u2500\u2500 \ud83d\udcc4 OpenBLAS\n\u251c\u2500\u2500 \ud83d\udcc4 ScaLAPACK\n\u2514\u2500\u2500 \ud83d\udcc4 FlexiBLAS\n</code></pre> Wulver Software Version Dependent Toolchain Module Load Command foss 2022b - <code>module load foss/2022b</code> foss 2023b - <code>module load foss/2023b</code> foss 2021b - <code>module load foss/2021b</code> <p>To see GCC and OpenMPI versions details in each toolchain, see the list of compiler versions and OpenMPI versions.</p>"},{"location":"Software/programming/compilers/#intel","title":"Intel","text":"<p>Like <code>foss</code>, <code>intel</code> toolchains are versioned with yearletter scheme, e.g. <code>intel/2021b</code> is the second intel toolchain composed in 2021.</p> <pre><code>\ud83d\udcc1 intel\n\u251c\u2500\u2500 \ud83d\udcc1 intel-compilers\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 GCCcore   \n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 zlib\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 binutils\n\u251c\u2500\u2500 \ud83d\udcc1 iimpi\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 iccifort\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 Intel MPI\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 numactl\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 UCX\n\u2514\u2500\u2500 \ud83d\udcc4 Intel Math Kernel Library\n</code></pre> Wulver Software Version Dependent Toolchain Module Load Command intel 2022b - <code>module load intel/2022b</code> intel 2023b - <code>module load intel/2023b</code> intel 2022a - <code>module load intel/2022a</code> intel 2021b - <code>module load intel/2021b</code> <p>The <code>intel-compilers</code> and <code>impi</code> versions in <code>intel</code> toolchains are tabulated in intel versions and impi versions. To see the versions of <code>GCCcore</code> and <code>mkl</code> libraries of <code>intel</code> toolchain, please load the intel toolchain module with yearletter version, e.g. <code>module load intel/2021b</code> and then use <code>module li</code>.</p>"},{"location":"Software/programming/python/","title":"Python","text":"<p>Python is a high-level, general-purpose programming language. Python supports multiple programming paradigms, including structured, object-oriented, and functional programming. It is used in a wide range of applications such as machine learning, molecular dynamics, scientific computing, automation, image processing, etc.</p>"},{"location":"Software/programming/python/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command Python 3.9.6 foss/2021b <code>module load foss/2021b Python/3.9.6</code> Python 3.10.4 foss/2020a <code>module load foss/2020a Python/3.10.4</code> Python 3.11.5 foss/2023b <code>module load foss/2023b Python/3.11.5</code> Python 3.10.8 foss/2022b <code>module load foss/2022b Python/3.10.8</code>"},{"location":"Software/programming/python/#python-libraries","title":"Python libraries","text":"<p>Apart from Python\u2019s standard library, Python offers a wide range of additional libraries that need to be loaded as modules before users can use these. Here, we list these additional libraries. Please contact us to file a ticket with Service Now in case you do not find the libraries you want to use.</p> Libraries Version Python Version Module load command NumPy 1.21.3 3.9.6 <code>module load foss/2021b SciPy-bundle</code> Matplotlib 3.4.3 3.9.6 <code>module load foss/2021b matplotlib</code> SciPy 2021.10 3.9.6 <code>module load foss/2021b SciPy-bundle</code> <p>For using multiple libraries, you simply need to add the library name in <code>module load</code> command. For example, to load NumPy, Matplotlib, and SciPy together, you need to use the following command. </p> <pre><code>module load foss/2021b SciPy-bundle matplotlib\n</code></pre>"},{"location":"Software/programming/python/#using-conda-or-pip-to-install-python-libraries","title":"Using Conda or <code>pip</code> to Install Python Libraries","text":"<p>Since sometimes users a specific version of Python libraries, it is advisable to use Conda so that users can create their own environment where they can install required packages based on their requirements. Conda is a cross-language package manager that excels at managing environments and dependencies, making it suitable for scientific computing and data science projects. It can handle non-Python libraries and binaries, offering a comprehensive solution. On the other hand, pip is the default Python package installer, known for its simplicity and compatibility with the Python Package Index (PyPI). While both tools serve the same purpose, it is generally recommended to choose one and remain consistent within a project to avoid potential conflicts. Please see Conda Documentation on how to install Python packages via Conda.</p>"},{"location":"Software/programming/python/conda/","title":"Conda","text":"<p>Since Python supports a wide range of additional libraries in machine learning or data science research, it is not always possible to install every package on HPC. Also, users sometimes need to use a specific version of Python or its libraries to conduct their research. Therefore, in that case, users can build their own Python version along with a specific library. One of the ways to accomplish this is to use Conda.</p>"},{"location":"Software/programming/python/conda/#conda","title":"Conda","text":"<p>Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because conda is also an environment manager. </p>"},{"location":"Software/programming/python/conda/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command Miniforge3 24.1.2-0 - <code>module load Miniforge3/24.1.2-0</code> Anaconda3 2023.09-0 - <code>module load Anaconda3/2023.09-0</code> Anaconda3 5.3.0 - <code>module load Anaconda3/5.3.0</code> <p>Warning</p> <p>Users can use Conda after loading any of the modules mentioned above. However, please note that the Anaconda module may be removed in the future, as Anaconda is no longer free for non-profit academic research at institutions with more than 200 employees. While we have not received any official communication from Anaconda yet, please use the Miniforge3 module instead of Anaconda. You do not need to reinstall or recreate any environment if you have already created a Conda environment with Anaconda. Simply load Miniforge3, and the remaining steps are the same.</p>"},{"location":"Software/programming/python/conda/#create-and-activate-a-conda-virtual-environment","title":"Create and Activate a Conda Virtual Environment","text":"<p>Load the Miniforge3 Module</p> <pre><code>module load Miniforge3\n</code></pre>"},{"location":"Software/programming/python/conda/#create-environment-with-conda","title":"Create Environment with <code>conda</code>","text":"<p>To create an environment use the <code>conda create</code> command. Once the environment is created, you need to use <code>conda activate</code> to activate the environment. To create an environment with a specific python version, use <code>conda create --name ENV python=3.9</code> where <code>ENV</code> is the name of the environment. You can choose any environment name of your choice.</p>"},{"location":"Software/programming/python/conda/#activate-and-deactivate-conda-environment","title":"Activate and Deactivate Conda Environment","text":"<p>Once you create an environment, you need to activate the environment to install python packages Use <code>conda activate ENV</code> to activate the Conda environment (<code>ENV</code> is the name of the environment). Following the activation of the conda environment, the name of the environment appears at the left of the hostname in the terminal. </p> <pre><code>login1-41 ~ &gt;: module load Miniforge3\nlogin1-41 ~ &gt;: conda create --name ENV python=3.9\nlogin1-41 ~ &gt;: conda activate ENV\n(ENV) login-41 ~ &gt;:\n</code></pre> <p>Once you finish the installation of Python packages, deactivate the conda environment using <code>conda deactivate ENV</code>. </p> <p>Warning</p> <p>Please note that you may need to create multiple Conda environments, as some packages may not work in a single environment. For example, if you want to install PyTorch and TensorFlow, it's advisable to create separate environments as sometimes both packages in a single environment can cause errors. To create another environment make sure to deactivate the previous environment by using the <code>conda deactivate</code> command. </p>"},{"location":"Software/programming/python/conda/#install-python-packages-via-conda","title":"Install Python Packages Via Conda","text":"<p>Once Conda environment is activated, you can install packages via <code>conda install package_name</code> command. For example, if you want to install <code>matplotlib</code>, you need to use</p> <p><pre><code>(ENV) login-41 ~ &gt;: conda install matplotlib\n</code></pre> Make sure to activate the conda environment prior to installing Python packages. </p>"},{"location":"Software/programming/python/conda/#conda-channel","title":"Conda Channel","text":"<p>Conda Channel refers to a repository or collection of software packages that are available for installation using Conda. Conda Channels are used to organize and distribute packages, and they play a crucial role in the Conda ecosystem. Channels can be specified using the <code>--channel</code> or <code>-c</code> option with the conda install command i.e.  <code>conda install -c channel_name package_name</code>. In the above example, if you want to specify the channel name to install <code>matplotlib</code>, you need to use</p> <p>Note</p> <p>Since memory and CPU usage are limited, it's better to start an interactive session with the compute node whenever you are installing Python packages via Conda.</p> <p><pre><code>(ENV) login-41 ~ &gt;: conda install -c conda-forge matplotlib\n</code></pre> This will install <code>matplotlib</code> from <code>conda-forge</code> channel which is a community-maintained collection of Conda packages where a wide range of packages contributed by the community are available.  Users can prioritize channels by listing them in a specific order, so that Conda searches channels in the order they are listed, installing the first version of a package that it finds. To list the channels, create a file <code>.condarc</code> in the <code>$HOME</code> directory and add the following</p> <p><pre><code>auto_activate_base: false\nchannels:\n  - conda-forge\n</code></pre> The advantage of using <code>.condarc</code> is that you don't have to mention the channel name every time you install a package. However, please note that you still need to use the channel name if you want to install Python packages that require a specific channel other than the <code>conda-forge</code> channel.</p>"},{"location":"Software/programming/python/conda/#examples","title":"Examples","text":"<p>Here, we provide some examples of how to use <code>conda</code> to install application </p>"},{"location":"Software/programming/python/conda/#install-tensorflow-with-gpu","title":"Install TensorFlow with GPU","text":"<p>The following example will create a new conda environment based on Python 3.9 and install TensorFlow in the environment.</p> <pre><code>login1-41 ~ &gt;: module load Miniforge3\nlogin1-41 ~ &gt;: conda create --name tf python=3.9\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/g/guest24/.conda/envs/tf\n\n  added / updated specs:\n    - python=3.9\n\n\nThe following packages will be downloaded:\n\n &lt;output snipped&gt;\n\nProceed ([y]/n)?y\n\n &lt;output snipped&gt;\n#\n# To activate this environment, use\n#\n#     $ conda activate tf\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n</code></pre> <p>Activate the new 'tf' environment <pre><code>login1-41 ~ &gt;: conda activate tf\n(tf) login-41 ~ &gt;:\n</code></pre> Install tensorflow-gpu <pre><code>(tf) node430-41 ~ &gt;: conda install -c anaconda tensorflow-gpu\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/g/guest24/miniconda3/envs/tf\n\n  added / updated specs:\n    - tensorflow-gpu\n\n&lt;output snipped&gt;\n\nThe following packages will be SUPERSEDED by a higher-priority channel:\n\n  ca-certificates                                 pkgs/main --&gt; anaconda\n  certifi                                         pkgs/main --&gt; anaconda\n  openssl                                         pkgs/main --&gt; anaconda\n\n\nProceed ([y]/n)?y\n\n&lt;output snipped&gt;\n\nmkl_fft-1.1.0        | 143 KB    | ####################################################################################### | 100%\nurllib3-1.25.9       | 98 KB     | ####################################################################################### | 100%\ncudatoolkit-10.1.243 | 513.2 MB  | ####################################################################################### | 100%\nprotobuf-3.12.3      | 711 KB    | ####################################################################################### | 100%\nblinker-1.4          | 21 KB     | ####################################################################################### | 100%\nrequests-2.24.0      | 54 KB     | ####################################################################################### | 100%\nwerkzeug-1.0.1       | 243 KB    | ####################################################################################### | 100%\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n</code></pre> Check to see if TensorFlow can be loaded <pre><code>(tf) login1-41 ~ &gt;: python\nPython 3.9.13 (main, Oct 13 2022, 21:15:33)\n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre> Simple TensorFlow test program to make sure the virtual env can access a GPU. Program is called </p> tf.gpu.test.py <pre><code>import tensorflow as tf\n\nif tf.test.gpu_device_name():\n\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n\nelse:\n\n   print(\"Please install GPU version of TF\")\n</code></pre> Slurm script to submit the job Wulver <pre><code>#!/bin/bash -l\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Purge any module loaded by default\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver # Load slurm, easybuild\nmodule load Miniforge3\nconda activate tf\nsrun python tf.gpu.test.py\n</code></pre> <p>Result: <pre><code>Starting /home/g/guest24/.bash_profile ... standard AFS bash profile\n\nHome directory : /home/g/guest24 is not in AFS -- skipping quota check\n\nOn host node430 :\n         17:14:13 up 1 day,  1:17,  0 users,  load average: 0.01, 0.07, 0.06\n\n      Your Kerberos ticket and AFS token status \nklist: No credentials cache found (filename: /tmp/krb5cc_22967_HvCVvuvMMX)\nKerberos :\nAFS      :\n\nLoading default modules ...\nCreate file : \"/home/g/guest24/.modules\" to customize.\n\nNo modules loaded\n2020-07-29 17:14:19.047276: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2020-07-29 17:14:19.059941: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200070000 Hz\n2020-07-29 17:14:19.060093: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ea8ebfdb90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-07-29 17:14:19.060136: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2020-07-29 17:14:19.061484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n\n&lt;ouput snipped&gt;\n\n2020-07-29 17:14:19.817386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n2020-07-29 17:14:19.817392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0\n2020-07-29 17:14:19.817397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N\n2020-07-29 17:14:19.819082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/device:GPU:0 with 15064 MB memory) -&gt; physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:02:00.0, compute capability: 6.0)\nDefault GPU Device: /device:GPU:0\n</code></pre> Next, deactivate the environment using <code>conda deactivate tf</code> command.</p>"},{"location":"Software/programming/python/conda/#install-pytorch-with-gpu","title":"Install PyTorch with GPU","text":"<ul> <li>To install PyTorch with GPU, load the <code>Miniforge3</code> module as described above and then use the following</li> </ul> <pre><code>conda create --name torch-cuda\nconda activate torch-cuda\nconda install -c \"nvidia/label/cuda-12.2.0\" cuda-toolkit\nconda install -c pytorch -c nvidia pytorch torchvision torchaudio pytorch-cuda -y\n</code></pre> <p>Note</p> <p>In the example above, we mentioned the channel name as we intend to install PyTorch and PyTorch-CUDA from a specific channel. For the default channel please see Channels.</p> <ul> <li>Check the Torch version <pre><code>python -c \"import torch; print( torch.__version__)\"\n</code></pre></li> <li>Check the CUDA version <pre><code>python -c \"import torch; print(torch. version .cuda)\"\n</code></pre></li> <li>Check whether Torch is compiled with CUDA <pre><code>python -c \"import torch; print(torch.cuda. is_available())\"\n</code></pre></li> </ul> <p>Important</p> <p>While checking the CUDA version or PyTorch compilation using the commands mentioned above, make sure to start an interactive session on a GPU node; otherwise, the command will not recognize CUDA or the GPU.</p> <ul> <li>A simple PyTorch test program is given below to check whether PyTorch has been installed properly. Program is called</li> </ul> torch_tensor.py <pre><code># -*- coding: utf-8 -*-\n\nimport torch\nimport math\n\n\ndtype = torch.float\n#device = torch.device(\"cpu\")   # Uncomment this to run on CPU\ndevice = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n\n# Create random input and output data\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Randomly initialize weights\na = torch.randn((), device=device, dtype=dtype)\nb = torch.randn((), device=device, dtype=dtype)\nc = torch.randn((), device=device, dtype=dtype)\nd = torch.randn((), device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    if t % 100 == 99:\n        print(t, loss)\n\n    # Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights using gradient descent\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n</code></pre> <p>User can use the following job script to run the script.</p> torch-cuda.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH --job-name=torch_test\n#SBATCH --output=%x.%j.out # %x.%j expands to JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Purge any module loaded by default\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver # Load slurm, easybuild\nmodule load Miniforge3\nconda activate torch-cuda\nsrun python touch_tensor.py\n</code></pre> <p>Warning</p> <p>When working with Python, it is generally advised to avoid mixing package management tools such as pip and conda within the same environment. Pip and Conda manage dependencies differently, and their conflict can lead to compatibility issues and unexpected behavior. Mixing the two can result in an environment where packages installed with one tool may not interact seamlessly with those installed using the other. </p>"},{"location":"Software/programming/python/conda/#mamba-the-conda-alternative","title":"Mamba: The Conda Alternative","text":"<p>Mamba is a fast, robust, and cross-platform package manager and particularly useful for building complicated environments, where <code>conda</code> is unable to 'solve' the required set of packages within a reasonable amount of time. Users can install packages with <code>mamba</code> in the same way as with <code>conda</code>. <pre><code>module load Miniforge3\n\n# create new environment\nmamba create --name env_name python numpy pandas \n# install a new package into an existing environment\nconda activate env_name\nmamba install scipy\n</code></pre></p>"},{"location":"Software/programming/python/conda/#example-of-installing-pytorch-via-mamba","title":"Example of Installing PyTorch via mamba","text":"<pre><code>module load Miniforge3\nconda create --name torch-cuda\nconda activate torch-cuda\nmamba install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n</code></pre> <p>This will install pytorch in the <code>torch-cuda</code> environment.</p> <p>Warning</p> <p>Ensure to unload the Mamba module, as when you use <code>Anaconda3</code> and <code>Mamba</code> together, Mamba installs the PyTorch packages within the Conda environment. Consequently, the next time you activate this environment, there's no need to load the 'Mamba' module. Only employ 'Mamba' when installing the packages.</p>"},{"location":"Software/programming/python/conda/#export-and-import-conda-environment","title":"Export and Import Conda Environment","text":"<p>Exporting and importing Conda environments allows users to capture and reproduce the exact set of dependencies for a project. With Conda, a popular package and environment management system, users can export an environment, including all installed packages, into a YAML file. This file can then be shared or version-controlled. Importing the environment from the YAML file on another system ensures consistent dependencies, making it easier to recreate the development or execution environment. </p> <p>Tips</p> <p>When installing Python packages via Conda, ensure that you perform the installation on the compute node rather than the login node. The CPU and memory resources on login nodes are limited, and installing Python packages on the login node can be time-consuming. To avoid this, initiate an tnteractive session with compute node.</p>"},{"location":"Software/programming/python/conda/#export-conda-environment","title":"Export Conda Environment","text":"<p>To export a conda environment to a new directory or a different machine, you need to activate the environment first that you intend to export. Please see Conda environment on how to activate the environment. Once your environment is activated, you can export it to a YAML file: <pre><code>conda env export &gt; my_environment.yml\n</code></pre> The YAML should look like this</p> <p><pre><code>name: my_env\nchannels:\n- defaults\ndependencies:\n- _libgcc_mutex=0.1=main\n- _openmp_mutex=5.1=1_gnu\n- blas=1.0=mkl\n\n&lt;ouput snipped&gt;\n\n#the last line is the path of the env\nprefix: /home/a/abc3/.conda/envs/my_env.\n</code></pre> Next, edit the <code>my_environment.yml</code> file to make sure it has the correct environment name and other settings. The last line of the file specifies the path of the environment.</p> <p>Once the YAML file is ready, you can transfer the <code>my_environment.yml</code> file to the new machine or directory where you want to replicate the environment. See cluster file transfer for details on transferring the files to clusters.</p>"},{"location":"Software/programming/python/conda/#set-different-location-for-conda-environment-and-package","title":"Set Different Location for Conda Environment and Package","text":"<p>Since Conda, by default, downloads packages and creates environments in the <code>$HOME</code> directory, users might encounter disk quota errors if multiple environments are created. In such cases, please follow the above steps to move the existing environments from <code>$HOME</code> to <code>/project</code>. For future environment and package downloads, create a <code>.condarc</code> file in the <code>$HOME</code> directory and add the following: <pre><code>auto_activate_base: false\nenvs_dirs:\n  - /path/to/custom/conda/envs/directory\npkgs_dirs:\n  - /path/to/custom/conda/pkgs\n</code></pre></p> <p>Replace <code>/path/to/custom/conda/envs/directory</code> with the path you want to use. </p>"},{"location":"Software/programming/python/conda/#import-environment-on-new-machine","title":"Import Environment on New Machine","text":"<p>On the new machine, first load Anaconda and initialize conda as before. Then, create the environment from the YAML file:</p> <p><pre><code>conda env create -f my_environment.yml\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n&lt;ouput snipped&gt;\n\nDownloading and Extracting Packages\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n# $ conda activate my_env\n#\n# To deactivate an active environment, use\n#\n# $ conda deactivate\n</code></pre> After running this command, Conda will set up the environment as it was on the original machine, including downloading and installing packages. To activate the New Environment use <code>conda activate my_env</code> where <code>my_env</code> is the environment name. You can check your current environments using <code>conda env list</code>.</p>"},{"location":"Software/programming/python/conda/#importing-to-a-different-location","title":"Importing to a Different Location","text":"<p>If you want to import the conda environment to a different location, use the <code>--prefix</code> or <code>-p</code> option <pre><code>conda env create -f my_environment.yml -p /project/hpcadmins/abc3/conda_env/my_env\n</code></pre> This will create the environment in the specified directory instead of the default conda environment directory. Please note that in that case, you need to provide the full path of the environment to activate it.</p> <p><pre><code>conda activate /project/hpcadmins/abc3/conda_env/my_env\n(/project/hpcadmins/abc3/conda_env/my_env) abc3@login01:~$ conda env list\n# conda environments:\n#\nbase /apps/easybuild/software/Miniforge3/2023.09-0\n* /project/hpcadmins/abc3/conda_env/my_env\n</code></pre> By following these steps, you can successfully export a conda environment from one machine and import it to another, ensuring a consistent working environment across different machines or directories.</p> <p>Warning</p> <p>It is advisable to use the <code>/project</code> directory to store the Conda environment rather than using the <code>$HOME</code> directory. On Wulver, the storage space on <code>$HOME</code> is limited (50G) and cannot be increased. See Wulver Filesystems for details. </p>"},{"location":"Software/programming/python/conda/#conda-user-commands","title":"Conda User Commands","text":"Task Command Activate environment: <code>conda activate [environment_name]</code> Deactivate environment: <code>conda deactivate [environment_name]</code> Show the list of environments: <code>conda env list</code> Delete environment: <code>conda remove --name [environment_name] --all</code> Export environment: <code>conda env export &gt; [environment_name].yml</code> Import environment from YAML: <code>conda env create -f [environment_name].yml</code> Import environment to different location: <code>conda env create -f [environment_name].yml -p [PATH]</code>"},{"location":"Software/programming/python/jupyter/","title":"Jupyter Notebooks","text":"<p>The Jupyter Notebook is a web-based interactive computing platform. The notebook combines live code, equations, narrative text, and visualizations. In our cluster, we have JupyterLab which is the next-generation user interface for Project Jupyter offering all the familiar building blocks of the classic Jupyter Notebook (notebook, terminal, text editor, file browser, rich outputs, etc.) in a flexible and powerful user interface. </p>"},{"location":"Software/programming/python/jupyter/#using-jupyter-notebook-on-wulver","title":"Using Jupyter Notebook on Wulver","text":"<p>Jupyter Notebook via slurm on Wulver is deprecated </p> <p>Since two-factor authentication has been implemented on Wulver, the use of Jupyter Notebook via SLURM scripts has been discontinued and is no longer supported. Users should use OnDemand to use Jupyter Notebook on Wulver. First, users need to install Jupyter Notebook in their Conda environment. Once the Conda Environment is activated, users can install Jupyter Notebook using the command <code>conda install -c conda-forge jupyter notebook</code>. Then, you need to specify the environment in OnDemand to start the Jupyter Notebook session. Check here for details.</p>"},{"location":"Software/programming/python/jupyter/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/slurm/slurm/","title":"SLURM","text":"<p>Slurm (Simple Linux Utility for Resource Management) is an open-source workload manager and job scheduler designed for high-performance computing clusters. It is widely used in research, academia, and industry to efficiently manage and allocate computing resources such as CPUs, GPUs, memory, and storage for running various types of jobs and tasks. Slurm helps optimize resource utilization, minimizes job conflicts, and provides a flexible framework for distributing workloads across a cluster of machines. It offers features like job prioritization, fair sharing of resources, job dependencies, and real-time monitoring, making it an essential tool for orchestrating complex computational workflows in diverse fields.</p>"},{"location":"Software/slurm/slurm/#availability","title":"Availability","text":"Software Module Load Command slurm <code>module load wulver</code> <p>Please note that the module <code>wulver</code> is already loaded when a user logs in to the cluster. If you use <code>module purge</code> command, make sure to use <code>module load wulver</code> in the slurm script to load SLURM.</p>"},{"location":"Software/slurm/slurm/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of SLURM is available at SLURM manual. </p>"},{"location":"Software/slurm/slurm/#managing-and-monitoring-jobs","title":"Managing and Monitoring Jobs","text":"<p>SLURM has numerous tools for monitoring jobs. Below are a few to get started. More documentation is available on the SLURM website.</p> <p>The most common commands are: </p> <ul> <li>List all current jobs: <code>squeue</code></li> <li>Job deletion: <code>scancel [job_id]</code></li> <li>Run a job: <code>sbatch [submit script]</code></li> <li>Run a command: <code>srun &lt;slurm options&gt; &lt;command name&gt;</code></li> </ul>"},{"location":"Software/slurm/slurm/#slurm-user-commands","title":"SLURM User Commands","text":"Task Command Job submission: <code>sbatch [script_file]</code> Job deletion: <code>scancel [job_id]</code> Job status by job: <code>squeue [job_id]</code> Job status by user: <code>squeue -u [user_name]</code> Job hold: <code>scontrol hold [job_id]</code> Job release: <code>scontrol release [job_id]</code> List enqueued jobs: <code>squeue</code> List nodes: <code>sinfo -N OR scontrol show nodes</code> Cluster status: <code>sinfo</code>"},{"location":"Software/slurm/slurm/#using-slurm-on-wulver","title":"Using SLURM on Wulver","text":"<p>In Wulver, SLURM submission will have new requirements, intended for a more fair sharing of resources without impinging on investor/owner rights to computational resources.  All jobs must now be charged to a PI-group (Principal Investigator) account.</p>"},{"location":"Software/slurm/slurm/#account-use-account","title":"Account (Use <code>--account</code>)","text":"<p>To specify the job, use <code>--account=PI_ucid</code>.  You can specify <code>--account</code> as either an <code>sbatch</code> or <code>#SBATCH</code> parameter. If you don't know the UCID of PI, use<code>quota_info</code>, and you will see SLURM account you sre associated with. Check <code>quota_info</code> for details.</p>"},{"location":"Software/slurm/slurm/#partition-use-partition","title":"Partition (Use <code>--partition</code>)","text":"<p>Wulver has three partitions, differing in GPUs or RAM available:</p> Partition Nodes Cores/Node CPU GPU Memory Service Unit (SU) Charge <code>--partition=general</code> 100 128 2.5G GHz AMD EPYC 7763 (2) NA 512 GB 1 SU per hour per cpu <code>--partition=debug</code> 1 4 2.5G GHz AMD EPYC 7763 (2) NA 512 GB No charges, must be used with <code>--qos=debug</code> <code>--partition=gpu</code> 25 128 2.0 GHz AMD EPYC 7713 (2) NVIDIA A100 GPUs (4) 512 GB 3 SU per hour per GPU node <code>--partition=bigmem</code> 2 128 2.5G GHz AMD EPYC 7763 (2) NA 2 TB 1.5 SU per CPU hour"},{"location":"Software/slurm/slurm/#priority-use-qos","title":"Priority (Use <code>--qos</code>)","text":"<p>Wulver has three levels of \u201cpriority\u201d, utilized under SLURM as Quality of Service (QoS):  Qos Purpose Rules Wall time limit (hours) Valid Users <code>--qos=standard</code> Normal jobs. Faculty PIs are allocated 300,000 Service Units (SU) per year SU charges based on node type (see partitions table above), jobs can be preempted by high QoS enqueued jobs 72 Everyone <code>--qos=low</code> Free access, no SU charge jobs can be preempted by high or standard QoS enqueued jobs 72 Everyone <code>--qos=high_$PI</code> Replace <code>$PI</code> with the UCID of PI, only available to owners/investors Highest Priority Jobs, no SU Charges. 72 owner/investor PI Groups <code>--qos=debug</code> Intended for debugging and testing jobs No SU Charges, maximum 4 CPUs are allowed, must be used with <code>--partition=debug</code> 8 Everyone </p>"},{"location":"Software/slurm/slurm/#check-quota","title":"Check Quota","text":"<p>Faculty PIs are allocated 300,000 Service Units (SU) per year upon request at no cost, which can be utilized via <code>--qos=standard</code> on the SLURM job. It's important to regularly check the usage of SUs so that users can be aware of their consumption and switch to <code>--qos=low</code> to prevent exhausting all allocated SUs. Users can check their quota using the <code>quota_info UCID</code> command.  <pre><code>[ab1234@login01 ~]$ module load wulver\n[ab1234@login01 ~]$ quota_info $LOGNAME\nUsage for account: xy1234\n   SLURM Service Units (CPU Hours): 277557 (300000 Quota)\n     User ab1234 Usage: 1703 CPU Hours (of 277557 CPU Hours)\n   PROJECT Storage: 867 GB (of 2048 GB quota)\n     User ab1234 Usage: 11 GB (No quota)\n   SCRATCH Storage: 791 GB (of 10240 GB quota)\n     User ab1234 Usage: 50 GB (No quota)\nHOME Storage ab1234 Usage: 0 GB (of 50 GB quota)\n</code></pre> Here, <code>xy1234</code> represents the UCID of the PI, and \"SLURM Service Units (CPU Hours): 277557 (300000 Quota)\" indicates that members of the PI group have already utilized 277,557 CPU hours out of the allocated 300,000 SUs, and the user <code>xy1234</code> utilized 1703 CPU Hours out of 277,557 CPU Hours. This command also displays the storage usage of directories such as <code>$HOME</code>, <code>/project</code>, and <code>/scratch</code>. Users can view both the group usage and individual usage of each storage. In the given example, the group usage from the 2TB project quota is 867 GB, with the user's usage being 11 GB out of that 867 GB. For more details file system quota, see Wulver Filesystem.</p>"},{"location":"Software/slurm/slurm/#example-of-slurm-script","title":"Example of slurm script","text":""},{"location":"Software/slurm/slurm/#submitting-jobs-on-cpu-nodes","title":"Submitting Jobs on CPU Nodes","text":"Sample Job Script to use: submit.sh <pre><code>#!/bin/bash -l\n#SBATCH --job-name=job_nme\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=59:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4000M\n</code></pre> <ul> <li>Here, the job requests 1 node with 8 cores, on the <code>general</code> partition with <code>qos=standard</code>. Please note that the memory relies on the number of cores you are requesting. </li> <li>As per the policy, users can request up to 4GB memory per core, therefore the flag  <code>--mem-per-cpu</code> is used for memory requirement. </li> <li>In this above script <code>--time</code> indicates the wall time which is used to specify the maximum amount of time that a job is allowed to run. The maximum allowable wall time depends on SLURM QoS, which you can find in QoS. </li> <li>To submit the job, use <code>sbatch submit.sh</code> where the <code>submit.sh</code> is the job script. Once the job has been submitted, the jobs will be in the queue, which will be executed based on priority-based scheduling. </li> <li>To check the status of the job use <code>squeue -u $LOGNAME</code> and you should see the following  <pre><code>  JOBID PARTITION     NAME     USER  ST    TIME    NODES  NODELIST(REASON)\n   635   general     job_nme   ucid   R   00:02:19    1      n0088\n</code></pre> Here, the <code>ST</code> stands for the status of the job. You may see the status of the job <code>ST</code> as <code>PD</code> which means the job is pending and has not been assigned yet. The status change depends upon the number of users using the partition and resources requested in the job. Once the job starts, you will see the output file with an extension of <code>.out</code>. If the job causes any errors, you can check the details of the error in the file with the <code>.err</code> extension.</li> </ul>"},{"location":"Software/slurm/slurm/#submitting-jobs-on-gpu-nodes","title":"Submitting Jobs on GPU Nodes","text":"<p>In case of submitting the jobs on GPU, you can use the following SLURM script </p> Sample Job Script to use: gpu_submit.sh <pre><code>#!/bin/bash -l\n#SBATCH --job-name=gpu_job\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=gpu\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --gres=gpu:2\n#SBATCH --time=59:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4000M\n</code></pre> <p>This will request 2 GPUS per node on the <code>GPU</code> partition.</p>"},{"location":"Software/slurm/slurm/#submitting-jobs-on-debug","title":"Submitting Jobs on <code>debug</code>","text":"<p>The <code>debug</code> QoS in Slurm is intended for debugging and testing jobs. It usually provides a shorter queue wait time and quicker job turnaround. Jobs submitted with the <code>debug</code> QoS have access to a limited set of resources (Only 4 CPUS on Wulver), making it suitable for rapid testing and debugging of applications without tying up cluster resources for extended periods. </p> Sample Job Script to use: debug_submit.sh <pre><code>#!/bin/bash -l\n#SBATCH --job-name=debug\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=debug\n#SBATCH --qos=debug\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --time=7:59:00  # D-HH:MM:SS, Maximum allowable Wall Time 8 hours\n#SBATCH --mem-per-cpu=4000M\n</code></pre> <p>To submit the jobs, <code>sbatch</code> command.</p>"},{"location":"Software/slurm/slurm/#interactive-session-on-a-compute-node","title":"Interactive session on a compute node","text":"<p>Interactive sessions are useful for tasks that require direct interaction with the compute node's resources and software environment. To start an interactive session on the compute node, use <code>interactive</code> after logging into Wulver.</p>"},{"location":"Software/slurm/slurm/#the-interactive-command","title":"The <code>interactive</code> Command","text":"<p>We provide a built-in shortcut command, <code>interactive</code>, that allows you to quickly and easily request a session in compute node.</p> <p>The <code>interactive</code> command acts as a convenient wrapper for Slurm\u2019s salloc command. Similar to sbatch, which is used for batch jobs, <code>salloc</code> is specifically designed for interactive jobs. </p> <pre><code>$ interactive -h\nUsage: interactive -a ACCOUNT -q QOS -j JOB_TYPE\nStarts an interactive SLURM job with the required account and QoS settings.\n\nRequired options:\n  -a ACCOUNT       Specify the account to use.\n  -q QOS           Specify the quality of service (QoS).\n  -j JOB_TYPE      Specify the type of job: 'cpu' for CPU jobs or 'gpu' for GPU jobs.\n\nExample: Run an interactive GPU job with the 'test' account and 'test' QoS:\n  /apps/site/bin/interactive -a test -q test -j gpu\n\nThis will launch an interactive job on the 'gpu' partition with the 'test' account and QoS 'test',\nusing 1 GPU, 1 CPU, and a walltime of 1 hour by default.\n\nOptional parameters to modify resources:\n  -n NTASKS        Specify the number of CPU tasks (Default: 1).\n  -t WALLTIME      Specify the walltime in hours (Default: 1).\n  -g GPU           Specify the number of GPUs (Only for GPU jobs, Default: 1).\n  -p PARTITION     Specify the SLURM partition (Default: 'general' for CPU jobs, 'gpu' for GPU jobs).\n\nUse '-h' to display this help message.\n</code></pre> CPU NodesGPU NodesDebug Nodes <pre><code>$ interactive -a $PI_UCID -q standard -j cpu\nStarting an interactive session with the general partition and 1 core for 01:00:00 of walltime in standard priority\nsalloc: Pending job allocation 466577\nsalloc: job 466577 queued and waiting for resources\nsalloc: job 466577 has been allocated resources\nsalloc: Granted job allocation 466577\nsalloc: Nodes n0103 are ready for job   \n</code></pre> <pre><code>$ interactive -a $PI_UCID -q standard -j gpu\nStarting an interactive session with the GPU partition, 1 core and 1 GPU for 01:00:00 of walltime in standard priority\nsalloc: Pending job allocation 466579\nsalloc: job 466579 queued and waiting for resources\nsalloc: job 466579 has been allocated resources\nsalloc: Granted job allocation 466579\nsalloc: Nodes n0048 are ready for job\n</code></pre> <pre><code>$ interactive -a $PI_UCID -q debug -j cpu -p debug\nStarting an interactive session with the debug partition and 1 core for 01:00:00 of walltime in debug priority\nsalloc: Pending job allocation 466581\nsalloc: job 466581 queued and waiting for resources\nsalloc: job 466581 has been allocated resources\nsalloc: Granted job allocation 466581\nsalloc: Waiting for resource configuration\nsalloc: Nodes n0127 are ready for job\n</code></pre> <p>Replace <code>$PI_UCID</code> with PI's NJIT UCID.  Now, once you get the confirmation of job allocation, you can either use <code>srun</code> or <code>ssh</code> to access the particular node allocated to the job. </p>"},{"location":"Software/slurm/slurm/#customizing-your-resources","title":"Customizing Your Resources","text":"<p>Please note that, by default, this interactive session will request 1 core (for CPU jobs), 1 GPU (for GPU jobs), with a 1-hour walltime. To customize the resources, use the <code>-h</code> option for help. Run <code>interactive -h</code> for more details. Here is an explanation of each flag given below.</p> Flag Explanation Example <code>-a</code> Mandatory option. This is followed by your group's name. Use <code>quota_info</code> to check the account/group name. <code>-a $PI_UCID</code> <code>-q</code> Mandatory option. Used to access the priority <code>-q standard</code> <code>-j</code> Mandatory option. Specify whether you want CPU or GPU node. <code>-j cpu</code> <code>-n</code> Optional. The total number of CPUs. Default is 1 core unless specified. <code>-n 1</code> <code>-t</code> Optional. The amount of walltime to reserve for your job in hours. Default is 1 hour unless specified. <code>-t 1</code> <code>-g</code> Optional. The total number of GPUs. Default is 1 GPU unless specified. <code>-g 1</code> <code>-p</code> Optional. Specify the name of the partition. (Default: <code>general</code> for CPU jobs, <code>gpu</code> for GPU jobs). <code>-p debug</code> <p>Warning</p> <p>Login nodes are not designed for running computationally intensive jobs. You can use the head node to edit and manage your files, or to run small-scale interactive jobs. The CPU usage is limited per user on the head node. Therefore, for serious computing either submit the job using <code>sbatch</code> command or start an interactive session on the compute node.</p> <p>Note</p> <p>Please note that if you are using GPUs, check whether your script is parallelized. If your script is not parallelized and only depends on GPU, then you don't need to request more cores per node. In that case use <code>--ntasks-per-node=1</code>, as this will request 1 CPU per GPU. It's important to keep in mind that using multiple cores on GPU nodes may result in unnecessary CPU hour charges. Additionally, implementing this practice can make service unit accounting significantly easier.</p>"},{"location":"Software/slurm/slurm/#additional-resources","title":"Additional Resources","text":"<ul> <li>SLURM Tutorial List</li> </ul>"},{"location":"Software/utilities/apptainer/","title":"Apptainer","text":"<p>Apptainer (formerly Singularity) s a container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Apptainer on your laptop, and then run it on many of the largest HPC clusters in the world, local university or company clusters, a single server, in the cloud, or on a workstation down the hall. Your container is a single file, and you don\u2019t have to worry about how to install all the software you need on each different operating system.</p> <p>Apptainer was created to run complex applications on HPC clusters in a simple, portable, and reproducible way. First developed at Lawrence Berkeley National Laboratory, it quickly became popular at other HPC sites, academic sites, and beyond. Apptainer is an open-source project, with a friendly community of developers and users. The user base continues to expand, with Apptainer now used across industry and academia in many areas of work.</p> <p>Many container platforms are available, but Apptainer is focused on:</p> <ul> <li>Verifiable reproducibility and security, using cryptographic signatures, an immutable container image format, and in-memory decryption.</li> <li>Integration over isolation by default. Easily make use of GPUs, high-speed networks, parallel filesystems on a cluster or server by default.</li> <li>Mobility of computing. The single file SIF container format is easy to transport and share.</li> <li>A simple, effective security model. You are the same user inside a container as outside, and cannot gain additional privilege on the host system by default. Read more about Security in Apptainer.</li> </ul>"},{"location":"Software/utilities/apptainer/#use-cases","title":"Use Cases","text":"<ul> <li>BYOE: Bring Your Own Environment!</li> <li>Reproducible science</li> <li>Commercially supported code requiring a particular environment</li> <li>Static environments (software appliances)</li> <li>Legacy code on old operating systems</li> <li>Complicated software stacks that are very host specific</li> <li>Complicated work-flows that require custom installation and/or data</li> </ul>"},{"location":"Software/utilities/dmtcp/","title":"DMTCP","text":""},{"location":"Software/utilities/dmtcp/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command DMTCP 3.0.0 - <code>module load DMTCP/3.0.0</code> DMTCP 2.6.0 - <code>module load DMTCP/2.6.0</code>"},{"location":"Software/utilities/dmtcp/#related-applications","title":"Related Applications","text":""},{"location":"Software/utilities/dmtcp/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/utilities/parallel/","title":"GNU Parallel","text":"<p>GNU Parallel is a shell tool for executing jobs in parallel using one or more computers. A job can be a single command or a small script that has to be run for each of the lines in the input. The typical input is a list of files, a list of hosts, a list of users, a list of URLs, or a list of tables. A job can also be a command that reads from a pipe. GNU parallel can then split the input and pipe it into commands in parallel.</p> <p>Tip</p> <p>You can use GNU Parallel on your Mac OSX laptop/desktop to run applications in parallel. Just install using either brew or Macports</p> <pre><code>sudo port install parallel\nbrew install parallel\n</code></pre>"},{"location":"Software/utilities/parallel/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command parallel 20230722 foss/2022b <code>module load foss/2022b parallel/20230722</code>"},{"location":"Software/utilities/parallel/#usage","title":"Usage","text":"Usage <pre><code>module load parallel/20200422\nparallel echo ::: 1 2 3 ::: 4 5 6\n</code></pre>"},{"location":"Software/utilities/parallel/#purpose","title":"Purpose","text":"<p>GNU Parallel is a tool for running a series of jobs, mostly serial jobs, in parallel. This tool is best suited for running a bunch of serial jobs that may not run for the same simulation time. For example, the most easiest way to run a series of serial jobs in parallel on n cpus within one submit script is as follows</p> <pre><code>./job_1 &amp;\n./job_2 &amp;\n...\n./job_n &amp;\nwait\n./job_(n+1) &amp;\n./job_(n+2) &amp;\n...\n./job_2n &amp;\nwait\n./job_(2n+1) &amp;\n./job_(2n+2) &amp;\n...\n./job_3n &amp;\nwait\n</code></pre> <p>This works most efficiently when all jobs have the same or almost the same run time. If run times for jobs are unequal, then n jobs are run simultaneously and the cpus remain idle until all n jobs are completed before looping through the next n jobs. This will lead to idle time and inefficient consumption of cpu time.</p> <p></p> <p>GNU Parallel solves this issue by first launching n jobs. When one job completes, then the next job in sequence is started. This permits efficient use of cpu time by reducing the wait time and letting a number of small jobs to run while some cpus work on longer jobs.</p> <pre><code>parallel job_{1} ::: $(seq 1 3n)\n</code></pre> <p></p>"},{"location":"Software/utilities/parallel/#single-node-example","title":"Single Node example","text":""},{"location":"Software/utilities/parallel/#multi-node-example","title":"Multi Node example","text":""},{"location":"Software/utilities/parallel/#related-links","title":"Related Links","text":"<ul> <li>https://www.gnu.org/software/parallel/parallel_tutorial.html</li> <li>https://www.biostars.org/p/63816/</li> <li>http://www.shakthimaan.com/posts/2014/11/27/gnu-parallel/news.html</li> <li>https://www.msi.umn.edu/support/faq/how-can-i-use-gnu-parallel-run-lot-commands-parallel</li> <li>https://github.com/LangilleLab/microbiome_helper/wiki/Quick-Introduction-to-GNU-Parallel</li> <li>https://davetang.org/muse/2013/11/18/using-gnu-parallel/</li> <li>https://sites.google.com/a/stanford.edu/rcpedia/parallel-processing/gnu-parallel-examples</li> <li>http://phili.pe/posts/free-concurrency-with-gnu-parallel/</li> </ul>"},{"location":"Software/utilities/parallel/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/ase/","title":"ASE","text":"<p>ASE (Atomic Simulation Environment) is an open-source Python library for performing atomic-scale simulations of materials. It provides a collection of tools and interfaces for setting up, running, and analyzing simulations of atoms, molecules, and solids using a variety of simulation packages.</p> <p>ASE is designed to be flexible and modular, allowing users to easily switch between different simulation packages and methods without having to modify their code. It currently supports a number of popular simulation packages, including Quantum ESPRESSO, LAMMPS, and GROMACS etc.</p> <p>ASE provides a wide range of functionality for working with atomic-scale simulations, including geometry optimization, molecular dynamics simulations, electronic structure calculations, and vibrational analysis. It also includes a number of built-in tools for analyzing simulation output, such as calculating radial distribution functions, computing surface area and volume, and visualizing simulation trajectories.</p> <p>ASE is actively developed and maintained by a community of researchers and developers from around the world, and is available as a free download under an open-source license.</p>"},{"location":"Software/visualization/ase/#availability","title":"Availability","text":"Wulver <p> Software Version Dependent Toolchain Module Load Command  ```</p>"},{"location":"Software/visualization/ase/#related-applications","title":"Related Applications","text":""},{"location":"Software/visualization/ase/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/gnuplot/","title":"GNUPlot","text":""},{"location":"Software/visualization/gnuplot/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command gnuplot 5.4.2 foss/2021b <code>module load foss/2021b gnuplot/5.4.2</code> gnuplot 5.4.6 foss/2022b <code>module load foss/2022b gnuplot/5.4.6</code>"},{"location":"Software/visualization/gnuplot/#related-applications","title":"Related Applications","text":""},{"location":"Software/visualization/gnuplot/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/ovito/","title":"OVITO","text":"<p>OVITO is a scientific visualization and analysis software tool designed for the analysis of atomistic simulation data. It is used primarily in the field of materials science and computational chemistry, and is particularly well-suited for the analysis of molecular dynamics simulations and other atomistic simulation techniques.</p> <p>OVITO provides a graphical user interface (GUI) that allows users to interactively visualize and analyze large datasets of atomistic simulation data. It supports a wide range of file formats commonly used in molecular dynamics simulations, including LAMMPS, GROMACS, and DL_POLY.</p> <p>In addition to its visualization capabilities, OVITO also provides a set of built-in analysis tools that allow users to compute various properties of the simulated systems. These include calculation of radial distribution functions, Voronoi tessellation, and common structural analysis metrics like bond order parameters and coordination numbers.</p>"},{"location":"Software/visualization/ovito/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command OVITO 3.7.11-basic foss/2021b <code>module load foss/2021b OVITO/3.7.11-basic</code>"},{"location":"Software/visualization/ovito/#related-applications","title":"Related Applications","text":""},{"location":"Software/visualization/ovito/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/paraview/","title":"Paraview","text":"<p>ParaView is an open-source, cross-platform data visualization and analysis tool that allows users to create visualizations and analyze large datasets. It was developed to process and visualize scientific and engineering data, such as computational fluid dynamics (CFD) simulations, seismic data, medical imaging, and climate data.</p> <p>ParaView provides a graphical user interface (GUI) that enables users to interactively explore and visualize data. It supports a wide range of data formats and allows users to customize and manipulate visualizations to suit their needs. Additionally, ParaView provides a scripting interface that allows users to automate repetitive tasks and create custom analysis pipelines.</p> <p>ParaView is widely used in a variety of scientific and engineering fields, including aerospace, automotive, biomedical, energy, and geosciences, among others. It is actively developed and maintained by Kitware, a software company that specializes in open-source solutions for scientific computing and data analysis.</p>"},{"location":"Software/visualization/paraview/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command ParaView 5.11.2-egl - <code>module load ParaView/5.11.2-egl</code> ParaView 5.11.2-osmesa - <code>module load ParaView/5.11.2-osmesa</code> ParaView 5.11.0-osmesa - <code>module load ParaView/5.11.0-osmesa</code> ParaView 5.11.0-egl - <code>module load ParaView/5.11.0-egl</code> ParaView 5.9.1-mpi foss/2021b <code>module load foss/2021b ParaView/5.9.1-mpi</code> ParaView 5.11.0-mpi foss/2022b <code>module load foss/2022b ParaView/5.11.0-mpi</code> ParaView 5.11.1 foss/2022b <code>module load foss/2022b ParaView/5.11.1</code>"},{"location":"Software/visualization/paraview/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of ParaView is available at ParaView manual. To use ParaView on the cluster, users need to use the same version of ParaView on their local machine. You can download the ParaView from ParaView official download page</p>"},{"location":"Software/visualization/paraview/#using-paraview","title":"Using ParaView","text":"<p>ParaView supports GPU acceleration, which can significantly improve performance and reduce processing times for certain types of data and operations. GPU acceleration is particularly useful for large datasets with many points or cells, as well as for operations such as volume rendering and streamlines. You can use ParaView with GPU acceleration, but you need to use GPU nodes on our cluster. ParaView is also designed to work in parallel environments, and it supports the Message Passing Interface (MPI) standard for distributed computing. With MPI support, ParaView can be used to visualize and analyze large-scale datasets on clusters. </p> Sample Batch Script to Run ParaView with MPI support: pvserver_cpu.submit.sh <pre><code>#!/bin/bash -l\n#SBATCH --job-name=pvserver_cpu\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load the slurm, easybuild \nmodule load ParaView/5.11.0-osmesa \n################################################\n#\n# Open an ssh tunnel to the login node\n#\n################################################\n# Run on random port\nport=$(shuf -i 6000-9999 -n 1)\nHOST=$(hostname)\nif [ $(hostname) == $HOST ]; then\n  /usr/bin/ssh -N -f -R $port:localhost:$port login01.tartan.njit.edu\nfi\n################################################\ncat&lt;&lt;EOF\n\npvserver is running on: $(hostname)\nJob starts at: $(date)\n\nStep 1: Create SSH tunnel\n\nOpen new terminal window, and run:\n(If you are off campus you will need VPN running)\n\nssh -L $port:localhost:$port $USER@login01.tartan.njit.edu\nEOF\n################################################\n# Run MPI pvserver\n#\n################################################\nhost_list=$(srun hostname -s | sort | uniq | paste -s -d, -)\nmpiexec -np $SLURM_NTASKS -rmk slurm -hosts $host_list pvserver --server-port=$port --force-offscreen-rendering\n</code></pre> <p>To use ParaView with GPU, you need to use the following job script</p> Sample Batch Script to Run ParaView with GPU support: pvserver_gpu.submit.sh <pre><code>#!/bin/bash -l\n#SBATCH --job-name=pvserver_gpu\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=16\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load ParaView/5.11.0-egl\n################################################\n#\n# Open an ssh tunnel to the login node\n#\n################################################\nport=$(shuf -i 6000-9999 -n 1)\nHOST=$(hostname)\nif [ $(hostname) == $HOST ]; then\n     /usr/bin/ssh -N -f -R $port:localhost:$port login02\nfi\n################################################\ncat&lt;&lt;EOF\n\npvserver is running on: $(hostname)\nJob starts at: $(date)\n\nStep 1: Create SSH tunnel\n\nOpen a new terminal window on your local machine, and run:\n(If you are off campus you will need a VPN running)\n\nssh -L $port:localhost:$port $USER@login02.tartan.njit.edu\nEOF\n################################################\n#\n# Run MPI pvserver\n#\n################################################\n\nhost_list=$(srun hostname -s | sort | uniq | paste -s -d, -)\n\nmpiexec -np $SLURM_NTASKS -rmk slurm -hosts $host_list pvserver --server-port=$port --force-offscreen-rendering\n</code></pre> <p>Submit the job script using the sbatch command: <code>sbatch pvserver_gpu.submit.sh</code> or <code>sbatch pvserver_cpu.submit.sh</code>. Once you submit the job, please open the output file with <code>.out</code> extension, and get the port number from the output file. Once you open the output file (with <code>.out</code> extension) and go to the end of the file, you should see the following </p> <pre><code>Step 1: Create SSH tunnel\n\nOpen new terminal window, and run:\n(If you are off campus you will need VPN running)\n\nssh -L 1234:localhost:1234 user@login01.tartan.njit.edu\nWaiting for client...\nConnection URL: cs://n0003:1234\nAccepting connection(s): n0003:1234\n</code></pre> <p>Next, open a new terminal and type</p> <p><code>ssh -L $port:localhost:$port $USER@login01.tartan.njit.edu</code></p> <p>where <code>$port</code> corresponds to the port number. Once you open ParaView from your local machine go to <code>File --&gt; Connnect</code>, and you will see a dialogue box with the name <code>Choose Server Configuration</code>. You need to select Add Server option and there you need to use the following as shown below.</p>    Your browser does not support the video tag.  <p>Make sure to use the same port number in Port option. Once you add the server, you need to select Connect to connect ParaView to the cluster.</p> <p>Note</p> <p>The port number may change every time you submit the job. In that case, you need to modify the port number by selecting the Edit Server option. The step to modify the server is shown in the above tutorial.</p>"},{"location":"Software/visualization/paraview/#related-applications","title":"Related Applications","text":"<ul> <li>Tecplot</li> </ul>"},{"location":"Software/visualization/paraview/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"about/","title":"ARCS Leadership","text":"Name Title Phone Email Office Alex Pacheco Associate CIO - Advanced Research Computing (973) 596-2672 alex.pacheco@njit.edu GITC 5401 Gedaliah Wolosh Director, High Performance Research Computing (973) 596-5437 gwolosh@njit.edu GITC 5203 Vacant Director, Research and Academic Technologies Kate Cahill Associate Director, High Performance Research Computing (973) 596-2721 katharine.cahill@njit.edu GITC 2203 Kevin Walsh Assistant Director, Advanced Computing Infrastructure (973) 596-5747 walsh@njit.edu GITC 2202"},{"location":"about/#high-performance-research-computing","title":"High Performance Research Computing","text":"Name Title Phone Email Office Gedaliah Wolosh Director, High Performance Research Computing (973) 596-5437 gwolosh@njit.edu GITC 2203 Kevin Walsh Assistant Director, Advanced Computing Infrastructure (973) 596-5747 walsh@njit.edu GITC 2202 Kate Cahill Associate Director, High Performance Research Computing (973) 596-2721 katharine.cahill@njit.edu GITC 2203 Abhishek Mukherjee Research Computing Facilitator (973) 642-4132 abhishek.mukherjee@njit.edu GITC 2316B Hui(Julia) Zhao Research Computing Facilitator (973) 596-2727 hui.zhao@njit.edu GITC 2316B"},{"location":"about/#research-and-academic-technologies","title":"Research and Academic Technologies","text":"Name Title Phone Email Office Vacant Director, Research and Academic Technologies Rick Gaine Research Technology System Administrator (973) 596-5441 rgaine@njit.edu GITC 2404 Charles Yan Research Technology Support Specialist (973) 596-2907 cyan@njit.edu GITC 2301"},{"location":"about/#alex-pacheco","title":"Alex Pacheco","text":"<p>Alex Pacheco joined NJIT on August 2, 2022 as the Associate CIO of Advanced Research Computing. Alex has over twenty years combined experience as an HPC user, user support consultant and Manager of HPC resources. He helped develop a strategic vision for a shared HPC resource and served as a co-PI on an NSF Campus Cyberinfrastructure award at Lehigh University.  </p>"},{"location":"about/#gedaliah-wolosh","title":"Gedaliah Wolosh","text":"<p>Dr. Wolosh has been at NJIT for over twenty years working in research computing. He has been the lead architect for all of the HPC resources at NJIT. He specializes in building scientific software stacks.  </p> <p>Schedule an appointment with Gedaliah </p>"},{"location":"about/#kate-cahill","title":"Kate Cahill","text":"<p>Kate started at NJIT in September 2023. Previously, she was the Education &amp; Training Specialist at the Ohio Supercomputer Center for 8 years, where she led the training programs for OSC as well as external education programs related to HPC and computational science for the XSEDE project as well as other grant-funded efforts.</p>"},{"location":"about/#kevin-walsh","title":"Kevin Walsh","text":""},{"location":"about/#abhishek-mukherjee","title":"Abhishek Mukherjee","text":"<p>Abhishek Mukherjee is a computational scientist and has experience in multidisciplinary projects, providing engineering solutions for computational fluid dynamics problems. He has expertise in HPC software installation and is an active contributor to EasyBuild that allows to manage software on HPC systems in an efficient way.  </p> <p>You can schedule appointments with Abhishek to consult on problems or questions you are encountering related to your work using the high-performance computing and big data resources at NJIT. Please, before making appointments with Abhishek Mukherjee, send your query to hpc@njit.edu, so that an incident number will be created which is required to schedule an appointment.</p>"},{"location":"about/#huijulia-zhao","title":"Hui(Julia) Zhao","text":"<p>Hui(Julia) joined NJIT in December 2023, following twenty years in a similar role at Memorial Sloan Kettering Cancer Center. Julia's experience includes roles as a Bioinformatician, Unix System Administrator and High Performance Computing Engineer, with expertise in system and application configuration and user support. She helps user apply high-performance computing techniques to address diverse challenges in scientific computing.</p>"},{"location":"about/contact/","title":"Contact Us","text":"<p>Requests for assistance with HPC resources have been integrated into the campus-wide ServiceNow system. To have a service ticket created automatically and assigned to the HPC staff, email hpc@njit.edu. Before requesting assistance, we encourage you to take a look at the relevant documentation on this site. </p> <p>For prompt response, please consider the following when submitting a ticket. </p> <ul> <li> <p>Email Subject</p> <ul> <li>Very short summary of the issue as a subject, examples:<ul> <li>Lochness: Unable to submit jobs to public partition</li> <li>Lochness: Job error - no space on disk</li> <li>Lochness: Jobs are getting killed</li> </ul> </li> </ul> </li> <li> <p>Email Content</p> <ul> <li>JOB NUMBER</li> <li>System, Software and Partition where you are having an issue.</li> <li>Detailed description of your issue including modules loaded, and location of input, output and error files.</li> <li>If you have an error to report, paste the error message. Do not take a snapshot and upload the image.</li> <li>An image is not helpful when a long string of characters including directory path or file names needs to be copied.</li> <li>Anything else that you think may be necessary for us to understand and find a resolution to your issue.</li> </ul> </li> <li> <p>Software requests</p> <ul> <li>Submit a request for HPC Software Installation by visiting the Service Catalog </li> <li>Note: Users should consider installing software in their home directory, especially Python and R packages. See Building your own for instructions on using conda or mamba. If you encounter issues, consider asking for help above rather than requesting a software install.</li> <li>You will need to enter the following information<ul> <li>Name of software and URL to download.</li> <li>If software is behind a paywall or account, please download the software, copy it over to lochness and provide location from where we can copy it for installation.</li> </ul> </li> </ul> </li> <li> <p>Account requests</p> <ul> <li>Requests for an account must come from faculty advisors by sending an email to hpc@njit.edu including the UCIDs for the accounts to be created.</li> <li>Students making requests should add their faculty advisor to the request. Requests will only be approved after confirmation by faculty advisor.</li> <li>To use HPC in courses, the course instructor should submit a request and include UCID's for the accounts to be created.</li> <li>Guest Accounts: Faculty can use Service Now to request Guest Access on HPC resources only for their external collaborators for a period of 1 year (can be renewed annually). </li> </ul> </li> <li> <p>Storage Requests</p> <ul> <li>Requests for storage must come from faculty advisors by sending an email to hpc@njit.edu. The request should include the amount of storage needed.</li> </ul> </li> <li> <p>Consultations</p> <ul> <li>During the migration, HPC leadership will host office hours for faculty and PIs. These can be virtual or in person at GITC 2200. Please sign up here.</li> <li> <p>We also provide one-on-one consultations for HPC related issues. Schedule an appointment directly from below.</p> <p></p> </li> <li> <p>Please note that, when requesting assistance provide the information described above (where applicable), so we can most effectively assist you. Before requesting one-on-one consultations, please send you query to hpc@njit.edu, so that an incident number will be created which is required to schedule an appointment. </p> </li> </ul> </li> </ul>"},{"location":"clusters/","title":"Current cluster","text":"<ul> <li>Wulver is NJIT's newest High Performance Cluster made available to users in Jan 2024.</li> </ul>"},{"location":"clusters/#old-cluster","title":"Old cluster","text":"<ul> <li>Lochness</li> </ul>"},{"location":"clusters/Wulver_filesystems/","title":"Wulver Filesystems","text":"<p>The Wulver environment is quite a bit like Lochness, but there are some key differences, especially in filesystems and SLURM partitions and priorities.</p> <p>Wulver Filesystems are deployed with more attention to PI ownership / group efforts:</p> <ol> <li>The <code>$HOME</code> directory is not intended for primary storage and has a 50GB quota. The main location for storing files is the group project directory which has 2TB of storage per PI group. To run the simulations, compilations, etc., users need to use a project directory which has 2TB of storage per PI group. Students can store their files under their corresponding PI\u2019s UCID in the <code>/project</code> directory.  For example, if PI\u2019s UCID is <code>doctorx</code>, then students need to use the <code>/project/doctorx/</code> directory. </li> <li>Users can also store temporary files under the <code>/scratch</code> directory, likewise under a PI-group directory. For example, PI\u2019s UCID is <code>doctorx</code>, so students need to use the <code>/scratch/doctorx/</code> directory.  Please note that the files under <code>/scratch</code> will be periodically deleted. To store files for longer than computations, please use the <code>/project</code> directory.  Files under <code>/scratch</code> are not backed up. For best performance simulations should be performed in the <code>/scratch</code> directory. Once the simulation is complete, the results should be copied into the <code>$HOME</code> or <code>/project</code> directory.  Files are deleted from <code>/scratch</code> after they are 30 days old.</li> <li>The <code>/research</code> and <code>$HOME</code> directory from Lochness will be mounted on Wulver as <code>/research</code> and <code>/oldhome</code> respectively. For details, see Migration.</li> </ol> Filesystem Purpose Characterics Backup policy Total Size Default Quota Deletion Policy Cost per TB <code>/home</code> Non-research user files, such as profile, history, etc., files not intended for sharing.  Not for actual research files. Kalray Pixstor / GPFS (expensive) Daily ~1 PB 50GB per user (locked) One year after owner leaves NJIT Not possible to increase size, use <code>/project</code> or <code>/research</code> instead <code>/project</code> Active research by groups. Deployed as <code>/project/$PI_UCID/$LOGIN/</code> Kalray Pixstor / GPFS (expensive) Daily ~1 PB 2TB per group TBD $200 per TB for a duration of five years, minimum storage allocation of 5TB is required <code>/scratch</code> Temporary space for intermediate results, downloads, checkpoints, and such. MOVE YOUR RESULTS &amp; IMPORTANT FILES TO <code>/project</code> or <code>/research</code> Nvme  (very expensive) NEVER ~ 150 TB 10TB per group Files deleted after 30 days or sooner if 80% full No charge, but files are automatically deleted <code>/tmp</code> Very high speed temporary storage Node-local SSD or NVME (very expensive) NEVER 1 TB per node shared by all users NONE Files deleted after job completion No charge, but files are automatically deleted <code>/research</code> Long term archive. Users can buy as much as they need. The users need to buy this space.  Existing purchases/quotas will be kept over from Lochness. NFS (inexpensive) Daily 8 TB Whatever the PI purchases. TBD $100 per TB per five years <code>/oldhome</code> Lochness <code>/home</code> is mounted as read-only on Wulver login nodes NFS CSO 8 TB NA All to be deleted one year after last login is migrated from Lochness to Wulver. NA"},{"location":"clusters/cluster_access/","title":"Access to NJIT Clusters","text":"<p>The following instructions are provided to access NJIT HPC clusters from a local computer.</p>"},{"location":"clusters/cluster_access/#getting-a-login","title":"Getting a Login","text":"<p>Faculty can obtain a login to NJIT's HPC by sending an email to hpc@njit.edu. Students can obtain a login either by taking a class that uses one of the systems or by asking their faculty adviser to contact on their behalf. Your login and password are the same as for any NJIT AFS system.</p>"},{"location":"clusters/cluster_access/#access-to-clusters","title":"Access to Clusters","text":"<p>Make sure the user is connected to <code>NJITsecure</code> if the user is on campus. If working off campus, NJIT VPN is required. Please find the details here. Here we will provide instructions for connecting to NJIT HPC on Mac/Linux and Windows OS.</p> <p>Update</p> <p>In the recent update (Sep 10<sup>th</sup> 2024) Cisco two-factor authentication (TFA) is deployed, similar to what is already deployed on NJIT websites and VPN. </p> Mac/LinuxWindows <p>Open terminal from Launchpad and select terminal. Type the following in the terminal and replace <code>$UCID</code> with your NJIT UCID.. <pre><code>  localhost&gt; ssh -X -Y $UCID@wulver.njit.edu  \n</code></pre></p> <p>Users will be prompted for your password. Enter your NJIT UCID password. Users can omit the <code>-X -Y</code> if you are not using a graphic interface. Once the password is provided, User will authenticate via Cisco two-factor authentication (TFA)</p> <pre><code>(guest@wulver.njit.edu) Duo two-factor login for guest\n\nEnter a passcode or select one of the following options:\n\n1. Duo Push to XXX-XXX-2332\n2. Phone call to XXX-XXX-2332\n3. SMS passcodes to XXX-XXX-2332 (next code starts with: 1)\n\nPasscode or option (1-3):\n</code></pre> <p>Based on the option provided, user will be logged in after succesfull authetication.  </p> <p>Download the MobaXterm from this link.  Open MobaXterm after installation is completed.  Select Start local terminal to open the terminal.</p> <p></p> <p>Type <code>ssh $UCID@wulver.njit.edu</code>. Replace <code>$UCID</code> with your NJIT UCID.</p> <p>. </p> <p>User will be prompted to type the password. The password is NJIT UCID password. After successful login, user will see the following in the terminal, which means the user is now connected to HPC.</p> <pre><code>Last login: Tue Oct 25 12:37:31 2022 from 10.192.228.138\nStarting /home/a/user/.bash_profile ... standard AFS bash profile\n\n========================\nHome directory : /home/u/user is not in AFS -- skipping quota check\n========================\n\nOn host login-1 :\n         12:38:05 up 315 days, 22:36, 35 users,  load average: 10.32, 10.10, 10.63\n\n=== === === Your Kerberos ticket and AFS token status === === ===\nKerberos :  Renew until 10/26/22 12:38:01, Flags: FRIA Renew until 10/26/22 12:38:01, Flags: FRA\nAFS      :  User's (AFS ID 105631) tokens for afs@cad.njit.edu [Expires Oct 25 22:38]\n\nRunning your /home/u/user/.modules file:\n\nTo see your aliases, enter \"alias\"\n\nlogin-1-41 ~ &gt;:\n</code></pre>"},{"location":"clusters/cluster_access/#transfer-the-data-from-the-local-machine-to-clusters-or-vice-versa","title":"Transfer the Data from the Local Machine to Clusters or vice versa","text":"Mac/LinuxWindows <p>Users need to use the command in the terminal to transfer in and out the data.  </p> <p>User needs to select the <code>follow terminal folder</code> of the left pane of the MobaXterm terminal. </p> <p></p> <p>Next, to transfer the data from the local machine to Wulver user needs to select the <code>Upload to current folder</code> option, as shown below. Selecting this option will open a dialogue box where user needs to select the files to upload.</p> <p></p> <p>For transferring the data from Wulver to the local machine user needs to select the directory or the data from the left pane and then select <code>Download selected files</code>.</p>"},{"location":"clusters/cluster_access/#rsync","title":"<code>rsync</code>:","text":"<ul> <li> <p>Transfer the data from local machine to HPC cluster <pre><code>rsync -avzP /path/to/local/machine $UCID@wulver.njit.edu:/path/to/destination\n</code></pre></p> </li> <li> <p>To transfer the data from HPC cluster to local machine use <pre><code>rsync -avzP $UCID@wulver.njit.edu:/path/to/source /path/to/local/machine\n</code></pre></p> </li> </ul>"},{"location":"clusters/cluster_access/#scp","title":"<code>scp</code>:","text":"<ul> <li> <p>Copy files from remote machine to local machine <pre><code>scp [option] [$UCID@wulver.njit.edu:path/to/source/file] [target/path]\n</code></pre></p> </li> <li> <p>Copy files from local machine to remote machine <pre><code>scp [option] [path/to/source/file] [$UCID@wulver.njit.edu:target/path] \n</code></pre></p> </li> <li> <p>Example of scp: <pre><code>scp -r example $UCID@wulver.njit.edu:/home/dir \n</code></pre> Copy the \u201cexample\u201d folder recursively to <code>/home/dir</code></p> </li> </ul>"},{"location":"clusters/get_started_on_Wulver/","title":"Getting Started on Wulver","text":"<p>Wulver is a high performance computing (HPC) cluster \u2013 a collection of computers and data storage connected with high-speed low-latency networks. We refer to individual computers in this network as nodes. Wulver is only accessible to researchers remotely; your gateways to the cluster are the login nodes. From these nodes, you can view and edit files and dispatch jobs to computers configured for computation, called compute nodes. The tool we use to manage these jobs is called a job scheduler. All compute nodes on a cluster mount several shared filesystems; a file server or set of servers store files on a large array of disks. This allows your jobs to access and edit your data from any compute node. </p> <p></p>"},{"location":"clusters/get_started_on_Wulver/#being-a-good-hpc-citizen","title":"Being a Good HPC Citizen","text":"<p>While using HPC resources, here are some important things to remember:</p> <ul> <li>Do not run jobs or computation on a login node, instead submit jobs to compute nodes.  You should be using <code>sbatch</code>, <code>srun</code>, or OnDemand to run your jobs.  </li> <li>Never give your password to anyone else.</li> <li>Do not run larger numbers of very short (less than a minute) jobs Use of the clusters is also governed by our official guidelines. Violating the guidelines might result in having your access to Wulver revoked, but more often the result is your jobs will run painfully slower.</li> </ul>"},{"location":"clusters/get_started_on_Wulver/#remote-access","title":"Remote Access","text":"<p>All users access the Wulver cluster remotely, either through ssh or a browser using the Open OnDemand portal. See these detailed login instructions. NB: If you want to access the clusters from outside NJIT\u2019s network, you must use the VPN.</p>"},{"location":"clusters/get_started_on_Wulver/#schedule-a-job","title":"Schedule a Job","text":"<p>On our cluster, you control your jobs using a job scheduling system called Slurm that allocates and manages compute resources for you. You can submit your jobs in one of two ways. For testing and small jobs, you may want to run a job interactively. This way you can directly interact with the compute node(s) in real time. The other way, which is the preferred way for multiple jobs or long-running jobs, involves writing your job commands in a script and submitting that to the job scheduler. Please see our Slurm documentation or review our training materials for more details.  </p>"},{"location":"clusters/get_started_on_Wulver/#use-software","title":"Use Software","text":"<p>To best serve the diverse needs of all our researchers, we use software modules to make multiple versions of popular software available. Modules allow you to swap between different applications and versions of those applications. The software can be loaded via <code>module load</code> command. You see the following modules are loaded once you log in to the Wulver. Use the <code>module li</code> command to see the modules.  <pre><code>   1) easybuild   2) slurm/wulver   3) null\n</code></pre> If you cannot find certain software or libraries on the Wulver cluster, please submit a request for HPC Software Installation by visiting the Service Catalog. The list of installed software or packages on the Wulver HPC cluster can be found in the Software List.</p>"},{"location":"clusters/get_started_on_Wulver/#shared-filesystems","title":"Shared Filesystems","text":"<p>A critical component of Wulver is its shared filesystem, which facilitates the efficient storage, retrieval, and sharing of data among the various compute nodes. It enables multiple users and applications to read from and write to a common storage pool, ensuring data consistency and accessibility across the entire system. See Wulver Filesystems for a different type of shared filesystems.</p>"},{"location":"clusters/get_started_on_Wulver/#transfer-your-files","title":"Transfer Your Files","text":"<p>As part of setting up and running jobs and collecting results, you will want to copy files between your computer and the clusters. We have a few options, and the best for each situation usually depends on the size and number of files you would like to transfer. For most situations, uploading a small number of smaller files through Open OnDemand's upload interface is the best option. This can be done directly through the file viewer interface by clicking the Upload button and dragging and dropping your files into the upload window. Check the Ondemand file transfer for more details. For more information on other upload methods, see our transferring data instructions. </p>"},{"location":"clusters/get_started_on_Wulver/#workshop-and-training-videos","title":"Workshop and Training Videos","text":"<p>Each semester, we host webinars and training sessions. Please check the list of events and register from HPC Events. You can also check the recordings of previously hosted webinars at HPC Training.</p>"},{"location":"clusters/get_started_on_Wulver/#linux","title":"Linux","text":"<p>Our cluster runs Red Hat Enterprise Linux 8, utilizing the bash (or zsh set via https://myucid.njit.edu) command line interface (CLI).  A basic familiarity with Linux commands is required for interacting with the clusters. We have a list of commonly used commands here. We periodically run an Intro to Linux Training to get you started, see our Events for upcoming training. There are also many excellent beginner tutorials available for free online, including the following:</p> <ul> <li>Unix Tutorial for Beginners</li> <li>Cornell Virtual Workshop: An Introduction to Linux</li> </ul> <p>In the table below, you can find the basic linux commands required to use the cluster. For more details on linux commands, please see the Linux commands cheat sheets.</p> Linux Commands Description <code>cd [directory]</code> Change directory <code>cd ..</code> Change to one directory up <code>mkdir [directory]</code> create a directory <code>mkdir -p [directory]</code> create directories as necessary, if the directories exist, no error is specified <code>pwd</code> Print current working directory <code>ls</code> lists directory contents of files and directories <code>ls -ltra</code> all the files in long format with the newest one at the bottom <code>cp /path/to/source/file1 /path/to/destination/file2</code> Copy files from source to destination <code>cp \u2013r /path/to/source/dir1 /path/to/destination/dir2</code> Copy directory from source to destination <code>mv /source/file1 /source/file2</code> move directories or files  and rename them <code>cat filename1</code> display the content of text files <code>rm -rf /path/to/dir</code> used to delete files and directories"},{"location":"clusters/get_started_on_Wulver/#get-help","title":"Get Help","text":"<p>If you have additional questions, please email us at hpc@njit.edu. If you are having a problem with <code>sbatch</code> or <code>srun</code>, please include the following information:</p> <ul> <li>Job ID#(s)</li> <li>Error messages</li> <li>Command used to submit the job(s)</li> <li>Path(s) to scripts called by the submission command</li> <li>Path(s) to output files from your jobs</li> </ul> <p>Here are some tips to get help faster:</p> <ul> <li>The fastest way to get help is by sending an email to HPC@NJIT.EDU, this is routed right to us at ARCS HPC and will be answered by the most knowledgeable team member based on your question.</li> <li>Never reply to an earlier HPC email incident as the ticketing system does not make a new ticket when you respond to an already closed ticket.</li> </ul>"},{"location":"clusters/wulver/","title":"Wulver","text":"<p>Wulver is NJIT's newest cluster, which went into production in early 2024. To get started with Wulver, please check the details in get started on Wulver</p>"},{"location":"clusters/wulver/#specifications","title":"Specifications:","text":"General Bigmem GPU Partition <code>general</code> <code>bigmem</code> <code>gpu</code> Nodes 100 2 25 CPU Type AMD EPYC 7753 AMD EPYC 7753 AMD EPYC 7713 CPU Speed (GHZ) 2.45 2.45 2.0 CPUs/Socket 64 64 64 Sockets/Node 2 2 2 RAM/Node (GB) 512 2048 512 GPU Type - - nVIDIA A100 GPUs/Node - - 4 GPU Memory (GB) - - 80 Total CPUs 12800 256 3200 Total RAM (TB) 50 4 12.5 Total SUs/year 112128000 2242560 28032000 Peak Performance (TFLOPs) 501.76 10.035 1072.4 HPL (TFLOPs/Node) - - - <ul> <li>100 GB High throughput/low latency Infiniband network</li> <li>Arcastream Storage<ul> <li>Parallel filesystem allowing multiple read/write operations</li> <li>Max performance 20 GB/s</li> <li>Hierarchical storage; NVME scratch, SAS, Cloud</li> <li>User transparent transfer from high-speed to medium to archive storage</li> <li>1 PB total storage</li> </ul> </li> <li>Virtualized Support and login nodes</li> <li>Management and deployment overseen by NJIT:<ul> <li>Nvidia Bright Cluster Manager</li> <li>X-ISS system administration</li> <li>SLURM scheduler with full support</li> <li>InfiniBand support via Dell or directly from Mellanox</li> <li>Will be deployed in the Databank data center in Piscataway</li> </ul> </li> </ul>"},{"location":"clusters/decommissioned/lochness/","title":"Lochness","text":"<p>This very heterogeneous cluster is a mix of manufacturers, components, and capacities as it was built up in incremental purchases spanning several years. </p> <p>Lochness was decommissioned on March 2024. Much of lochness nodes will be incorporated into Wulver cluster 2Q 2024.</p>"},{"location":"clusters/decommissioned/lochness/#specifications","title":"Specifications:","text":"Current Partitions Number of Nodes Processor Family Cores Per Node Total Cores Number of GPU per node Total GPUs Model of GPU bader 1 Sky Lake 20.0 20 4.0 4 Titan V cld 11 Sandy Bridge 20.0 220 0.0 9 cld-gpu 2 Sandy Bridge 20.0 40 2.0 4 K20Xm datasci 7 Sandy Bridge 20.0 140 2.0 14 P100 datasci 1 Sky Lake 20.0 20 4.0 4 Titan RTX datasci3 1 Sky Lake 20.0 20 4.0 4 Titan RTX datasci4 1 Sky Lake 20.0 20 4.0 4 Titan RTX davidsw 1 Cascade Lake 32.0 32 2.0 2 A100 ddlab 1 Sandy Bridge 20.0 20 0.0 0 ddlab 1 Cascade Lake 20.0 20 2.0 2 V100 esratoy 2 Cascade Lake 32.0 64 0.0 0 fahmadpo 14 Cascade Lake 64.0 896 0.0 0 fahmadpo-gpu 1 Cascade Lake 24.0 24 4.0 4 Titan RTX gor 28 Broadwell 20.0 560 0.0 0 gperry 3 Cascade Lake 36.0 108 0.0 0 hrjin 1 Sky Lake 32.0 32 4.0 3 Titan XP jyoung 3 Cascade Lake 96.0 288 0.0 0 phan 1 Cascade Lake 20.0 20 4.0 4 1 Titan X,3 Ge Force project 1 Cascade Lake 32.0 32 0.0 0 public 37 Cascade Lake 32.0 1184 0.0 0 samaneh 16 Cascade Lake 40.0 640 0.0 0 shakib 20 Cascade Lake 24.0 480 0.0 0 singhp 1 Ice Lake 56.0 56 0.0 0 smarras 4 Cascade Lake 48.0 192 0.0 0 smarras 2 Cascade Lake 48.0 96 2.0 2 A100 solarlab 1 Cascade Lake 48.0 48 2.0 2 A100 solarlab 4 Cascade Lake 48.0 192 0.0 0 solarlab 1 Ice Lake 48.0 48 1.0 1 A100 solarlab 2 Ice Lake 48.0 96 0.0 0 solarlab 1 Ice Lake 48.0 48 0.0 0 xt3 11 Cascade Lake 32.0 352 0.0 0 xye 1 Cascade Lake 16.0 16 5.0 5 3 types of GPUs zhiwei 1 Ice Lake 56.0 56 4.0 4 A40 183 6080 72 <ul> <li>All nodes have:<ul> <li>Ethernet network interface (1GigE or 10GigE)</li> <li>Infiniband network interface (mix of HDR100, EDR, and FDR speeds)</li> <li>1TB local storage (mostly SSD but a few HD)</li> </ul> </li> <li>All nodes have network accessible storage:<ul> <li><code>/home/</code>: 26 TB</li> <li><code>/research/</code>: 97 TB</li> <li><code>/afs/cad/</code>: 50 TB </li> </ul> </li> </ul> <p>The cluster also features</p> <ul> <li>\"CentOS Linux 7 (Core)\" operating system</li> <li>Virtualized login and control nodes</li> <li>SLURM job scheduler</li> <li>Warewulf stateless node provisioning</li> <li>Managed entirely by NJIT personnel</li> </ul> <p>Roughly half under warranty; rest are time and materials</p>"},{"location":"clusters/decommissioned/stheno/","title":"Stheno","text":"<p>Stheno was decommissioned on October 6<sup>th</sup>, 2023. Stheno users can access their data from Lochness when Lochness is returned to service.</p>"},{"location":"clusters/decommissioned/stheno/#specifications","title":"Specifications","text":"<ul> <li>GPU nodes: 2</li> <li>Total GPUs: 4</li> <li>GPU models:</li> <li>2x  node(s) w/ 2x K20m GPU(s)</li> <li>Total Nodes: 22</li> <li>Total Cores: 1224</li> <li>Total GB RAM: 2481667</li> <li>Nodes:  GB:<ul> <li>15x     128</li> <li>7x      96</li> </ul> </li> <li>Processor models:<ul> <li>15x Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz</li> <li>7x Intel(R) Xeon(R) CPU E5649  @ 2.53GHz</li> </ul> </li> </ul>"},{"location":"facilities/facilities/","title":"Facilities","text":"<p>As New Jersey\u2019s Science and Technology University, New Jersey Institute of Technology (NJIT) has developed a local cyberinfrastructure well positioned to allow NJIT faculty and students to collaborate at local, national, and global levels on many issues at the forefront of science and engineering research.</p>"},{"location":"facilities/facilities/#cyberinfrastructure","title":"Cyberinfrastructure","text":"<p>NJIT\u2019s Information Services and Technology (IST) resources provide members of the university community with universal access to a wealth of resources and services available over the NJIT network. NJIT's multi-100 gigabit backbone and multi-gigabit user network provides access to classrooms, laboratories, residence halls, faculty and staff offices, the library, student organization offices and others. 50% of these locations are provided with speeds of 5Gb/s or more. The campus wireless network 2,900+ access points blanket the university\u2019s public areas, classrooms, offices, collaboration spaces and outdoor areas. Over 60% of the Wi-Fi network is Wi-fi 6 capable, enabling NJIT\u2019s mobile users connectivity with multiple devices at increasing speeds. NJIT\u2019s connectivity to NJEdge, NJ\u2019s state-wide higher education network, provides access to the Internet and Internet 2. Students have the opportunity to work closely with faculty and researchers as new families of advanced applications are developed for an increasingly networked and information-based society. NJIT is also directly connected to cloud service providers such as AWS (Amazon Web Services) to provide low latency high speed access to cloud resources. A redundant diverse 100Gb network connection is being provisioned to support NJIT\u2019s new HPC co-location facility in Piscataway NJ.</p>"},{"location":"facilities/facilities/#compute-resources","title":"Compute Resources","text":"<p>NJIT\u2019s Advanced Research Computing Services (ARCS) group presently maintains a 127-node heterogeneous condominium cluster at the new HPC co-location facility, Databank in Piscataway, NJ, which went into production for general use in early 2024. The previous 224-node heterogeneous condominium cluster, Lochness, was decommissioned; however, a few nodes from Lochness will soon be integrated into the Wulver.</p> Category Lochness Wulver Total Nodes 224 127 Total Cores 5600 16256 Total RAM / GB 65291 68096 Total GPUs 64 100 Total GPU RAM / GB 888 8000 Access General General Network (Infiniband; Ethernet) mix of HDR100, EDR, and FDR HDR100 and 10Gig Theoretical Peak Performance (TFLOPs) 514.79 1584.19"},{"location":"facilities/facilities/#storage-resources","title":"Storage Resources","text":"<p>Storage on Wulver will be provided by a 1PB arcastream high-performance storage that combines flash, disk, tape, and cloud storage into a unified single namespace architecture. Data moves seamlessly through various tiers of storage - from fast flash to cost-effective, high capacity object storage, all the way out to the cloud.</p> <p>The ARCS team provides 50TB of research and academic storage via the Andrew File System (AFS) distributed file system. AFS is backed up daily via IST\u2019s enterprise backup system. The current AFS implementation, OpenAFS, which is open source, will be replaced with a commercial implementation, AuriStor, during the 2022-2023 academic year, providing important enhancements in performance, security,capacities, authorization, permissions, and administration as well as bug fixes and technical support.</p>"},{"location":"facilities/facilities/#afs-storage","title":"AFS Storage","text":"<p>AFS is a distributed file system. Its principal components are :</p> <ul> <li>Database servers : provide information on authorization, and directory and file locations on file servers </li> <li>File servers : store all data in discrete \"volumes\", with associated quotas </li> <li>Client software : connects AFS clients to database servers and file servers over a network. Every client has access to every file in AFS, subject to the permissions attached to the identity of the user logged into the client. Client software is available for Linux, MacOS, and Windows. Single global name space for all clients. See the identical path names and permissions.</li> </ul> <p>An AFS \"cell\" is a group of database servers and file servers sharing the same cell name.</p> <p>AFS was designed for highly-distributed wide area network (WAN) use. A cell can be concentrated in one physical location, or be widely geographically dispersed. A client in one cell can be given fine-grained levels of access to the data in other cells.</p> <p>The NJIT cell name is <code>cad.njit.edu</code>; all file and directory paths begin with <code>/afs/cad.njit.edu/</code>(abbreviated to <code>/afs/cad/</code>). This cell currently contains about 27TB of research data, 4TB of user data,and 1.4TB of applications data, in about 47,700 volumes.</p> <p>The current AFS implementation, OpenAFS, which is open source, will be replaced with a commercial implementation during the 2022-2023 academic year, providing important enhancements in performance, security,capacities, authorization, permissions, and administration as well as bug fixes and technical support.</p> <p>All of <code>/afs/cad/</code> is backed up daily via IST enterprise backup.</p>"},{"location":"facilities/facilities/#cloud-computing","title":"Cloud Computing","text":"<p>Access to Cloud Computing is provided via Rescale, a cloud computing platform combining scientific software with high performance computing. Rescale takes advantage of commercial cloud computing vendors such as AWS, Azure and Google Cloud to provide compute cycles as well as storage. The Rescale services also include applications setup, and billing and provides a pay-as-you-go method for researchers to use commercial cloud services - e.g., Amazon Web Services, Azure, Google Cloud Platform.</p>"},{"location":"facilities/facilities/#data-center-space-power-and-cooling","title":"Data Center: Space, Power and Cooling","text":"<p>NJIT\u2019s recent purchase of the Wulver cluster exceeds the capacity of NJIT\u2019s current computing facilities in terms of power and cooling. To accommodate Wulver and future expansion, NJIT has partnered with Databank, a leader in collocation facilities. Databank has more than 65 datacenters in over 27 metropolitan areas, supporting many industries including very large HPC deployments. The Databank location in Piscataway NJ will provide NJIT with 100% uptime SLA due to redundant power, cooling, and network facilities. The facility also provides water-cooling instead of traditional air-conditioned cooling in order to support far denser equipment needed for modern HPC. </p>"},{"location":"facilities/facilities/#services","title":"Services","text":"<p>NJIT employs nine FTE staff members to administer and support research computing resources. Services available to the user community include system design, installation, and administration of research computing resources, application support, assistance with software purchasing and consulting services to faculty members, their research associates, and students.</p>"},{"location":"faq/faq/","title":"FAQs","text":"<p>Welcome to our most frequently asked questions. If you cannot find an entry related to your question, please contact us, and we will add it.</p>"},{"location":"faq/faq/#login-issues-access","title":"Login Issues / Access","text":"How do I get access to the Wulver HPC cluster? <ul> <li>If you are a student or researcher, your research/faculty advisor will need to request an account on your behalf. </li> <li>If you are a faculty member, then you can directly email us at hpc@njit.edu for an account.</li> <li>For individuals non affiliated with NJIT have to contact a faculty member in NJIT to sponsor you a guest account</li> <li>If you are taking a course that requires computation on Wulver please ask your course instructor to email hpc@njit.edu to request access for the students.</li> <li>For detail information please click here.</li> </ul> How do I connect to the Wulver HPC cluster? <ul> <li>For detail information please click here.</li> </ul> Why can\u2019t I log in to the HPC? <ul> <li>It\u2019s possible that your account is not activated on Wulver. Make sure to follow the instructions in How do I get access to the Wulver HPC cluster?. Once your account is created, you will receive a confirmation in your NJIT email.</li> <li>Ensure that your UCID and password are correct.</li> <li>If you are working off-campus, make sure to connect to NJIT VPN before logging into Wulver. Visit https://ist.njit.edu/vpn for more information on VPN installation.</li> </ul> Why is my password not showing in the terminal when I type? <ul> <li>The Command Line Interface hides passwords as you type to protect against visual exposure and enhance security.</li> </ul> Why am I getting \u201cpermission denied\u201d ? <ul> <li>You might not have access to the HPC yet. For requesting access, please check our user access.</li> <li>Your instructor/PI might not have added you into the group yet.</li> <li>Cluster might be having downtime due to maintenance or other reasons.</li> </ul> How do I transfer data to and from the HPC cluster? <ul> <li>Detailed instructions are given  here.</li> </ul> What security measures should I be aware of when using the HPC cluster? <ul> <li>Do not share your login information with anyone else or allow anyone to login with your account.</li> </ul> Which directory will I land on when I login? <ul> <li>You will enter into your home directory named under your UCID.</li> <li>Please note that you are on the login node of Wulver. Do not run any computations on the login nodes, as CPU and memory usage are limited per user on these nodes. To perform computations, you need to request resources from the compute nodes via SLURM.</li> </ul>"},{"location":"faq/faq/#file-systems-storage","title":"File Systems / Storage","text":"What are the different file storage systems available on Wulver? <ul> <li>Please visit our file system page for detailed information. </li> </ul> How can I get more storage? <ul> <li>When your account is active on Wulver, you will have access to following file systems<ul> <li>$HOME directory - Default quota of 50GB per user and cannot be increased</li> <li>/project - Default quota of 2TB per PI group and can be increased by purchasing additional storage at $200/TB for 5 years.</li> <li>/scratch - No quota but files will be deleted after 30 days or sooner if the directory reaches 80% capacity. Users will be notified prior to deletion so they can review and move important files to <code>/project</code> if necessary.</li> </ul> </li> <li>For more details, visit file system.</li> </ul> I have an error \u201cdisk quota exceeded\u201d while running a job, how can I solve this error? <ul> <li>First, check which filesystem is causing the error. If it\u2019s in <code>$HOME</code>, it means the 50GB quota has been exceeded. You can either delete files, compress them, or move them to <code>/project</code>. If the error is in <code>/project</code>, you need to compress or delete some files. Alternatively, you can ask your PI to purchase an increase  in the <code>/project</code> quota by emailing us at hpc@njit.edu. You can also run your code in <code>/scratch</code> and, after the simulation, transfer the important files to <code>/project</code>. </li> <li>Use <code>quota_info</code> command to check the filesystem quota. </li> <li>If you are encountering this error while running Python or Jupyter Notebook via Conda, the issue is likely due to Conda packages or cache files stored in <code>$HOME</code>. Use the <code>sn p $HOME</code> command to view a detailed breakdown of each directory in $HOME, showing how much space it is consuming. If you notice that the <code>.cache</code> directory is consuming a significant amount of space, you should remove it. If the <code>.conda</code> directory is taking up too much space, you need to move the Conda environment to <code>/project</code>. Refer to this guide for detailed steps. </li> </ul> What is <code>/scratch</code> used for? <ul> <li>Run your code in <code>/scratch</code> and, after the simulation, transfer the important files to <code>/project</code>. </li> <li>NB : This space is not backed up; files will be deleted after 30 days.</li> </ul>"},{"location":"faq/faq/#jobs-and-scheduling","title":"Jobs and scheduling","text":"How do I submit and manage jobs on the Wulver HPC cluster? <ul> <li>We use the SLURM resource manager and scheduler on Wulver for submitting and managing jobs on the compute nodes. </li> <li>Check our SLURM page on the website for more detailed guidance.</li> </ul> What is Walltime? <ul> <li>Walltime refers to the maximum amount of real-world time a job is allowed to run on the cluster. The actual job may finish earlier. </li> <li>To set the walltime, you'll typically specify it in the job submission script:  <pre><code>#SBATCH --time=01:00:00      # Request 1 hour of walltime\n</code></pre></li> </ul> How can I monitor the status of my jobs? <ul> <li>For checking and monitoring the status of your job please use this guide for detailed information.</li> </ul> Where does my output will appear after I submit a job? <ul> <li>Your output will appear in the file which you initialized in your job script which look like below: <pre><code>#SBATCH --output=file_name.%j.out # %j expands to slurm JobID\n</code></pre></li> <li>By default, this file is in the same directory as the job script, unless you specify the working directory via <code>sbatch</code></li> </ul> How can I see the status of the cluster? <ul> <li>For checking the cluster load use this link</li> </ul>"},{"location":"faq/faq/#maintenance","title":"Maintenance","text":"When does maintenance occur? <ul> <li>Second Tuesday 9AM - 9PM monthly.</li> </ul> I submitted my job before maintenance, but why did it go to pending status as \"ReqNodeNotAvail, Reserved for maintenance\" ? <ul> <li>If your job walltime request indicates the job will not finish before the maintenance starts then the scheduler will hold the job.  </li> <li>If you resubmit the job with a shorter walltime, it will not be held. </li> <li>For example: If you submit a job on Monday with a walltime of 2 days and maintenance is scheduled on Tuesday then your job overlaps with the maintenance schedule. Hence, SLURM will immediately put it on hold until the maintenance is completed and start it later.</li> </ul> How will maintenance affect me? <ul> <li>During the maintenance downtime, logins will be disabled, users will not have access to their stored data in <code>/project</code>, <code>/home</code> and <code>/scratch</code>.</li> <li>All jobs that do not end before 9AM will be held by the scheduler until the downtime is complete and the systems are returned to service.</li> </ul> Will I receive a notification before maintenance? <ul> <li>Our regular monthly maintenance cycle is every 2<sup>nd</sup> Tuesday. If there is a change to this cycle, all users will receive notification.</li> </ul> What happens to my jobs during maintenance? <ul> <li>Jobs queued just before the maintenance will be held in Pending State and then later gets continued.</li> </ul>"},{"location":"faq/faq/#software-and-hardware-specifications","title":"Software and Hardware Specifications","text":"Is there any installed software? <ul> <li>We have a variety of software packages already installed on Wulver.</li> <li>For more information, please visit our Software guide.</li> </ul> I want to install new software? <ul> <li>Please first check our already installed software list, and if you still don\u2019t find it then visit our guide for detailed guidance on software installation.</li> </ul> What are the hardware specifications of the Wulver HPC cluster? <ul> <li>Please check out our Wulver page for complete details on hardware specifications.</li> </ul> What programming languages are commonly used on the HPC cluster? <ul> <li>Most common ones are C, C++, Fortran, Python, R</li> </ul> What are the tools and compilers available on HPC cluster? <ul> <li>Common programming tools include Intel and GNU compilers as well as MPI for multi-node jobs.</li> <li>For GPU acceleration we have CUDA and OpenACC.</li> <li>Details on these resources are available here.</li> </ul> How can I optimize my code for parallel processing? <ul> <li>To optimize your code, you can use different parallelization techniques depending on your setup:<ul> <li>MPI and OpenMP for parallelizing code and then specifying the cores.</li> <li>CUDA and OpenACC for GPU acceleration, especially for compute-heavy tasks.</li> </ul> </li> <li>Focus on optimizing CPU, I/O, memory, and parallelism.</li> <li>If you have specific questions about this, please email HPC Help at hpc@njit.edu to request support.</li> </ul> Can I request additional resources or customization for my projects? <ul> <li>Please see Wulver Usage and Condo Policy for resource allocation details. </li> <li>The Research Computing Advisory Board is working on establishing policies and procedures for requesting additional computing time beyond the standard 300K SU/year.</li> </ul> My software requires a license, how should I proceed? <ul> <li>License is purchased by the user and his/her department.</li> <li>Please visit the software\u2019s documentation for your specific software.</li> </ul>"},{"location":"faq/faq/#miscellaneous","title":"Miscellaneous","text":"What documentation and support resources are available for Wulver users? <ul> <li>Please visit the Education and Training tab on our website.</li> <li>If you have any specific questions which are not covered, please contact us at hpc@njit.edu</li> </ul>"},{"location":"migration/","title":"Important Announcement: HPC Cluster Migration from Lochness to Wulver","text":"<p>We are excited to share important news about the upcoming migration of users from the old HPC cluster, Lochness, to the new and improved cluster, Wulver. This migration is scheduled to commence on 1/16/2024.</p>"},{"location":"migration/#migration-details","title":"Migration Details:","text":"<ul> <li>Start Date: January 16, 2024</li> <li>Priority: Migration will be carried out in PI groups, with PIs who own nodes in Lochness being migrated first.</li> <li>Communication: PIs will receive prior communication from our team to discuss specific details tailored to their groups.</li> <li>Lochness Complete Shutdown: At the conclusion of the migration, Lochness will undergo a complete shutdown to facilitate its merger with Wulver. You will be advised of any necessary preparations needed on your end for a seamless transition.</li> <li>Restrictions on Lochness: As of the migration start date, no new user accounts or software installations will be permitted on Lochness. We appreciate your cooperation in adhering to these restrictions to ensure a smooth migration process.</li> <li>HPC Usage in Spring Courses: For coursework during the Spring semester, HPC usage will be exclusively on Wulver. Faculty members planning to integrate Wulver into their Spring courses are encouraged to contact us for testing and any necessary support. This proactive approach will help ensure a successful experience for both faculty and students.</li> </ul>"},{"location":"migration/#action-required","title":"Action Required:","text":"<p>PIs: Expect communication from our team before the migration date, providing essential details and discussing the migration plan tailored to your group's needs. We will need a list of your current students and software in use by your group.</p>"},{"location":"migration/#important-note-regarding-documentation","title":"Important Note Regarding Documentation:","text":"<p>To facilitate a smooth transition, please refer to FAQs, and other resources specific to the migration process.</p>"},{"location":"migration/#benefits-of-migration","title":"Benefits of Migration:","text":"<p>Wulver is equipped with enhanced capabilities and improved performance, ensuring a more efficient and streamlined high-performance computing experience for all users.</p> <p>If you have immediate concerns or questions, please feel free to reach out to the group via Contact US.</p>"},{"location":"migration/Lochness_node_owners/","title":"Lochness Node Owners Information","text":""},{"location":"migration/Lochness_node_owners/#overview","title":"Overview","text":"<p>As a node owner on the Lochness high-performance computing (HPC) cluster, we would like to provide you with important information regarding the upcoming migration to the new cluster, Wulver.</p>"},{"location":"migration/Lochness_node_owners/#node-migration-to-wulver","title":"Node Migration to Wulver","text":""},{"location":"migration/Lochness_node_owners/#privately-owned-nodes","title":"Privately Owned Nodes","text":"<p>Most privately owned nodes on Lochness will be migrated to Wulver as part of the cluster upgrade. Owners of these nodes will be considered \"condo investors\" and will receive higher priority access on an equivalent amount of resources for a period of time to be determined. This ensures that your investment in HPC resources continues to be recognized and prioritized during the transition.</p>"},{"location":"migration/Lochness_node_owners/#decommissioning-of-out-of-warranty-nodes","title":"Decommissioning of Out-of-Warranty Nodes","text":"<p>Nodes that are out of warranty and deemed too old to become part of Wulver's infrastructure will be decommissioned. We will contact owners of such nodes for further discussions and potential considerations.</p>"},{"location":"migration/Lochness_node_owners/#node-exclusions-from-wulver-migration","title":"Node Exclusions from Wulver Migration","text":""},{"location":"migration/Lochness_node_owners/#nodes-with-consumer-grade-gpus","title":"Nodes with Consumer Grade GPUs","text":"<p>Nodes equipped with consumer-grade GPUs, such as Titan, RTX, etc., will not be migrated to Wulver. Instead, these nodes will be relocated to the GITC 4320 datacenter and will be individually managed by ARCS.</p> <p>This decision is made to ensure the optimal performance and compatibility of the Wulver cluster while providing an alternative solution for nodes with specialized hardware configurations.</p>"},{"location":"migration/Lochness_node_owners/#action-required","title":"Action Required","text":"<ol> <li>Privately Owned Node Owners: Await further communication regarding the migration process and priority access details.</li> <li>Owners of Out-of-Warranty Nodes: Await further communication for discussions on potential considerations and decommissioning.</li> <li>Owners of Nodes with Consumer Grade GPUs: Expect your nodes to be relocated to the GITC 4320 datacenter, managed individually by ARCS.</li> </ol>"},{"location":"migration/Lochness_node_owners/#timeline","title":"Timeline","text":"<p>The migration timeline and the duration of higher priority access for condo investors will be communicated to you in advance.</p>"},{"location":"migration/faqs/","title":"","text":""},{"location":"migration/faqs/#the-following-are-questions-we-have-received-over-the-last-several-weeks-regarding-the-migration-from-lochness-to-wulver","title":"The following are questions we have received over the last several weeks regarding the migration from Lochness to Wulver.","text":""},{"location":"migration/faqs/#what-is-the-timeline-for-the-migration-from-lochness-to-wulver","title":"What is the timeline for the migration from Lochness to Wulver?","text":"Answer <p>We anticipate the migration process to be complete by the end of February 2024. The migration will commence on January 16, 2024, and our team is dedicated to ensuring a smooth transition for all users. We will keep you informed about any updates or changes to the timeline as the migration progresses. Your cooperation and understanding during this period are greatly appreciated. If you have any specific concerns about the timeline, please feel free to reach out to our support team for further clarification.</p>"},{"location":"migration/faqs/#how-will-the-migration-impact-my-existing-workflows-and-computations","title":"How will the migration impact my existing workflows and computations?","text":"Answer <p>The research facilitation team is committed to assisting you during the migration process. Our team will work closely with you to ensure that your existing workflows and computations are seamlessly transferred to Wulver. We understand the importance of minimizing disruptions to your research activities, and our experts will provide guidance and support to address any compatibility issues that may arise. You can expect personalized assistance to make the transition as smooth as possible. If you have specific concerns about your workflows, please don't hesitate to reach out to our team for tailored support.</p>"},{"location":"migration/faqs/#what-specific-steps-should-i-take-to-prepare-for-the-migration","title":"What specific steps should I take to prepare for the migration?","text":"Answer <p>To prepare for the migration, there are a few crucial steps:</p> <ul> <li> <p>Provide a List of Current Students and Postdocs: </p> <ul> <li>Please share with us an updated list of current students, postdocs, and external collaborators who are actively using the HPC resources on Lochness. This information will ensure that user accounts are accurately migrated to Wulver, and access is maintained for the relevant individuals.</li> </ul> </li> <li> <p>List of Required Software: </p> <ul> <li>Compile a list of software applications that are essential for your research. This includes both commonly used software and any specialized tools unique to your work. Knowing your software requirements enables us to ensure that the necessary applications are available and properly configured on Wulver. You can find the list of software applications installed on Wulver in Software. If you don't find your applications in the list, submit a request for HPC Software Installation by visiting the Service Catalog.</li> </ul> </li> <li> <p>Planning for Former Students: </p> <ul> <li>If you have former students who may still have data or files on Lochness, it's essential to plan for their data migration or archival. We recommend reaching out to former students to coordinate any necessary data transfers or backups to avoid potential data loss.</li> </ul> </li> </ul> <p>These steps will contribute to a successful migration, allowing us to tailor the process to your specific needs. If you have any questions or need assistance with these preparations, please contact our support team.</p>"},{"location":"migration/faqs/#will-there-be-any-downtime-during-the-migration-and-how-will-it-affect-my-research-activities","title":"Will there be any downtime during the migration, and how will it affect my research activities?","text":"Answer <p>Once the migration has started for your group, Lochness will be inaccessible. If access to Lochness is critical for specific tasks during this period, we strongly encourage you to reach out to us. We understand that some users may have time-sensitive activities or dependencies on Lochness, and we are committed to working with you to find solutions that meet your needs. Please contact our support team by sending an email to hpc@njit.edu to discuss your specific requirements, and we will do our best to accommodate your situation during the migration process. You can check Contact Us to see how to create tickets with us and what information is required to create tickets. </p>"},{"location":"migration/faqs/#is-there-a-plan-for-backing-up-and-restoring-data-during-the-migration-process","title":"Is there a plan for backing up and restoring data during the migration process?","text":"Answer <p>Yes, there is a plan in place for data continuity. The data on Lochness resides on a shared filesystem, and these same filesystems will be mounted and available on Wulver post-migration. This approach ensures that your data remains accessible and seamlessly transfers to the new cluster.</p> <p>There is no need for a separate backup and restoration process as the shared filesystem continuity facilitates a smooth transition. If you have specific data-related concerns or requirements, please feel free to reach out to our support team (Contact Us) for further clarification and assistance. Your data integrity and accessibility are our top priorities throughout the migration process.</p>"},{"location":"migration/faqs/#what-changes-will-occur-in-the-file-directory-structure-especially-regarding-the-research-and-home-directories","title":"What changes will occur in the file directory structure, especially regarding the <code>/research</code> and <code>/home</code> directories?","text":"Answer <p>With the migration to Wulver, there will be changes in the file directory structure:</p> <p>Filesystems on Wulver: Wulver will have three filesystems available for use: <code>/home</code>, <code>/project</code>, and <code>/scratch</code>.</p> <ul> <li> <p>Availability of <code>/research</code> Directory: The <code>/research</code> directory from Lochness will be mounted on Wulver and will be available for use. This ensures continuity for research-related files and data.</p> </li> <li> <p>Lochness <code>/home</code> Directory on Wulver: The Lochness <code>/home</code> directory will be mounted on the Wulver login node as <code>/oldhome</code>. Users will be able to read files in this directory and move files from this directory but will not be able to write files into this deirectory. This allows users to access their personal home directories from Lochness during the migration period. The Lochness files under <code>/oldhome</code> will be available until 1-Jan-2025. Users should move needed files into <code>/project/PI_UCID</code> directory (replace PI_UCID with the UCID of PI). </p> </li> </ul> <p>These changes are designed to optimize the file organization on Wulver while maintaining accessibility to critical research data. The research facilitation team will work closely with users to ensure a smooth transition of data and assist in adapting to the new file directory structure. If you have specific questions or require assistance with data migration, please don't hesitate to contact our support team.</p>"},{"location":"migration/faqs/#how-will-the-migration-impact-access-to-specialized-software-or-applications-that-i-currently-use-on-lochness","title":"How will the migration impact access to specialized software or applications that I currently use on Lochness?","text":"Answer <p>The research facilitation team is dedicated to ensuring a smooth transition for users in terms of software and applications:</p> <ul> <li>Installation Support: <ul> <li>The research facilitation team will handle the installation of necessary software on Wulver, ensuring that essential tools and applications are available for your research. You can request for HPC Software Installation by visiting the Service Catalog. Please visit Software to see the list of applications isntalled on Wulver.</li> </ul> </li> <li>Code Compilation Assistance: <ul> <li>If your research involves custom code that needs compilation, the research facilitation team will provide assistance to ensure a successful compilation on Wulver. This support extends to helping users adapt their code to the new environment.</li> </ul> </li> </ul> <p>Our goal is to minimize any disruptions to your research activities and provide the necessary support for a seamless transition. If you have specific software requirements or need assistance with code compilation, please reach out to the research facilitation team, and they will be happy to assist you.</p>"},{"location":"migration/faqs/#can-i-continue-using-lochness-for-specific-tasks-during-the-migration-or-will-it-be-completely-inaccessible","title":"Can I continue using Lochness for specific tasks during the migration, or will it be completely inaccessible?","text":"Answer <p>Once the migration has started for your group, Lochness will be inaccessible. If access to Lochness is critical for specific tasks during this period, we strongly encourage you to reach out to us. We understand that some users may have time-sensitive activities or dependencies on Lochness, and we are committed to working with you to find solutions that meet your needs. Please contact our support team to discuss your specific requirements, and we will do our best to accommodate your situation during the migration process.</p>"},{"location":"migration/faqs/#what-happens-to-my-existing-job-submissions-and-queued-jobs-during-the-migration-process","title":"What happens to my existing job submissions and queued jobs during the migration process?","text":"Answer <p>For the most part, the migration will start after all running jobs have been completed. We understand the importance of job completion for ongoing research activities. Accommodations will be made for long-running jobs that cannot be checkpointed.   Our aim is to minimize disruptions to your computational tasks and ensure a smooth transition. If you have specific concerns about job submissions or if you anticipate long-running jobs during the migration period, please communicate with our support team. We are here to work collaboratively and make necessary accommodations to facilitate the completion of your jobs during the migration process.</p>"},{"location":"migration/faqs/#are-there-any-adjustments-needed-for-code-or-scripts-to-ensure-compatibility-with-wulver","title":"Are there any adjustments needed for code or scripts to ensure compatibility with Wulver?","text":"Answer <p>Yes, adjustments will be required for code or submission scripts to ensure compatibility with Wulver:</p> <ul> <li>Submit Script Changes: <ul> <li>Submit scripts will need to be modified to accommodate changes in partitions, hardware configurations, policies, and filesystems on Wulver. The research facilitation team will provide guidance and support in updating your submit scripts for seamless job submissions. Check the sample submit scipts for Wulver in SLURM.</li> </ul> </li> <li>Code Recompilation: <ul> <li>Due to differences in hardware, code may need to be recompiled to ensure optimal performance on Wulver. The research facilitation team is ready to assist you in this process, offering support to recompile code and address any related issues.</li> <li>If you code is compiled based on FOSS Toolchain (GCC and OpenMPI), you need to compile the code the same way you did in Lochness. Just make sure all the dependency libraries are installed on Wulver. Please visit Software to check the list of applications installed on Wulver.</li> <li>If your code is based on the Intel toolchain, you need to add the following while configuring your code.      <pre><code>./configure CFLAGS=\"-march=core-avx2\"\n</code></pre></li> <li>When installing codes, ensure that you perform the installation on the compute node rather than the login node. Since the hardware architecture is different on the login node, it's best practice to compile your code on the compute node. You need to initiate an interactive session with compute node before compiling your code.</li> </ul> </li> </ul> <p>Assistance will be provided to help you adapt your code and scripts to the new environment on Wulver. If you have specific concerns or require support in making these adjustments, please reach out to our research facilitation team, and they will work with you to ensure a smooth transition.</p>"},{"location":"migration/faqs/#will-there-be-training-sessions-or-documentation-available-to-help-faculty-and-researchers-transition-smoothly-to-wulver","title":"Will there be training sessions or documentation available to help faculty and researchers transition smoothly to Wulver?","text":"Answer <p>While there are no official training sessions scheduled at this point, comprehensive documentation is available at NJIT HPC Documentation to assist faculty and researchers during the transition to Wulver.   In addition to documentation, the research facilitation team is committed to providing personal assistance to faculty and researchers. If you have specific questions, require hands-on support, or need guidance on using Wulver effectively for your research, please do not hesitate to reach out to the research facilitation team. We are here to ensure that you receive the assistance you need for a successful transition.</p>"},{"location":"migration/faqs/#how-can-i-request-additional-resources-or-discuss-specific-requirements-for-my-research-projects-on-wulver","title":"How can I request additional resources or discuss specific requirements for my research projects on Wulver?","text":"Answer <p>To request additional resources or discuss specific requirements for your research projects on Wulver, please reach out to us at hpc@njit.edu. Our team is ready to assist you with any inquiries related to resource allocation, project needs, or any other aspects that can enhance your experience on the Wulver cluster. Your requests will be promptly addressed, and we are committed to providing the support necessary for the success of your research endeavors.</p>"},{"location":"migration/faqs/#what-support-services-will-be-available-during-and-after-the-migration-to-address-any-issues-or-concerns","title":"What support services will be available during and after the migration to address any issues or concerns?","text":"Answer <p>The research facilitation team is committed to providing personalized assistance and support services during and after the migration:</p> <ul> <li> <ul> <li>The research facilitation team is dedicated to offering personal assistance to each user. Whether you need help with data migration, code adjustments, or understanding the new environment, our team is here to provide tailored support.</li> </ul> <p>Personal Assistance:</p> <ul> <li>Issue Resolution:</li> <li> <p>Any issues or concerns that arise during or after the migration will be promptly addressed by the research facilitation team. We aim to ensure a smooth transition for all users and are ready to tackle any challenges that may arise.</p> </li> <li> <p>Ongoing Support:</p> </li> <li>Support services will continue to be available after the migration to address ongoing needs, answer questions, and assist with any further optimizations or adjustments required for your research projects.</li> </ul> </li> </ul> <p>Your success is our priority, and the research facilitation team is here to guide you through the migration process and beyond. If you encounter any issues or have specific concerns, please reach out to the team for personalized assistance.</p>"},{"location":"migration/faqs/#can-i-test-my-applications-or-simulations-on-wulver-before-the-migration-to-ensure-compatibility","title":"Can I test my applications or simulations on Wulver before the migration to ensure compatibility?","text":"Answer <p>Yes, absolutely! We encourage users to proactively test their applications or simulations on Wulver before the migration to ensure compatibility and identify any potential issues. This testing phase allows you to familiarize yourself with the new environment and address any concerns in advance.</p> <p>If you encounter challenges or have questions during the testing process, please don't hesitate to reach out to us. Our team is here to provide guidance, answer queries, and assist you in ensuring a smooth transition for your research activities on Wulver. Your proactive testing will contribute to a successful migration experience.</p>"},{"location":"migration/faqs/#how-long-will-the-oldhome-directory-on-lochness-be-accessible-after-the-migration-and-what-actions-should-i-take-regarding-data-stored-there","title":"How long will the <code>/oldhome</code> directory on Lochness be accessible after the migration, and what actions should I take regarding data stored there?","text":"Answer <p>The <code>/oldhome</code> directory on Lochness will be accessible for 6 months after the migration is complete. During this period, users are advised to review and move their data to other locations on Wulver or archive it as needed. This timeframe provides a reasonable window for users to organize and transfer their data while ensuring a smooth transition.</p> <p>If you have specific questions about data migration or need assistance during this post-migration period, please reach out to our support team. We are here to help you with any further steps or considerations related to your data on Lochness.</p>"},{"location":"migration/faqs/#will-afs-be-available-on-wulver","title":"Will AFS be available on Wulver?","text":"Answer <p>The AFS will not be available on Wulver. However we will setup a self-serve procedure for researchers to move AFS files to <code>/research</code>. We can do this as part of the full migration process. Please reach out to us at hpc@njit.edu for any questions.</p>"},{"location":"migration/faqs/#i-have-several-conda-environments-on-lochness-should-i-perform-a-fresh-installation-or-copy-the-environment-to-wulver","title":"I have several Conda environments on Lochness. Should I perform a fresh installation or copy the environment to Wulver?","text":"Answer <p>We recommend a fresh installation since the hardware architecture is different on Wulver. However, you can export the existing Conda environment to a YAML file, transfer it to Wulver, and then install the environment using this YAML file. See Conda Export Environment for details.</p>"},{"location":"migration/lochness_filesystem/","title":"Lochness to Wulver Migration: File Directory Changes","text":""},{"location":"migration/lochness_filesystem/#overview","title":"Overview","text":"<p>As part of the HPC cluster migration from Lochness to Wulver, there will be changes in the file directory structure to ensure a smooth transition. This documentation provides details on how the <code>/research</code> and home directories will be handled on the new Wulver cluster.</p>"},{"location":"migration/lochness_filesystem/#research-directory","title":"<code>/research</code> Directory","text":"<p>The <code>/research</code> directory from Lochness will be mounted on Wulver as <code>/research</code>. This ensures that users can seamlessly access their research-related files and data in the familiar directory structure. All content, including subdirectories and files, within <code>/research</code> on Lochness will be avaialble on the same location on Wulver.</p>"},{"location":"migration/lochness_filesystem/#home-directories","title":"Home Directories","text":""},{"location":"migration/lochness_filesystem/#lochness-home-directory","title":"Lochness Home Directory","text":"<p>The home directories from Lochness will be available on Wulver under the directory <code>/oldhome</code>. Users will find their personal home directories within <code>/oldhome</code>, allowing them to access their files and configurations.</p>"},{"location":"migration/lochness_filesystem/#locking-of-oldhome-directory","title":"Locking of <code>/oldhome</code> Directory","text":"<p>After the migration is completed, the <code>/oldhome</code> directory on Lochness will be locked. This means that no additional files or directories can be created in <code>/oldhome</code>. The locking of this directory is a crucial step to maintain the integrity of the migrated data and to ensure a stable environment on both clusters.</p> <p>Users are advised to review and transfer any necessary files from <code>/oldhome</code> on Lochness to their new home directories on Wulver during the migration period.</p>"},{"location":"migration/lochness_filesystem/#important-notes","title":"Important Notes","text":"<ul> <li>Migration Date: The migration process is scheduled to start on January 16, 2024.</li> <li>Locking Date for <code>/oldhome</code>: After the migration, the <code>/oldhome</code> directory on Lochness will be locked to prevent further modifications.</li> <li>File Access: Users will have continued access to their research files under <code>/research</code> on Wulver and their home directories under <code>/oldhome</code> on Lochness until the specified locking date.</li> </ul> <p>Please make the necessary adjustments to your workflows and ensure a smooth transition by reviewing and moving your files as needed. If you have any questions or concerns, feel free to reach out to our support team.</p>"},{"location":"news/","title":"Cluster Maintenance Updates and News","text":""},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/","title":"Relocation of Lochness to Databank Datacenter","text":"<p>Dear Lochness Users,</p> <p>We hope this message finds you well. We want to inform you of an upcoming significant event regarding our HPC (High-Performance Computing) cluster, lochness.njit.edu. The GITC datacenter is scheduled for demolition on November 1, 2023. In order to maintain the operation of our computing infrastructure, we will be relocating the cluster to the Databank colocation facility in Piscataway.</p>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#key-details","title":"Key Details:","text":"<ul> <li>Cluster Shutdown Date: October 6<sup>th</sup>, 2023 at Noon</li> <li>Anticipated Duration: Up to Seven Days</li> <li>Operational Continuity: After the move the cluster will remain operational until the end of the semester.</li> <li>User Migration: We are actively working on migrating all users to the new cluster, wulver.njit.edu, before the end of the semester.</li> </ul>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#cluster-relocation-details","title":"Cluster Relocation Details:","text":"<p>The scheduled shutdown of the lochness.njit.edu cluster will take place on October 6<sup>th</sup>, 2023. We have estimated that the relocation process will require no longer than five days to complete. During this time, the cluster will not be accessible. We understand the importance of uninterrupted access to computational resources, and we will make every effort to minimize downtime.</p>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#operational-continuity","title":"Operational Continuity:","text":"<p>Rest assured that we are committed to maintaining cluster availability for your research and academic needs. The lochness.njit.edu cluster will remain operational until the end of the current semester. This means that you will have access to its computing power throughout your ongoing projects and coursework.</p>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#user-migration","title":"User Migration:","text":"<p>Our team is actively working on the migration process to ensure a smooth transition for all users. We plan to migrate all users to the new cluster, wulver.njit.edu, well before the end of the semester. Detailed instructions and support will be provided to facilitate this transition, and we will keep you updated on the migration progress.</p> <p>We understand that this relocation may raise questions or concerns, and we are here to address them. Please feel free to reach out to hpc@njit.edu you have any specific inquiries or require further information.</p> <p>We appreciate your understanding and cooperation during this transitional period. The relocation of our HPC cluster is aimed at providing you with an improved and more reliable computing environment.</p> <p>Thank you for your ongoing support and contributions to our research community.</p>"},{"location":"news/2023/10/14/wulver-maintenance/","title":"Wulver Maintenance","text":"<p>GPFS Fileset Changes</p> <p>Wulver will be out of service Wed Oct 18<sup>th</sup> between 9:00 am-11:00 am for updates and configuration changes. The maintence will be conducted to fix the <code>stale file handle</code> error on <code>/scratch</code> while accessing files from login node.</p>"},{"location":"news/2023/10/14/wulver-maintenance/#maintencance-plans","title":"Maintencance Plans","text":""},{"location":"news/2023/10/14/wulver-maintenance/#recommendation","title":"Recommendation:","text":"<ul> <li>Each fileset gets it\u2019s own inode namespace</li> <li>Fileset names to automatically inherit pool policies</li> <li>Additional fileset settings for chmod to not conflict with ACLs</li> </ul>"},{"location":"news/2023/10/14/wulver-maintenance/#migration-plan","title":"Migration Plan:","text":"<ul> <li>Create new filesets and link under /mmfs1/Scratch and /mmfs1/Project<ul> <li>New fileset names with sata1-project_xx and nvme1-scratch_xx (no bearing on FS path)</li> <li>New fileset have own inode spaces</li> </ul> </li> <li>Rsync data from old to new location</li> <li>Job outage for final copy and change</li> <li>Final rsyncs<ul> <li>Remove symlink for /mmfs1/scratch and create /mmfs1/scratch</li> <li>Unlink/relink filesets in new location</li> <li>Resolve any links remaining on nodes/images</li> </ul> </li> </ul>"},{"location":"news/2023/10/18/lochness-maintenance-updates/","title":"Lochness Maintenance Updates","text":"<p>Lochness is Back Online!</p> <p>Lochness is mostly back up after being moved to a new facility.  The move required complete disassembly and reassembly of the entire cluster. There are 8 nodes down as they were damaged in the move, repairs forthcoming. Infiniband network issues affect 50 nodes, these are in \"drain\" state. Currently 120 nodes are fully functional. You can use <code>sinfo</code> to see the exact states of nodes accessible to you. Please email hpc@njit.edu for assistance.</p>"},{"location":"news/2024/01/22/wulver-maintenance/","title":"Wulver Maintenance","text":"<p>Wulver Monthly Maintenance</p> <p>Beginning Feb 1, 2024, ARCS HPC will be instituting a monthly maintenance downtime on all HPC systems on the second Tuesday from 9AM - 9PM. Wulver and the associated GPFS storage will be taken out of service for maintenance, repairs, patches and upgrades. During the maintenance downtime, logins will be disabled, users will not have access to their stored data in <code>/project</code>, <code>/home</code> and <code>/scratch</code>. All jobs that do not end before 9AM will be held by the scheduler until the downtime is complete and the systems are returned to service.</p> <p>We anticipate maintenance to be completed by the scheduled time. However, occasionally the maintenance may be completed earlier than scheduled or could be extended to the following days. A notification will be sent to the user mailing list when the systems are returned to service or the maintenance window is extended. Additionally, users will encounter the cluster service information upon logging in to Wulver during maintenance. Please pay attention to the Message of the Day when logging in, as it will serve as a reminder for upcoming downtimes or other crucial cluster-related information. Users should take into account the maintenance window when scheduling jobs and developing plans to meet various deadlines. Please do not contact the help desk, HPC staff or open SNOW tickets for access to the cluster or data during the maintenance downtime.</p>"},{"location":"news/2024/06/18/hpc-summer-events/","title":"HPC Summer Events","text":"<p>ARCS HPC invites you to our upcoming events. Please register for the events you plan to attend.</p>"},{"location":"news/2024/06/18/hpc-summer-events/#nvidia-workshop-fundamentals-of-accelerated-data-science","title":"NVIDIA Workshop \u2014 Fundamentals of Accelerated Data Science","text":"<p>Save the Date</p> <ul> <li>Date: July 15, 2024</li> <li>Location: GITC 3700</li> <li>Time: 9 a.m. - 5 p.m.</li> </ul> <p>Learn to use GPU-accelerated resources to analyze data. This is an intermediate level workshop that is intended for those who have some familiarity with Python, especially NumPy and SciPy libraries. See more detail about the workshop here.</p> <p>Registration</p>"},{"location":"news/2024/06/18/hpc-summer-events/#hpc-research-symposium","title":"HPC Research Symposium","text":"<p>Save the Date</p> <ul> <li>Date: July 16, 2024</li> <li>Location: Student Center Atrium</li> <li>Time: 9 a.m. - 5 p.m.</li> </ul> <p>This past year has been transformative for HPC Research at NJIT. The introduction of our new shared HPC cluster, Wulver, has expanded our computational capacity and made the research into vital areas more accessible to our faculty. Please join us to highlight the work of researchers using the HPC resources and connect with the NJIT HPC community. </p> <p>Please register for the symposium here, you can also sign up to present your HPC research as a lightning talk or poster presentation:  </p>"},{"location":"news/2024/06/18/hpc-summer-events/#slurm-workload-manager-workshop","title":"SLURM Workload Manager Workshop","text":"<p>Save the Date</p> <ul> <li>Date: August 13 &amp; 14, 2024</li> <li>Location: GITC 3700</li> <li>Time: 9 a.m. - 5 p.m.</li> </ul> <p>This immersive 2-day experience will take you through comprehensive technical scenarios with lectures, demos, and workshop lab environments. The Slurm trainer will assist in identifying commonalities between previously used resources and schedulers, offering increased understanding and adoption of SLURM job scheduling, resource management, and troubleshooting techniques. </p> <p>Registration is now closed.</p>"},{"location":"news/2024/09/11/hpc-fall-events/","title":"HPC Fall Events","text":"<p>ARCS HPC invites you to our upcoming events. Please register for the events you plan to attend.</p>"},{"location":"news/2024/09/11/hpc-fall-events/#slurm-batch-system-basics","title":"SLURM Batch System Basics","text":"<p>Save the Date</p> <ul> <li>Date: Sep 18<sup>th</sup>, 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul> <p>Join us for an informative webinar designed to introduce researchers, scientists, and HPC users to the fundamentals of the SLURM (Simple Linux Utility for Resource Management) workload manager. This virtual  session will equip you with essential skills to effectively utilize HPC resources through SLURM.</p> <p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"news/2024/09/11/hpc-fall-events/#introduction-to-containers-on-wulver","title":"Introduction to Containers on Wulver","text":"<p>Save the Date</p> <ul> <li>Date: Oct 16<sup>th</sup>, 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul> <p>The HPC training event on using Singularity containers provides participants with a comprehensive introduction to container technology and its advantages in high-performance computing environments. Attendees will learn the fundamentals of Singularity, including installation, basic commands, and workflow, as well as how to create and build containers using definition files and existing Docker images.</p> <p>Registration is now closed. Check the HPC training for the webinar recording and slides. </p>"},{"location":"news/2024/09/11/hpc-fall-events/#job-arrays-and-advanced-submission-techniques-for-hpc","title":"Job Arrays and Advanced Submission Techniques for HPC","text":"<p>Save the Date</p> <ul> <li>Date: Nov 20<sup>th</sup>, 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul> <p>Elevate your High-Performance Computing skills with our advanced SLURM webinar! This session is designed for HPC users who are familiar with basic SLURM commands and are ready to dive into more sophisticated job management techniques.</p> <p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"news/2024/11/05/office-hours/","title":"Office Hours","text":"<p>We currently offer drop-in office hours every Wednesday and Friday from 2:00\u20134:00 p.m. Stop by to meet with our student consultants and ask any questions you have about using HPC resources. Whether you\u2019re just getting started or need help with a specific issue, feel free to bring your laptop to walk us through any problems you're facing. There's no need to create a ticket in advance; if follow-up is needed, the student consultants will open a ticket on your behalf, and you'll receive further instructions. </p> <p>Consulting Hours</p> <ul> <li>Date: Every Wednesday and Friday</li> <li>Location: GITC 2404</li> <li>Time: 2:00 PM - 4:00 PM</li> </ul>"},{"location":"news/2025/01/14/hpc-2025-spring-events/","title":"HPC 2025 Spring Events","text":"<p>ARCS HPC invites you to our upcoming events. Please register for the events you plan to attend.</p>"},{"location":"news/2025/01/14/hpc-2025-spring-events/#introduction-to-wulver-getting-started","title":"Introduction to Wulver: Getting Started","text":"<p>Save the Date</p> <ul> <li>Date: Jan 22<sup>nd</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul> <p>Join us for an informative webinar designed to introduce NJIT's HPC environment, Wulver. This virtual session will provide essential information about the Wulver cluster, how to get an account, and allocation details.</p> <p>Registration is now closed.</p>"},{"location":"news/2025/01/14/hpc-2025-spring-events/#introduction-to-wulver-accessing-system-running-jobs","title":"Introduction to Wulver: Accessing System &amp; Running Jobs","text":"<p>Save the Date</p> <ul> <li>Date: Jan 29<sup>th</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul> <p>This HPC training event focuses on providing the fundamentals of SLURM (Simple Linux Utility for Resource Management), a workload manager. This virtual session will equip you with the essential skills needed to effectively utilize HPC resources using SLURM.</p> <p>Registration is now closed.</p>"},{"location":"news/2025/01/23/office-hours/","title":"Office Hours","text":"<p>This spring semester, we are offering drop-in office hours every Monday and Wednesday from 2:00 to 4:00 p.m. Stop by to meet with our student consultants and ask any questions you have about using HPC resources. Whether you\u2019re just getting started or need help with a specific issue, feel free to bring your laptop to walk us through any problems you're facing. There's no need to create a ticket in advance; if follow-up is needed, the student consultants will open a ticket on your behalf, and you'll receive further instructions. </p> <p>Consulting Hours</p> <ul> <li>Date: Every Monday and Wednesday</li> <li>Location: GITC 2404</li> <li>Time: 2:00 PM - 4:00 PM</li> </ul>"}]}